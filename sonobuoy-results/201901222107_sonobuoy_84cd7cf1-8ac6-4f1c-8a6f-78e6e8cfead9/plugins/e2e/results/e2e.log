I0122 21:07:17.058756      14 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-259822818
I0122 21:07:17.058950      14 e2e.go:224] Starting e2e run "b2776d0b-1e89-11e9-9284-e2fd7d97dccb" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1548191236 - Will randomize all specs
Will run 201 of 1946 specs

Jan 22 21:07:17.193: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 21:07:17.196: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 22 21:07:17.208: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 22 21:07:17.235: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 22 21:07:17.235: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 22 21:07:17.235: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 22 21:07:17.246: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 22 21:07:17.246: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 22 21:07:17.246: INFO: e2e test version: v1.13.0
Jan 22 21:07:17.248: INFO: kube-apiserver version: v1.13.2
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:07:17.249: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename daemonsets
Jan 22 21:07:17.335: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:07:17.352: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 22 21:07:17.363: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:17.370: INFO: Number of nodes with available pods: 0
Jan 22 21:07:17.370: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:07:18.380: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:18.387: INFO: Number of nodes with available pods: 0
Jan 22 21:07:18.387: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:07:19.373: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:19.376: INFO: Number of nodes with available pods: 1
Jan 22 21:07:19.376: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 22 21:07:19.407: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:19.411: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:20.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:20.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:21.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:21.418: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:22.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:22.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:23.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:23.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:24.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:24.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:25.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:25.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:26.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:26.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:27.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:27.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:28.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:28.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:29.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:29.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:30.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:30.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:31.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:31.420: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:32.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:32.420: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:33.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:33.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:34.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:34.420: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:35.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:35.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:36.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:36.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:37.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:37.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:38.413: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:38.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:39.420: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:39.426: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:40.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:40.420: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:41.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:41.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:42.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:42.418: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:43.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:43.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:44.417: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:44.426: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:45.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:45.418: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:46.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:46.419: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:47.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:47.418: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:48.414: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:48.417: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:49.417: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:49.422: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:50.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:50.418: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:51.416: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:51.421: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:52.415: INFO: Wrong image for pod: daemon-set-dlr8f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 22 21:07:52.415: INFO: Pod daemon-set-dlr8f is not available
Jan 22 21:07:52.418: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:53.428: INFO: Pod daemon-set-mkjv7 is not available
Jan 22 21:07:53.433: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 22 21:07:53.437: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:53.441: INFO: Number of nodes with available pods: 0
Jan 22 21:07:53.441: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:07:54.451: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:07:54.455: INFO: Number of nodes with available pods: 1
Jan 22 21:07:54.455: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-mcrn2, will wait for the garbage collector to delete the pods
Jan 22 21:07:54.581: INFO: Deleting DaemonSet.extensions daemon-set took: 5.125681ms
Jan 22 21:07:54.682: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.227786ms
Jan 22 21:08:01.985: INFO: Number of nodes with available pods: 0
Jan 22 21:08:01.985: INFO: Number of running nodes: 0, number of available pods: 0
Jan 22 21:08:01.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-mcrn2/daemonsets","resourceVersion":"28825"},"items":null}

Jan 22 21:08:01.990: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-mcrn2/pods","resourceVersion":"28825"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:08:01.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-mcrn2" for this suite.
Jan 22 21:08:08.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:08:08.047: INFO: namespace: e2e-tests-daemonsets-mcrn2, resource: bindings, ignored listing per whitelist
Jan 22 21:08:08.095: INFO: namespace e2e-tests-daemonsets-mcrn2 deletion completed in 6.096433111s

• [SLOW TEST:50.846 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:08:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:08:08.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-v44fm" to be "success or failure"
Jan 22 21:08:08.176: INFO: Pod "downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908718ms
Jan 22 21:08:10.181: INFO: Pod "downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010842391s
STEP: Saw pod success
Jan 22 21:08:10.182: INFO: Pod "downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:08:10.185: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:08:10.251: INFO: Waiting for pod downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:08:10.254: INFO: Pod downwardapi-volume-d1424802-1e89-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:08:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v44fm" for this suite.
Jan 22 21:08:16.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:08:16.363: INFO: namespace: e2e-tests-downward-api-v44fm, resource: bindings, ignored listing per whitelist
Jan 22 21:08:16.379: INFO: namespace e2e-tests-downward-api-v44fm deletion completed in 6.120621571s

• [SLOW TEST:8.284 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:08:16.379: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d6324652-1e89-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:08:16.456: INFO: Waiting up to 5m0s for pod "pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-j82ns" to be "success or failure"
Jan 22 21:08:16.459: INFO: Pod "pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.379081ms
Jan 22 21:08:18.463: INFO: Pod "pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007371874s
STEP: Saw pod success
Jan 22 21:08:18.463: INFO: Pod "pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:08:18.467: INFO: Trying to get logs from node secconf-node pod pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:08:18.490: INFO: Waiting for pod pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:08:18.495: INFO: Pod pod-secrets-d632b87c-1e89-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:08:18.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j82ns" for this suite.
Jan 22 21:08:24.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:08:24.556: INFO: namespace: e2e-tests-secrets-j82ns, resource: bindings, ignored listing per whitelist
Jan 22 21:08:24.653: INFO: namespace e2e-tests-secrets-j82ns deletion completed in 6.154707644s

• [SLOW TEST:8.274 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:08:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:08:24.773: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-jgnz4" to be "success or failure"
Jan 22 21:08:24.777: INFO: Pod "downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675132ms
Jan 22 21:08:26.782: INFO: Pod "downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009066374s
STEP: Saw pod success
Jan 22 21:08:26.782: INFO: Pod "downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:08:26.786: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:08:26.802: INFO: Waiting for pod downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:08:26.804: INFO: Pod downwardapi-volume-db272f7c-1e89-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:08:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jgnz4" for this suite.
Jan 22 21:08:32.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:08:32.831: INFO: namespace: e2e-tests-downward-api-jgnz4, resource: bindings, ignored listing per whitelist
Jan 22 21:08:32.886: INFO: namespace e2e-tests-downward-api-jgnz4 deletion completed in 6.077092029s

• [SLOW TEST:8.232 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:08:32.886: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 22 21:08:32.954: INFO: Waiting up to 5m0s for pod "pod-e0077245-1e89-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-jl289" to be "success or failure"
Jan 22 21:08:32.960: INFO: Pod "pod-e0077245-1e89-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.319424ms
Jan 22 21:08:34.968: INFO: Pod "pod-e0077245-1e89-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014000171s
STEP: Saw pod success
Jan 22 21:08:34.968: INFO: Pod "pod-e0077245-1e89-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:08:34.973: INFO: Trying to get logs from node secconf-node pod pod-e0077245-1e89-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:08:35.006: INFO: Waiting for pod pod-e0077245-1e89-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:08:35.013: INFO: Pod pod-e0077245-1e89-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:08:35.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jl289" for this suite.
Jan 22 21:08:41.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:08:41.100: INFO: namespace: e2e-tests-emptydir-jl289, resource: bindings, ignored listing per whitelist
Jan 22 21:08:41.123: INFO: namespace e2e-tests-emptydir-jl289 deletion completed in 6.095513234s

• [SLOW TEST:8.237 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:08:41.123: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:09:41.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-zd5fq" for this suite.
Jan 22 21:10:03.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:10:03.268: INFO: namespace: e2e-tests-container-probe-zd5fq, resource: bindings, ignored listing per whitelist
Jan 22 21:10:03.295: INFO: namespace e2e-tests-container-probe-zd5fq deletion completed in 22.092081188s

• [SLOW TEST:82.172 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:10:03.295: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-15ebd080-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:10:03.368: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-mmg2t" to be "success or failure"
Jan 22 21:10:03.373: INFO: Pod "pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.418589ms
Jan 22 21:10:05.376: INFO: Pod "pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008743024s
STEP: Saw pod success
Jan 22 21:10:05.377: INFO: Pod "pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:10:05.379: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:10:05.400: INFO: Waiting for pod pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:10:05.404: INFO: Pod pod-projected-secrets-15ec3acd-1e8a-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:10:05.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mmg2t" for this suite.
Jan 22 21:10:11.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:10:11.440: INFO: namespace: e2e-tests-projected-mmg2t, resource: bindings, ignored listing per whitelist
Jan 22 21:10:11.522: INFO: namespace e2e-tests-projected-mmg2t deletion completed in 6.113858633s

• [SLOW TEST:8.227 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:10:11.523: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-1ad54dc0-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:10:11.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-s66l4" to be "success or failure"
Jan 22 21:10:11.619: INFO: Pod "pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332407ms
Jan 22 21:10:13.623: INFO: Pod "pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009197842s
STEP: Saw pod success
Jan 22 21:10:13.623: INFO: Pod "pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:10:13.627: INFO: Trying to get logs from node secconf-node pod pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:10:13.656: INFO: Waiting for pod pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:10:13.661: INFO: Pod pod-configmaps-1ad60a14-1e8a-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:10:13.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-s66l4" for this suite.
Jan 22 21:10:19.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:10:19.759: INFO: namespace: e2e-tests-configmap-s66l4, resource: bindings, ignored listing per whitelist
Jan 22 21:10:19.759: INFO: namespace e2e-tests-configmap-s66l4 deletion completed in 6.09305157s

• [SLOW TEST:8.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:10:19.759: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:10:19.821: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:10:21.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-f9gg6" for this suite.
Jan 22 21:11:03.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:11:03.894: INFO: namespace: e2e-tests-pods-f9gg6, resource: bindings, ignored listing per whitelist
Jan 22 21:11:04.002: INFO: namespace e2e-tests-pods-f9gg6 deletion completed in 42.141385551s

• [SLOW TEST:44.243 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:11:04.002: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 22 21:11:04.133: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:04.138: INFO: Number of nodes with available pods: 0
Jan 22 21:11:04.138: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:05.142: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:05.145: INFO: Number of nodes with available pods: 0
Jan 22 21:11:05.145: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:06.143: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:06.147: INFO: Number of nodes with available pods: 1
Jan 22 21:11:06.147: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 22 21:11:06.161: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:06.167: INFO: Number of nodes with available pods: 0
Jan 22 21:11:06.167: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:07.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:07.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:07.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:08.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:08.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:08.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:09.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:09.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:09.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:10.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:10.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:10.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:11.170: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:11.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:11.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:12.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:12.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:12.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:13.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:13.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:13.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:14.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:14.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:14.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:15.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:15.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:15.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:16.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:16.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:16.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:17.170: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:17.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:17.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:18.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:18.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:18.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:19.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:19.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:19.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:20.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:20.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:20.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:21.173: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:21.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:21.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:22.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:22.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:22.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:23.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:23.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:23.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:24.173: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:24.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:24.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:25.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:25.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:25.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:26.173: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:26.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:26.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:27.173: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:27.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:27.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:28.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:28.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:28.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:29.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:29.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:29.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:30.174: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:30.177: INFO: Number of nodes with available pods: 0
Jan 22 21:11:30.177: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:31.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:31.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:31.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:32.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:32.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:32.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:33.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:33.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:33.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:34.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:34.176: INFO: Number of nodes with available pods: 0
Jan 22 21:11:34.176: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:35.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:35.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:35.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:36.172: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:36.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:36.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:37.170: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:37.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:37.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:38.170: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:38.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:38.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:39.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:39.173: INFO: Number of nodes with available pods: 0
Jan 22 21:11:39.173: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:40.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:40.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:40.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:41.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:41.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:41.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:42.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:42.174: INFO: Number of nodes with available pods: 0
Jan 22 21:11:42.174: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:43.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:43.175: INFO: Number of nodes with available pods: 0
Jan 22 21:11:43.175: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:11:44.171: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:11:44.174: INFO: Number of nodes with available pods: 1
Jan 22 21:11:44.174: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-4skgz, will wait for the garbage collector to delete the pods
Jan 22 21:11:44.238: INFO: Deleting DaemonSet.extensions daemon-set took: 6.924139ms
Jan 22 21:11:44.339: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.516175ms
Jan 22 21:12:22.043: INFO: Number of nodes with available pods: 0
Jan 22 21:12:22.044: INFO: Number of running nodes: 0, number of available pods: 0
Jan 22 21:12:22.046: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-4skgz/daemonsets","resourceVersion":"29440"},"items":null}

Jan 22 21:12:22.050: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-4skgz/pods","resourceVersion":"29440"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:12:22.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-4skgz" for this suite.
Jan 22 21:12:28.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:12:28.113: INFO: namespace: e2e-tests-daemonsets-4skgz, resource: bindings, ignored listing per whitelist
Jan 22 21:12:28.168: INFO: namespace e2e-tests-daemonsets-4skgz deletion completed in 6.105713912s

• [SLOW TEST:84.166 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:12:28.168: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-dt65s;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-dt65s;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-dt65s.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-dt65s.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-dt65s.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 55.225.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.225.55_udp@PTR;check="$$(dig +tcp +noall +answer +search 55.225.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.225.55_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-dt65s;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-dt65s;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-dt65s.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-dt65s.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-dt65s.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-dt65s.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-dt65s.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 55.225.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.225.55_udp@PTR;check="$$(dig +tcp +noall +answer +search 55.225.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.225.55_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 22 21:12:32.305: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.310: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.327: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.357: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.361: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.364: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.367: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.370: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.375: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.378: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.381: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:32.402: INFO: Lookups using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-dt65s wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-dt65s jessie_tcp@dns-test-service.e2e-tests-dns-dt65s jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc]

Jan 22 21:12:37.413: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.417: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.433: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.464: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.468: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.472: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.476: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.479: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.482: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.485: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.489: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:37.523: INFO: Lookups using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-dt65s wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-dt65s jessie_tcp@dns-test-service.e2e-tests-dns-dt65s jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc]

Jan 22 21:12:42.410: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.414: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.466: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.470: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.474: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.478: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.482: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.486: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.491: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.495: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:42.521: INFO: Lookups using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-dt65s wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-dt65s jessie_tcp@dns-test-service.e2e-tests-dns-dt65s jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc]

Jan 22 21:12:47.415: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.420: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.436: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.470: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.475: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.479: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.483: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.487: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.492: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.496: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.500: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:47.532: INFO: Lookups using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-dt65s wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-dt65s jessie_tcp@dns-test-service.e2e-tests-dns-dt65s jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc]

Jan 22 21:12:52.411: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.414: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.433: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.468: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.473: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.477: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.481: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.485: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.489: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.494: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.498: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:52.525: INFO: Lookups using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-dt65s wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-dt65s jessie_tcp@dns-test-service.e2e-tests-dns-dt65s jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc]

Jan 22 21:12:57.410: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.413: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.426: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.451: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.454: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.458: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.461: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.464: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.468: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.471: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.475: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc from pod e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb: the server could not find the requested resource (get pods dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb)
Jan 22 21:12:57.493: INFO: Lookups using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-dt65s wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-dt65s jessie_tcp@dns-test-service.e2e-tests-dns-dt65s jessie_udp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@dns-test-service.e2e-tests-dns-dt65s.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-dt65s.svc]

Jan 22 21:13:02.517: INFO: DNS probes using e2e-tests-dns-dt65s/dns-test-6c48baa8-1e8a-11e9-9284-e2fd7d97dccb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:13:02.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-dt65s" for this suite.
Jan 22 21:13:08.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:13:08.617: INFO: namespace: e2e-tests-dns-dt65s, resource: bindings, ignored listing per whitelist
Jan 22 21:13:08.707: INFO: namespace e2e-tests-dns-dt65s deletion completed in 6.124350731s

• [SLOW TEST:40.539 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:13:08.707: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Jan 22 21:13:08.784: INFO: Waiting up to 5m0s for pod "client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-containers-wxbt5" to be "success or failure"
Jan 22 21:13:08.788: INFO: Pod "client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535089ms
Jan 22 21:13:10.791: INFO: Pod "client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007316008s
STEP: Saw pod success
Jan 22 21:13:10.791: INFO: Pod "client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:13:10.794: INFO: Trying to get logs from node secconf-node pod client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:13:10.816: INFO: Waiting for pod client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:13:10.819: INFO: Pod client-containers-84706780-1e8a-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:13:10.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wxbt5" for this suite.
Jan 22 21:13:16.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:13:16.875: INFO: namespace: e2e-tests-containers-wxbt5, resource: bindings, ignored listing per whitelist
Jan 22 21:13:16.947: INFO: namespace e2e-tests-containers-wxbt5 deletion completed in 6.123424208s

• [SLOW TEST:8.240 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:13:16.947: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-895be1e8-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-895be1e8-1e8a-11e9-9284-e2fd7d97dccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:13:23.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hs4ls" for this suite.
Jan 22 21:13:45.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:13:45.237: INFO: namespace: e2e-tests-configmap-hs4ls, resource: bindings, ignored listing per whitelist
Jan 22 21:13:45.237: INFO: namespace e2e-tests-configmap-hs4ls deletion completed in 22.138289272s

• [SLOW TEST:28.290 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:13:45.237: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-j9c5z
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-j9c5z
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-j9c5z
Jan 22 21:13:45.406: INFO: Found 0 stateful pods, waiting for 1
Jan 22 21:13:55.410: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 22 21:13:55.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 21:13:55.646: INFO: stderr: ""
Jan 22 21:13:55.647: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 21:13:55.647: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 21:13:55.650: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 22 21:14:05.654: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 21:14:05.654: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 21:14:05.666: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999614s
Jan 22 21:14:06.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995686018s
Jan 22 21:14:07.674: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991007955s
Jan 22 21:14:08.678: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987002144s
Jan 22 21:14:09.683: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982287682s
Jan 22 21:14:10.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97733463s
Jan 22 21:14:11.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972818117s
Jan 22 21:14:12.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968772986s
Jan 22 21:14:13.701: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965024979s
Jan 22 21:14:14.707: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.261283ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-j9c5z
Jan 22 21:14:15.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 21:14:15.854: INFO: stderr: ""
Jan 22 21:14:15.854: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 21:14:15.854: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 21:14:15.856: INFO: Found 1 stateful pods, waiting for 3
Jan 22 21:14:25.861: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 21:14:25.861: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 21:14:25.861: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 22 21:14:25.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 21:14:26.008: INFO: stderr: ""
Jan 22 21:14:26.008: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 21:14:26.008: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 21:14:26.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 21:14:26.156: INFO: stderr: ""
Jan 22 21:14:26.156: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 21:14:26.156: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 21:14:26.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 21:14:26.298: INFO: stderr: ""
Jan 22 21:14:26.299: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 21:14:26.299: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 21:14:26.299: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 21:14:26.301: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 22 21:14:36.308: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 21:14:36.308: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 21:14:36.308: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 21:14:36.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999996s
Jan 22 21:14:37.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996275154s
Jan 22 21:14:38.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991132068s
Jan 22 21:14:39.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986129927s
Jan 22 21:14:40.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982570618s
Jan 22 21:14:41.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978691355s
Jan 22 21:14:42.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974200686s
Jan 22 21:14:43.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967855124s
Jan 22 21:14:44.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963399565s
Jan 22 21:14:45.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.682059ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-j9c5z
Jan 22 21:14:46.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 21:14:46.529: INFO: stderr: ""
Jan 22 21:14:46.529: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 21:14:46.529: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 21:14:46.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 21:14:46.706: INFO: stderr: ""
Jan 22 21:14:46.706: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 21:14:46.706: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 21:14:46.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-j9c5z ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 21:14:46.858: INFO: stderr: ""
Jan 22 21:14:46.859: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 21:14:46.859: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 21:14:46.859: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 22 21:14:56.874: INFO: Deleting all statefulset in ns e2e-tests-statefulset-j9c5z
Jan 22 21:14:56.877: INFO: Scaling statefulset ss to 0
Jan 22 21:14:56.884: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 21:14:56.886: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:14:56.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-j9c5z" for this suite.
Jan 22 21:15:02.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:15:03.001: INFO: namespace: e2e-tests-statefulset-j9c5z, resource: bindings, ignored listing per whitelist
Jan 22 21:15:03.001: INFO: namespace e2e-tests-statefulset-j9c5z deletion completed in 6.094122755s

• [SLOW TEST:77.764 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:15:03.001: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 22 21:15:03.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-dg5wq'
Jan 22 21:15:03.317: INFO: stderr: ""
Jan 22 21:15:03.317: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 22 21:15:04.321: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 21:15:04.321: INFO: Found 0 / 1
Jan 22 21:15:05.321: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 21:15:05.321: INFO: Found 1 / 1
Jan 22 21:15:05.321: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 22 21:15:05.324: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 21:15:05.325: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 22 21:15:05.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 patch pod redis-master-sdhcn --namespace=e2e-tests-kubectl-dg5wq -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 22 21:15:05.404: INFO: stderr: ""
Jan 22 21:15:05.404: INFO: stdout: "pod/redis-master-sdhcn patched\n"
STEP: checking annotations
Jan 22 21:15:05.408: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 21:15:05.408: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:15:05.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dg5wq" for this suite.
Jan 22 21:15:27.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:15:27.454: INFO: namespace: e2e-tests-kubectl-dg5wq, resource: bindings, ignored listing per whitelist
Jan 22 21:15:27.512: INFO: namespace e2e-tests-kubectl-dg5wq deletion completed in 22.100571323s

• [SLOW TEST:24.511 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:15:27.513: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d7299f63-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:15:27.573: INFO: Waiting up to 5m0s for pod "pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-892bg" to be "success or failure"
Jan 22 21:15:27.576: INFO: Pod "pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.006277ms
Jan 22 21:15:29.582: INFO: Pod "pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008841223s
STEP: Saw pod success
Jan 22 21:15:29.582: INFO: Pod "pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:15:29.588: INFO: Trying to get logs from node secconf-node pod pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:15:29.605: INFO: Waiting for pod pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:15:29.609: INFO: Pod pod-secrets-d72a0f0d-1e8a-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:15:29.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-892bg" for this suite.
Jan 22 21:15:35.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:15:35.685: INFO: namespace: e2e-tests-secrets-892bg, resource: bindings, ignored listing per whitelist
Jan 22 21:15:35.706: INFO: namespace e2e-tests-secrets-892bg deletion completed in 6.092131566s

• [SLOW TEST:8.193 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:15:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-dc0dbce9-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating configMap with name cm-test-opt-upd-dc0dbd10-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-dc0dbce9-1e8a-11e9-9284-e2fd7d97dccb
STEP: Updating configmap cm-test-opt-upd-dc0dbd10-1e8a-11e9-9284-e2fd7d97dccb
STEP: Creating configMap with name cm-test-opt-create-dc0dbd1d-1e8a-11e9-9284-e2fd7d97dccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:16:54.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jhs25" for this suite.
Jan 22 21:17:16.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:17:16.269: INFO: namespace: e2e-tests-configmap-jhs25, resource: bindings, ignored listing per whitelist
Jan 22 21:17:16.317: INFO: namespace e2e-tests-configmap-jhs25 deletion completed in 22.088254357s

• [SLOW TEST:100.611 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:17:16.317: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 22 21:17:16.378: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 22 21:17:16.385: INFO: Waiting for terminating namespaces to be deleted...
Jan 22 21:17:16.387: INFO: 
Logging pods the kubelet thinks is on node secconf-node before test
Jan 22 21:17:16.393: INFO: calico-node-6j2nx from kube-system started at 2019-01-21 09:37:56 +0000 UTC (2 container statuses recorded)
Jan 22 21:17:16.393: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 21:17:16.393: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 21:17:16.393: INFO: kube-proxy-6m8wc from kube-system started at 2019-01-21 09:37:56 +0000 UTC (1 container statuses recorded)
Jan 22 21:17:16.393: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 21:17:16.393: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn from heptio-sonobuoy started at 2019-01-22 21:07:14 +0000 UTC (2 container statuses recorded)
Jan 22 21:17:16.393: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 21:17:16.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 21:17:16.393: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (1 container statuses recorded)
Jan 22 21:17:16.393: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 21:17:16.393: INFO: sonobuoy-e2e-job-98d427a1d3c0481f from heptio-sonobuoy started at 2019-01-22 21:07:14 +0000 UTC (2 container statuses recorded)
Jan 22 21:17:16.393: INFO: 	Container e2e ready: true, restart count 0
Jan 22 21:17:16.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node secconf-node
Jan 22 21:17:16.414: INFO: Pod sonobuoy requesting resource cpu=0m on Node secconf-node
Jan 22 21:17:16.414: INFO: Pod sonobuoy-e2e-job-98d427a1d3c0481f requesting resource cpu=0m on Node secconf-node
Jan 22 21:17:16.414: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn requesting resource cpu=0m on Node secconf-node
Jan 22 21:17:16.414: INFO: Pod calico-node-6j2nx requesting resource cpu=250m on Node secconf-node
Jan 22 21:17:16.414: INFO: Pod kube-proxy-6m8wc requesting resource cpu=0m on Node secconf-node
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180a9541-1e8b-11e9-9284-e2fd7d97dccb.157c4845c68381ec], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-4zvxh/filler-pod-180a9541-1e8b-11e9-9284-e2fd7d97dccb to secconf-node]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180a9541-1e8b-11e9-9284-e2fd7d97dccb.157c4845ec0d36b2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180a9541-1e8b-11e9-9284-e2fd7d97dccb.157c4845edd1e65a], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-180a9541-1e8b-11e9-9284-e2fd7d97dccb.157c4845f78da2ea], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.157c48463e952dee], Reason = [FailedScheduling], Message = [0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node secconf-node
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:17:19.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-4zvxh" for this suite.
Jan 22 21:17:25.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:17:25.581: INFO: namespace: e2e-tests-sched-pred-4zvxh, resource: bindings, ignored listing per whitelist
Jan 22 21:17:25.585: INFO: namespace e2e-tests-sched-pred-4zvxh deletion completed in 6.121944499s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.269 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:17:25.586: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:17:25.670: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-xbnml" to be "success or failure"
Jan 22 21:17:25.674: INFO: Pod "downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811635ms
Jan 22 21:17:27.678: INFO: Pod "downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007418563s
STEP: Saw pod success
Jan 22 21:17:27.678: INFO: Pod "downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:17:27.681: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:17:27.704: INFO: Waiting for pod downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:17:27.708: INFO: Pod downwardapi-volume-1d8df771-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:17:27.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xbnml" for this suite.
Jan 22 21:17:33.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:17:33.750: INFO: namespace: e2e-tests-downward-api-xbnml, resource: bindings, ignored listing per whitelist
Jan 22 21:17:33.801: INFO: namespace e2e-tests-downward-api-xbnml deletion completed in 6.08923791s

• [SLOW TEST:8.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:17:33.802: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 22 21:17:33.862: INFO: Waiting up to 5m0s for pod "downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-jnbk6" to be "success or failure"
Jan 22 21:17:33.865: INFO: Pod "downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7176ms
Jan 22 21:17:35.868: INFO: Pod "downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006189328s
STEP: Saw pod success
Jan 22 21:17:35.868: INFO: Pod "downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:17:35.871: INFO: Trying to get logs from node secconf-node pod downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 21:17:35.903: INFO: Waiting for pod downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:17:35.906: INFO: Pod downward-api-22701684-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:17:35.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jnbk6" for this suite.
Jan 22 21:17:41.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:17:41.994: INFO: namespace: e2e-tests-downward-api-jnbk6, resource: bindings, ignored listing per whitelist
Jan 22 21:17:42.000: INFO: namespace e2e-tests-downward-api-jnbk6 deletion completed in 6.089461818s

• [SLOW TEST:8.198 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:17:42.000: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-2753c44e-1e8b-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:17:42.068: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-v82wg" to be "success or failure"
Jan 22 21:17:42.072: INFO: Pod "pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108708ms
Jan 22 21:17:44.076: INFO: Pod "pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007667507s
STEP: Saw pod success
Jan 22 21:17:44.077: INFO: Pod "pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:17:44.079: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:17:44.096: INFO: Waiting for pod pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:17:44.100: INFO: Pod pod-projected-secrets-275448d2-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:17:44.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v82wg" for this suite.
Jan 22 21:17:50.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:17:50.251: INFO: namespace: e2e-tests-projected-v82wg, resource: bindings, ignored listing per whitelist
Jan 22 21:17:50.273: INFO: namespace e2e-tests-projected-v82wg deletion completed in 6.169061985s

• [SLOW TEST:8.273 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:17:50.273: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:17:50.350: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:17:51.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-n4ljg" for this suite.
Jan 22 21:17:57.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:17:57.526: INFO: namespace: e2e-tests-custom-resource-definition-n4ljg, resource: bindings, ignored listing per whitelist
Jan 22 21:17:57.577: INFO: namespace e2e-tests-custom-resource-definition-n4ljg deletion completed in 6.161311193s

• [SLOW TEST:7.304 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:17:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-30a52c74-1e8b-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:17:57.704: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-9gt8m" to be "success or failure"
Jan 22 21:17:57.709: INFO: Pod "pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376166ms
Jan 22 21:17:59.714: INFO: Pod "pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00977838s
STEP: Saw pod success
Jan 22 21:17:59.714: INFO: Pod "pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:17:59.718: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:17:59.754: INFO: Waiting for pod pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:17:59.760: INFO: Pod pod-projected-configmaps-30a5d122-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:17:59.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9gt8m" for this suite.
Jan 22 21:18:05.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:18:05.833: INFO: namespace: e2e-tests-projected-9gt8m, resource: bindings, ignored listing per whitelist
Jan 22 21:18:05.882: INFO: namespace e2e-tests-projected-9gt8m deletion completed in 6.113819104s

• [SLOW TEST:8.305 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:18:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:18:12.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-6b784" for this suite.
Jan 22 21:18:18.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:18:18.062: INFO: namespace: e2e-tests-namespaces-6b784, resource: bindings, ignored listing per whitelist
Jan 22 21:18:18.204: INFO: namespace e2e-tests-namespaces-6b784 deletion completed in 6.190256155s
STEP: Destroying namespace "e2e-tests-nsdeletetest-8hnzw" for this suite.
Jan 22 21:18:18.208: INFO: Namespace e2e-tests-nsdeletetest-8hnzw was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-brl8k" for this suite.
Jan 22 21:18:24.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:18:24.252: INFO: namespace: e2e-tests-nsdeletetest-brl8k, resource: bindings, ignored listing per whitelist
Jan 22 21:18:24.317: INFO: namespace e2e-tests-nsdeletetest-brl8k deletion completed in 6.10899088s

• [SLOW TEST:18.435 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:18:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:18:24.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-dschd" for this suite.
Jan 22 21:18:30.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:18:30.531: INFO: namespace: e2e-tests-kubelet-test-dschd, resource: bindings, ignored listing per whitelist
Jan 22 21:18:30.541: INFO: namespace e2e-tests-kubelet-test-dschd deletion completed in 6.102373562s

• [SLOW TEST:6.223 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:18:30.541: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jan 22 21:18:32.665: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-4448d9b5-1e8b-11e9-9284-e2fd7d97dccb", GenerateName:"", Namespace:"e2e-tests-pods-zpgmx", SelfLink:"/api/v1/namespaces/e2e-tests-pods-zpgmx/pods/pod-submit-remove-4448d9b5-1e8b-11e9-9284-e2fd7d97dccb", UID:"444a567f-1e8b-11e9-945c-000c293afc9e", ResourceVersion:"30572", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63683788710, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"642325681"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.100.1.113/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rrxfc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002038f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rrxfc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001fdd848), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"secconf-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001cf7500), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fdd890)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fdd8b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001fdd8b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001fdd8bc)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683788710, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683788712, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683788712, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683788710, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.43.101", PodIP:"100.100.1.113", StartTime:(*v1.Time)(0xc001c1a1c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001c1a1e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/nginx:1.14-alpine", ImageID:"docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96", ContainerID:"docker://63a8c3447aff3a3723f1cad702862148fc2cca537f20b75b43fb74e43019200a"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 22 21:18:37.683: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:18:37.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zpgmx" for this suite.
Jan 22 21:18:43.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:18:43.729: INFO: namespace: e2e-tests-pods-zpgmx, resource: bindings, ignored listing per whitelist
Jan 22 21:18:43.809: INFO: namespace e2e-tests-pods-zpgmx deletion completed in 6.118508853s

• [SLOW TEST:13.268 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:18:43.809: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:19:07.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-bbhmt" for this suite.
Jan 22 21:19:13.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:19:13.131: INFO: namespace: e2e-tests-container-runtime-bbhmt, resource: bindings, ignored listing per whitelist
Jan 22 21:19:13.160: INFO: namespace e2e-tests-container-runtime-bbhmt deletion completed in 6.082473368s

• [SLOW TEST:29.351 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:19:13.160: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:19:13.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-j2frx" to be "success or failure"
Jan 22 21:19:13.227: INFO: Pod "downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.849864ms
Jan 22 21:19:15.230: INFO: Pod "downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006376653s
STEP: Saw pod success
Jan 22 21:19:15.230: INFO: Pod "downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:19:15.233: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:19:15.254: INFO: Waiting for pod downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:19:15.261: INFO: Pod downwardapi-volume-5da9847b-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:19:15.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j2frx" for this suite.
Jan 22 21:19:21.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:19:21.328: INFO: namespace: e2e-tests-projected-j2frx, resource: bindings, ignored listing per whitelist
Jan 22 21:19:21.378: INFO: namespace e2e-tests-projected-j2frx deletion completed in 6.11160585s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:19:21.379: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-t669
STEP: Creating a pod to test atomic-volume-subpath
Jan 22 21:19:21.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-t669" in namespace "e2e-tests-subpath-b4g8p" to be "success or failure"
Jan 22 21:19:21.479: INFO: Pod "pod-subpath-test-projected-t669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.797833ms
Jan 22 21:19:23.483: INFO: Pod "pod-subpath-test-projected-t669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006923919s
Jan 22 21:19:25.487: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 4.010891135s
Jan 22 21:19:27.490: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 6.013640766s
Jan 22 21:19:29.494: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 8.018119855s
Jan 22 21:19:31.499: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 10.022645348s
Jan 22 21:19:33.503: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 12.027090978s
Jan 22 21:19:35.506: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 14.030299715s
Jan 22 21:19:37.510: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 16.033697927s
Jan 22 21:19:39.514: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 18.03800016s
Jan 22 21:19:41.517: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 20.040806444s
Jan 22 21:19:43.521: INFO: Pod "pod-subpath-test-projected-t669": Phase="Running", Reason="", readiness=false. Elapsed: 22.04504711s
Jan 22 21:19:45.526: INFO: Pod "pod-subpath-test-projected-t669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049589918s
STEP: Saw pod success
Jan 22 21:19:45.526: INFO: Pod "pod-subpath-test-projected-t669" satisfied condition "success or failure"
Jan 22 21:19:45.528: INFO: Trying to get logs from node secconf-node pod pod-subpath-test-projected-t669 container test-container-subpath-projected-t669: <nil>
STEP: delete the pod
Jan 22 21:19:45.547: INFO: Waiting for pod pod-subpath-test-projected-t669 to disappear
Jan 22 21:19:45.553: INFO: Pod pod-subpath-test-projected-t669 no longer exists
STEP: Deleting pod pod-subpath-test-projected-t669
Jan 22 21:19:45.553: INFO: Deleting pod "pod-subpath-test-projected-t669" in namespace "e2e-tests-subpath-b4g8p"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:19:45.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-b4g8p" for this suite.
Jan 22 21:19:51.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:19:51.650: INFO: namespace: e2e-tests-subpath-b4g8p, resource: bindings, ignored listing per whitelist
Jan 22 21:19:51.650: INFO: namespace e2e-tests-subpath-b4g8p deletion completed in 6.087434406s

• [SLOW TEST:30.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:19:51.650: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-7499ea26-1e8b-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:19:51.711: INFO: Waiting up to 5m0s for pod "pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-stk45" to be "success or failure"
Jan 22 21:19:51.714: INFO: Pod "pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814589ms
Jan 22 21:19:53.716: INFO: Pod "pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005293795s
STEP: Saw pod success
Jan 22 21:19:53.716: INFO: Pod "pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:19:53.719: INFO: Trying to get logs from node secconf-node pod pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:19:53.738: INFO: Waiting for pod pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:19:53.742: INFO: Pod pod-secrets-749a4d8d-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:19:53.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-stk45" for this suite.
Jan 22 21:19:59.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:19:59.863: INFO: namespace: e2e-tests-secrets-stk45, resource: bindings, ignored listing per whitelist
Jan 22 21:19:59.877: INFO: namespace e2e-tests-secrets-stk45 deletion completed in 6.13073594s

• [SLOW TEST:8.228 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:19:59.877: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 22 21:19:59.944: INFO: Waiting up to 5m0s for pod "pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-fp8wp" to be "success or failure"
Jan 22 21:19:59.948: INFO: Pod "pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705727ms
Jan 22 21:20:01.952: INFO: Pod "pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007741904s
STEP: Saw pod success
Jan 22 21:20:01.953: INFO: Pod "pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:20:01.956: INFO: Trying to get logs from node secconf-node pod pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:20:01.973: INFO: Waiting for pod pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:20:01.977: INFO: Pod pod-798272a1-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:20:01.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fp8wp" for this suite.
Jan 22 21:20:07.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:20:08.024: INFO: namespace: e2e-tests-emptydir-fp8wp, resource: bindings, ignored listing per whitelist
Jan 22 21:20:08.067: INFO: namespace e2e-tests-emptydir-fp8wp deletion completed in 6.085469782s

• [SLOW TEST:8.190 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:20:08.067: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 22 21:20:08.132: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30915,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 22 21:20:08.132: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30915,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 22 21:20:18.139: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30929,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 22 21:20:18.139: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30929,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 22 21:20:28.147: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30944,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 22 21:20:28.147: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30944,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 22 21:20:38.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30958,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 22 21:20:38.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-a,UID:7e64a6c6-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30958,Generation:0,CreationTimestamp:2019-01-22 21:20:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 22 21:20:48.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-b,UID:96400a5e-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30971,Generation:0,CreationTimestamp:2019-01-22 21:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 22 21:20:48.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-b,UID:96400a5e-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30971,Generation:0,CreationTimestamp:2019-01-22 21:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 22 21:20:58.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-b,UID:96400a5e-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30985,Generation:0,CreationTimestamp:2019-01-22 21:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 22 21:20:58.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-5gzlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-5gzlh/configmaps/e2e-watch-test-configmap-b,UID:96400a5e-1e8b-11e9-945c-000c293afc9e,ResourceVersion:30985,Generation:0,CreationTimestamp:2019-01-22 21:20:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:21:08.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-5gzlh" for this suite.
Jan 22 21:21:14.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:21:14.216: INFO: namespace: e2e-tests-watch-5gzlh, resource: bindings, ignored listing per whitelist
Jan 22 21:21:14.258: INFO: namespace e2e-tests-watch-5gzlh deletion completed in 6.088187795s

• [SLOW TEST:66.191 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:21:14.258: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:21:16.354: INFO: Waiting up to 5m0s for pod "client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-pods-mfkgx" to be "success or failure"
Jan 22 21:21:16.362: INFO: Pod "client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.194358ms
Jan 22 21:21:18.367: INFO: Pod "client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013171794s
STEP: Saw pod success
Jan 22 21:21:18.367: INFO: Pod "client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:21:18.370: INFO: Trying to get logs from node secconf-node pod client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb container env3cont: <nil>
STEP: delete the pod
Jan 22 21:21:18.400: INFO: Waiting for pod client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:21:18.405: INFO: Pod client-envvars-a70dadda-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:21:18.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-mfkgx" for this suite.
Jan 22 21:22:02.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:22:02.468: INFO: namespace: e2e-tests-pods-mfkgx, resource: bindings, ignored listing per whitelist
Jan 22 21:22:02.515: INFO: namespace e2e-tests-pods-mfkgx deletion completed in 44.100426435s

• [SLOW TEST:48.256 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:22:02.515: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:22:02.605: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 22 21:22:07.609: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 22 21:22:07.610: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 22 21:22:07.626: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-xpn82,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xpn82/deployments/test-cleanup-deployment,UID:c59c8315-1e8b-11e9-945c-000c293afc9e,ResourceVersion:31148,Generation:1,CreationTimestamp:2019-01-22 21:22:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jan 22 21:22:07.631: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 22 21:22:07.631: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 22 21:22:07.631: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-xpn82,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xpn82/replicasets/test-cleanup-controller,UID:c29eee76-1e8b-11e9-945c-000c293afc9e,ResourceVersion:31150,Generation:1,CreationTimestamp:2019-01-22 21:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c59c8315-1e8b-11e9-945c-000c293afc9e 0xc000f3e907 0xc000f3e908}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 22 21:22:07.640: INFO: Pod "test-cleanup-controller-m245z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-m245z,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-xpn82,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xpn82/pods/test-cleanup-controller-m245z,UID:c2a0b5bd-1e8b-11e9-945c-000c293afc9e,ResourceVersion:31142,Generation:0,CreationTimestamp:2019-01-22 21:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.123/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller c29eee76-1e8b-11e9-945c-000c293afc9e 0xc001a6dff7 0xc001a6dff8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fccd2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fccd2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fccd2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a9c0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a9c110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:22:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:22:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:22:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:22:02 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.123,StartTime:2019-01-22 21:22:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 21:22:03 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://6689679801e0c0579f2ecf8d1af00770824247cfd2b82d4a31e1b1eaa2d09509}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:22:07.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xpn82" for this suite.
Jan 22 21:22:13.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:22:13.683: INFO: namespace: e2e-tests-deployment-xpn82, resource: bindings, ignored listing per whitelist
Jan 22 21:22:13.742: INFO: namespace e2e-tests-deployment-xpn82 deletion completed in 6.093707232s

• [SLOW TEST:11.227 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:22:13.742: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 22 21:22:13.796: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:22:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-5shxq" for this suite.
Jan 22 21:22:22.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:22:22.449: INFO: namespace: e2e-tests-init-container-5shxq, resource: bindings, ignored listing per whitelist
Jan 22 21:22:22.499: INFO: namespace e2e-tests-init-container-5shxq deletion completed in 6.094254744s

• [SLOW TEST:8.757 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:22:22.499: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 22 21:22:26.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:26.596: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:28.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:28.601: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:30.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:30.600: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:32.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:32.601: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:34.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:34.600: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:36.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:36.600: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:38.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:38.601: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:40.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:40.600: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:42.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:42.600: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:44.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:44.600: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 22 21:22:46.597: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 22 21:22:46.600: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:22:46.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-bqfzg" for this suite.
Jan 22 21:23:08.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:23:08.691: INFO: namespace: e2e-tests-container-lifecycle-hook-bqfzg, resource: bindings, ignored listing per whitelist
Jan 22 21:23:08.702: INFO: namespace e2e-tests-container-lifecycle-hook-bqfzg deletion completed in 22.08916934s

• [SLOW TEST:46.203 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:23:08.702: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Jan 22 21:23:08.764: INFO: Waiting up to 5m0s for pod "client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-containers-swctb" to be "success or failure"
Jan 22 21:23:08.769: INFO: Pod "client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.833721ms
Jan 22 21:23:10.772: INFO: Pod "client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007480898s
STEP: Saw pod success
Jan 22 21:23:10.772: INFO: Pod "client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:23:10.774: INFO: Trying to get logs from node secconf-node pod client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:23:10.788: INFO: Waiting for pod client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:23:10.792: INFO: Pod client-containers-ea0e2999-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:23:10.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-swctb" for this suite.
Jan 22 21:23:16.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:23:16.883: INFO: namespace: e2e-tests-containers-swctb, resource: bindings, ignored listing per whitelist
Jan 22 21:23:16.891: INFO: namespace e2e-tests-containers-swctb deletion completed in 6.095801257s

• [SLOW TEST:8.189 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:23:16.891: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 22 21:23:16.958: INFO: Waiting up to 5m0s for pod "pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-bfprz" to be "success or failure"
Jan 22 21:23:16.961: INFO: Pod "pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.775294ms
Jan 22 21:23:18.964: INFO: Pod "pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006224879s
STEP: Saw pod success
Jan 22 21:23:18.964: INFO: Pod "pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:23:18.967: INFO: Trying to get logs from node secconf-node pod pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:23:18.985: INFO: Waiting for pod pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:23:18.988: INFO: Pod pod-eef079a8-1e8b-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:23:18.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bfprz" for this suite.
Jan 22 21:23:25.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:23:25.053: INFO: namespace: e2e-tests-emptydir-bfprz, resource: bindings, ignored listing per whitelist
Jan 22 21:23:25.087: INFO: namespace e2e-tests-emptydir-bfprz deletion completed in 6.093287724s

• [SLOW TEST:8.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:23:25.087: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 22 21:23:27.681: INFO: Successfully updated pod "annotationupdatef3d2fcd9-1e8b-11e9-9284-e2fd7d97dccb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:23:31.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w79wj" for this suite.
Jan 22 21:23:53.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:23:53.771: INFO: namespace: e2e-tests-projected-w79wj, resource: bindings, ignored listing per whitelist
Jan 22 21:23:53.815: INFO: namespace e2e-tests-projected-w79wj deletion completed in 22.101569454s

• [SLOW TEST:28.728 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:23:53.815: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Jan 22 21:23:54.397: INFO: created pod pod-service-account-defaultsa
Jan 22 21:23:54.397: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 22 21:23:54.402: INFO: created pod pod-service-account-mountsa
Jan 22 21:23:54.402: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 22 21:23:54.406: INFO: created pod pod-service-account-nomountsa
Jan 22 21:23:54.406: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 22 21:23:54.411: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 22 21:23:54.411: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 22 21:23:54.417: INFO: created pod pod-service-account-mountsa-mountspec
Jan 22 21:23:54.417: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 22 21:23:54.425: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 22 21:23:54.425: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 22 21:23:54.433: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 22 21:23:54.433: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 22 21:23:54.439: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 22 21:23:54.439: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 22 21:23:54.450: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 22 21:23:54.450: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:23:54.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-g9bqs" for this suite.
Jan 22 21:24:16.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:24:16.484: INFO: namespace: e2e-tests-svcaccounts-g9bqs, resource: bindings, ignored listing per whitelist
Jan 22 21:24:16.543: INFO: namespace e2e-tests-svcaccounts-g9bqs deletion completed in 22.089072873s

• [SLOW TEST:22.728 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:24:16.544: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:24:16.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-s5vw9" to be "success or failure"
Jan 22 21:24:16.619: INFO: Pod "downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038436ms
Jan 22 21:24:18.625: INFO: Pod "downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009574082s
STEP: Saw pod success
Jan 22 21:24:18.625: INFO: Pod "downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:24:18.629: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:24:18.711: INFO: Waiting for pod downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:24:18.714: INFO: Pod downwardapi-volume-127f6c4f-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:24:18.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s5vw9" for this suite.
Jan 22 21:24:24.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:24:24.798: INFO: namespace: e2e-tests-projected-s5vw9, resource: bindings, ignored listing per whitelist
Jan 22 21:24:24.840: INFO: namespace e2e-tests-projected-s5vw9 deletion completed in 6.121036184s

• [SLOW TEST:8.297 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:24:24.841: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:24:24.973: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1776810e-1e8c-11e9-945c-000c293afc9e", Controller:(*bool)(0xc001e2d9e2), BlockOwnerDeletion:(*bool)(0xc001e2d9e3)}}
Jan 22 21:24:24.982: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"177512ba-1e8c-11e9-945c-000c293afc9e", Controller:(*bool)(0xc001e2dc32), BlockOwnerDeletion:(*bool)(0xc001e2dc33)}}
Jan 22 21:24:24.994: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1775ad74-1e8c-11e9-945c-000c293afc9e", Controller:(*bool)(0xc001f31fbe), BlockOwnerDeletion:(*bool)(0xc001f31fbf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:24:30.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xnt2d" for this suite.
Jan 22 21:24:36.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:24:36.093: INFO: namespace: e2e-tests-gc-xnt2d, resource: bindings, ignored listing per whitelist
Jan 22 21:24:36.098: INFO: namespace e2e-tests-gc-xnt2d deletion completed in 6.088524533s

• [SLOW TEST:11.257 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:24:36.098: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-74z27
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-74z27
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-74z27
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-74z27
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-74z27
Jan 22 21:24:38.208: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-74z27, name: ss-0, uid: 1f203e80-1e8c-11e9-945c-000c293afc9e, status phase: Pending. Waiting for statefulset controller to delete.
Jan 22 21:24:38.788: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-74z27, name: ss-0, uid: 1f203e80-1e8c-11e9-945c-000c293afc9e, status phase: Failed. Waiting for statefulset controller to delete.
Jan 22 21:24:38.793: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-74z27, name: ss-0, uid: 1f203e80-1e8c-11e9-945c-000c293afc9e, status phase: Failed. Waiting for statefulset controller to delete.
Jan 22 21:24:38.796: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-74z27
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-74z27
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-74z27 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 22 21:24:42.825: INFO: Deleting all statefulset in ns e2e-tests-statefulset-74z27
Jan 22 21:24:42.829: INFO: Scaling statefulset ss to 0
Jan 22 21:24:52.847: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 21:24:52.850: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:24:52.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-74z27" for this suite.
Jan 22 21:24:58.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:24:58.924: INFO: namespace: e2e-tests-statefulset-74z27, resource: bindings, ignored listing per whitelist
Jan 22 21:24:58.976: INFO: namespace e2e-tests-statefulset-74z27 deletion completed in 6.098502519s

• [SLOW TEST:22.878 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:24:58.976: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jan 22 21:24:59.609: INFO: Waiting up to 5m0s for pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th" in namespace "e2e-tests-svcaccounts-7nckv" to be "success or failure"
Jan 22 21:24:59.612: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th": Phase="Pending", Reason="", readiness=false. Elapsed: 3.644995ms
Jan 22 21:25:01.617: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008494316s
Jan 22 21:25:03.620: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011776039s
STEP: Saw pod success
Jan 22 21:25:03.621: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th" satisfied condition "success or failure"
Jan 22 21:25:03.624: INFO: Trying to get logs from node secconf-node pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th container token-test: <nil>
STEP: delete the pod
Jan 22 21:25:03.647: INFO: Waiting for pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th to disappear
Jan 22 21:25:03.650: INFO: Pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-9f9th no longer exists
STEP: Creating a pod to test consume service account root CA
Jan 22 21:25:03.655: INFO: Waiting up to 5m0s for pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf" in namespace "e2e-tests-svcaccounts-7nckv" to be "success or failure"
Jan 22 21:25:03.659: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094555ms
Jan 22 21:25:05.664: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008578071s
STEP: Saw pod success
Jan 22 21:25:05.664: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf" satisfied condition "success or failure"
Jan 22 21:25:05.669: INFO: Trying to get logs from node secconf-node pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf container root-ca-test: <nil>
STEP: delete the pod
Jan 22 21:25:05.698: INFO: Waiting for pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf to disappear
Jan 22 21:25:05.702: INFO: Pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-cf2cf no longer exists
STEP: Creating a pod to test consume service account namespace
Jan 22 21:25:05.711: INFO: Waiting up to 5m0s for pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr" in namespace "e2e-tests-svcaccounts-7nckv" to be "success or failure"
Jan 22 21:25:05.721: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.065887ms
Jan 22 21:25:07.724: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012965666s
STEP: Saw pod success
Jan 22 21:25:07.724: INFO: Pod "pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr" satisfied condition "success or failure"
Jan 22 21:25:07.726: INFO: Trying to get logs from node secconf-node pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr container namespace-test: <nil>
STEP: delete the pod
Jan 22 21:25:07.743: INFO: Waiting for pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr to disappear
Jan 22 21:25:07.746: INFO: Pod pod-service-account-2c1fb2b7-1e8c-11e9-9284-e2fd7d97dccb-nzjjr no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:25:07.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-7nckv" for this suite.
Jan 22 21:25:13.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:25:13.787: INFO: namespace: e2e-tests-svcaccounts-7nckv, resource: bindings, ignored listing per whitelist
Jan 22 21:25:13.829: INFO: namespace e2e-tests-svcaccounts-7nckv deletion completed in 6.078563573s

• [SLOW TEST:14.854 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:25:13.829: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:25:15.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-rvvwm" for this suite.
Jan 22 21:25:21.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:25:22.001: INFO: namespace: e2e-tests-emptydir-wrapper-rvvwm, resource: bindings, ignored listing per whitelist
Jan 22 21:25:22.017: INFO: namespace e2e-tests-emptydir-wrapper-rvvwm deletion completed in 6.086882285s

• [SLOW TEST:8.188 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:25:22.017: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 22 21:25:28.179: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:25:28.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-5bhpp" for this suite.
Jan 22 21:25:34.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:25:34.270: INFO: namespace: e2e-tests-gc-5bhpp, resource: bindings, ignored listing per whitelist
Jan 22 21:25:34.271: INFO: namespace e2e-tests-gc-5bhpp deletion completed in 6.086124502s

• [SLOW TEST:12.253 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:25:34.271: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-40d29333-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:25:34.339: INFO: Waiting up to 5m0s for pod "pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-jpphm" to be "success or failure"
Jan 22 21:25:34.347: INFO: Pod "pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.817351ms
Jan 22 21:25:36.353: INFO: Pod "pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014055258s
STEP: Saw pod success
Jan 22 21:25:36.353: INFO: Pod "pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:25:36.356: INFO: Trying to get logs from node secconf-node pod pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:25:36.381: INFO: Waiting for pod pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:25:36.388: INFO: Pod pod-configmaps-40d31d1d-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:25:36.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jpphm" for this suite.
Jan 22 21:25:42.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:25:42.462: INFO: namespace: e2e-tests-configmap-jpphm, resource: bindings, ignored listing per whitelist
Jan 22 21:25:42.478: INFO: namespace e2e-tests-configmap-jpphm deletion completed in 6.083645571s

• [SLOW TEST:8.208 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:25:42.478: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-45b65faa-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-45b65faa-1e8c-11e9-9284-e2fd7d97dccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:25:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cf9qb" for this suite.
Jan 22 21:26:08.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:26:08.637: INFO: namespace: e2e-tests-projected-cf9qb, resource: bindings, ignored listing per whitelist
Jan 22 21:26:08.680: INFO: namespace e2e-tests-projected-cf9qb deletion completed in 22.087926592s

• [SLOW TEST:26.201 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:26:08.680: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-5554aef9-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:26:08.745: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-4gd6p" to be "success or failure"
Jan 22 21:26:08.747: INFO: Pod "pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233421ms
Jan 22 21:26:10.752: INFO: Pod "pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007032346s
STEP: Saw pod success
Jan 22 21:26:10.753: INFO: Pod "pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:26:10.755: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:26:10.773: INFO: Waiting for pod pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:26:10.777: INFO: Pod pod-projected-configmaps-55551962-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:26:10.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4gd6p" for this suite.
Jan 22 21:26:16.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:26:16.841: INFO: namespace: e2e-tests-projected-4gd6p, resource: bindings, ignored listing per whitelist
Jan 22 21:26:16.864: INFO: namespace e2e-tests-projected-4gd6p deletion completed in 6.08351495s

• [SLOW TEST:8.184 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:26:16.864: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Jan 22 21:26:16.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 cluster-info'
Jan 22 21:26:17.053: INFO: stderr: ""
Jan 22 21:26:17.053: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:26:17.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6zdtn" for this suite.
Jan 22 21:26:23.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:26:23.146: INFO: namespace: e2e-tests-kubectl-6zdtn, resource: bindings, ignored listing per whitelist
Jan 22 21:26:23.154: INFO: namespace e2e-tests-kubectl-6zdtn deletion completed in 6.097616538s

• [SLOW TEST:6.290 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:26:23.154: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:26:25.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-l8xtk" for this suite.
Jan 22 21:27:03.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:27:03.335: INFO: namespace: e2e-tests-kubelet-test-l8xtk, resource: bindings, ignored listing per whitelist
Jan 22 21:27:03.348: INFO: namespace e2e-tests-kubelet-test-l8xtk deletion completed in 38.091001553s

• [SLOW TEST:40.194 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:27:03.348: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:27:03.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-lv4nw" to be "success or failure"
Jan 22 21:27:03.415: INFO: Pod "downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.978802ms
Jan 22 21:27:05.419: INFO: Pod "downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006957462s
STEP: Saw pod success
Jan 22 21:27:05.420: INFO: Pod "downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:27:05.422: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:27:05.437: INFO: Waiting for pod downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:27:05.440: INFO: Pod downwardapi-volume-75eaa301-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:27:05.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lv4nw" for this suite.
Jan 22 21:27:11.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:27:11.519: INFO: namespace: e2e-tests-projected-lv4nw, resource: bindings, ignored listing per whitelist
Jan 22 21:27:11.534: INFO: namespace e2e-tests-projected-lv4nw deletion completed in 6.091000836s

• [SLOW TEST:8.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:27:11.534: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-wglr
STEP: Creating a pod to test atomic-volume-subpath
Jan 22 21:27:11.595: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wglr" in namespace "e2e-tests-subpath-f89pj" to be "success or failure"
Jan 22 21:27:11.598: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998338ms
Jan 22 21:27:13.604: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 2.008774919s
Jan 22 21:27:15.609: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 4.013755996s
Jan 22 21:27:17.613: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 6.017562491s
Jan 22 21:27:19.616: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 8.020821758s
Jan 22 21:27:21.619: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 10.02373932s
Jan 22 21:27:23.622: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 12.026597786s
Jan 22 21:27:25.626: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 14.031238291s
Jan 22 21:27:27.630: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 16.034772307s
Jan 22 21:27:29.635: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 18.040231137s
Jan 22 21:27:31.639: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Running", Reason="", readiness=false. Elapsed: 20.044509783s
Jan 22 21:27:33.643: INFO: Pod "pod-subpath-test-configmap-wglr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.048116105s
STEP: Saw pod success
Jan 22 21:27:33.643: INFO: Pod "pod-subpath-test-configmap-wglr" satisfied condition "success or failure"
Jan 22 21:27:33.646: INFO: Trying to get logs from node secconf-node pod pod-subpath-test-configmap-wglr container test-container-subpath-configmap-wglr: <nil>
STEP: delete the pod
Jan 22 21:27:33.668: INFO: Waiting for pod pod-subpath-test-configmap-wglr to disappear
Jan 22 21:27:33.670: INFO: Pod pod-subpath-test-configmap-wglr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wglr
Jan 22 21:27:33.670: INFO: Deleting pod "pod-subpath-test-configmap-wglr" in namespace "e2e-tests-subpath-f89pj"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:27:33.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-f89pj" for this suite.
Jan 22 21:27:39.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:27:39.745: INFO: namespace: e2e-tests-subpath-f89pj, resource: bindings, ignored listing per whitelist
Jan 22 21:27:39.771: INFO: namespace e2e-tests-subpath-f89pj deletion completed in 6.095114107s

• [SLOW TEST:28.237 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:27:39.771: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:27:39.828: INFO: Creating deployment "test-recreate-deployment"
Jan 22 21:27:39.831: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 22 21:27:39.836: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 22 21:27:41.843: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 22 21:27:41.847: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 22 21:27:41.854: INFO: Updating deployment test-recreate-deployment
Jan 22 21:27:41.854: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 22 21:27:41.911: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-9qcbj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9qcbj/deployments/test-recreate-deployment,UID:8ba05ddd-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32619,Generation:2,CreationTimestamp:2019-01-22 21:27:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-01-22 21:27:41 +0000 UTC 2019-01-22 21:27:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-01-22 21:27:41 +0000 UTC 2019-01-22 21:27:39 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jan 22 21:27:41.914: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-9qcbj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9qcbj/replicasets/test-recreate-deployment-697fbf54bf,UID:8cd8b87f-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32617,Generation:1,CreationTimestamp:2019-01-22 21:27:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8ba05ddd-1e8c-11e9-945c-000c293afc9e 0xc001d78637 0xc001d78638}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 22 21:27:41.914: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 22 21:27:41.914: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-9qcbj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9qcbj/replicasets/test-recreate-deployment-5dfdcc846d,UID:8ba17397-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32607,Generation:2,CreationTimestamp:2019-01-22 21:27:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8ba05ddd-1e8c-11e9-945c-000c293afc9e 0xc001d78577 0xc001d78578}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 22 21:27:41.917: INFO: Pod "test-recreate-deployment-697fbf54bf-4f2bp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-4f2bp,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-9qcbj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9qcbj/pods/test-recreate-deployment-697fbf54bf-4f2bp,UID:8cd94570-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32618,Generation:0,CreationTimestamp:2019-01-22 21:27:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 8cd8b87f-1e8c-11e9-945c-000c293afc9e 0xc0024bc237 0xc0024bc238}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-k28tp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k28tp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k28tp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024bc390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024bc3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:27:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:27:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:27:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:27:41 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:,StartTime:2019-01-22 21:27:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:27:41.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9qcbj" for this suite.
Jan 22 21:27:47.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:27:47.975: INFO: namespace: e2e-tests-deployment-9qcbj, resource: bindings, ignored listing per whitelist
Jan 22 21:27:48.050: INFO: namespace e2e-tests-deployment-9qcbj deletion completed in 6.129923815s

• [SLOW TEST:8.279 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:27:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 22 21:27:48.145: INFO: Waiting up to 5m0s for pod "pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-z85kr" to be "success or failure"
Jan 22 21:27:48.150: INFO: Pod "pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291465ms
Jan 22 21:27:50.154: INFO: Pod "pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008977786s
STEP: Saw pod success
Jan 22 21:27:50.155: INFO: Pod "pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:27:50.159: INFO: Trying to get logs from node secconf-node pod pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:27:50.182: INFO: Waiting for pod pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:27:50.191: INFO: Pod pod-90939ce7-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:27:50.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z85kr" for this suite.
Jan 22 21:27:56.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:27:56.240: INFO: namespace: e2e-tests-emptydir-z85kr, resource: bindings, ignored listing per whitelist
Jan 22 21:27:56.303: INFO: namespace e2e-tests-emptydir-z85kr deletion completed in 6.104439826s

• [SLOW TEST:8.253 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:27:56.303: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-957aaebf-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:27:56.367: INFO: Waiting up to 5m0s for pod "pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-4jhlt" to be "success or failure"
Jan 22 21:27:56.370: INFO: Pod "pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.807916ms
Jan 22 21:27:58.374: INFO: Pod "pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006489498s
STEP: Saw pod success
Jan 22 21:27:58.374: INFO: Pod "pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:27:58.379: INFO: Trying to get logs from node secconf-node pod pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:27:58.414: INFO: Waiting for pod pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:27:58.424: INFO: Pod pod-configmaps-957b1e21-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:27:58.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-4jhlt" for this suite.
Jan 22 21:28:04.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:28:04.465: INFO: namespace: e2e-tests-configmap-4jhlt, resource: bindings, ignored listing per whitelist
Jan 22 21:28:04.564: INFO: namespace e2e-tests-configmap-4jhlt deletion completed in 6.129612411s

• [SLOW TEST:8.261 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:28:04.564: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 22 21:28:04.660: INFO: Waiting up to 5m0s for pod "pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-tvjgm" to be "success or failure"
Jan 22 21:28:04.665: INFO: Pod "pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.152107ms
Jan 22 21:28:06.668: INFO: Pod "pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007949197s
STEP: Saw pod success
Jan 22 21:28:06.668: INFO: Pod "pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:28:06.671: INFO: Trying to get logs from node secconf-node pod pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:28:06.692: INFO: Waiting for pod pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:28:06.697: INFO: Pod pod-9a6c108c-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:28:06.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tvjgm" for this suite.
Jan 22 21:28:12.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:28:12.773: INFO: namespace: e2e-tests-emptydir-tvjgm, resource: bindings, ignored listing per whitelist
Jan 22 21:28:12.777: INFO: namespace e2e-tests-emptydir-tvjgm deletion completed in 6.076863786s

• [SLOW TEST:8.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:28:12.777: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:28:12.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 version'
Jan 22 21:28:12.894: INFO: stderr: ""
Jan 22 21:28:12.894: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.2\", GitCommit:\"cff46ab41ff0bb44d8584413b598ad8360ec1def\", GitTreeState:\"clean\", BuildDate:\"2019-01-10T23:28:14Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:28:12.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8gdjd" for this suite.
Jan 22 21:28:18.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:28:18.983: INFO: namespace: e2e-tests-kubectl-8gdjd, resource: bindings, ignored listing per whitelist
Jan 22 21:28:18.992: INFO: namespace e2e-tests-kubectl-8gdjd deletion completed in 6.094555729s

• [SLOW TEST:6.214 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:28:18.992: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-a2fff9df-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating secret with name s-test-opt-upd-a2fffa1f-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a2fff9df-1e8c-11e9-9284-e2fd7d97dccb
STEP: Updating secret s-test-opt-upd-a2fffa1f-1e8c-11e9-9284-e2fd7d97dccb
STEP: Creating secret with name s-test-opt-create-a2fffa34-1e8c-11e9-9284-e2fd7d97dccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:28:23.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-86xt9" for this suite.
Jan 22 21:28:45.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:28:45.278: INFO: namespace: e2e-tests-secrets-86xt9, resource: bindings, ignored listing per whitelist
Jan 22 21:28:45.307: INFO: namespace e2e-tests-secrets-86xt9 deletion completed in 22.115781406s

• [SLOW TEST:26.316 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:28:45.308: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 22 21:28:45.421: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dt8qq,SelfLink:/api/v1/namespaces/e2e-tests-watch-dt8qq/configmaps/e2e-watch-test-watch-closed,UID:b2b778d9-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32865,Generation:0,CreationTimestamp:2019-01-22 21:28:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 22 21:28:45.422: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dt8qq,SelfLink:/api/v1/namespaces/e2e-tests-watch-dt8qq/configmaps/e2e-watch-test-watch-closed,UID:b2b778d9-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32866,Generation:0,CreationTimestamp:2019-01-22 21:28:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 22 21:28:45.440: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dt8qq,SelfLink:/api/v1/namespaces/e2e-tests-watch-dt8qq/configmaps/e2e-watch-test-watch-closed,UID:b2b778d9-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32867,Generation:0,CreationTimestamp:2019-01-22 21:28:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 22 21:28:45.440: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-dt8qq,SelfLink:/api/v1/namespaces/e2e-tests-watch-dt8qq/configmaps/e2e-watch-test-watch-closed,UID:b2b778d9-1e8c-11e9-945c-000c293afc9e,ResourceVersion:32868,Generation:0,CreationTimestamp:2019-01-22 21:28:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:28:45.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-dt8qq" for this suite.
Jan 22 21:28:51.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:28:51.534: INFO: namespace: e2e-tests-watch-dt8qq, resource: bindings, ignored listing per whitelist
Jan 22 21:28:51.552: INFO: namespace e2e-tests-watch-dt8qq deletion completed in 6.106529286s

• [SLOW TEST:6.244 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:28:51.552: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 22 21:28:51.634: INFO: Waiting up to 5m0s for pod "pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-lxdv8" to be "success or failure"
Jan 22 21:28:51.638: INFO: Pod "pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.576842ms
Jan 22 21:28:53.644: INFO: Pod "pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009644221s
STEP: Saw pod success
Jan 22 21:28:53.644: INFO: Pod "pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:28:53.648: INFO: Trying to get logs from node secconf-node pod pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:28:53.666: INFO: Waiting for pod pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:28:53.669: INFO: Pod pod-b66bc0fd-1e8c-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:28:53.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lxdv8" for this suite.
Jan 22 21:28:59.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:28:59.735: INFO: namespace: e2e-tests-emptydir-lxdv8, resource: bindings, ignored listing per whitelist
Jan 22 21:28:59.773: INFO: namespace e2e-tests-emptydir-lxdv8 deletion completed in 6.098823942s

• [SLOW TEST:8.221 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:28:59.773: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 22 21:29:02.372: INFO: Successfully updated pod "pod-update-bb50ea5c-1e8c-11e9-9284-e2fd7d97dccb"
STEP: verifying the updated pod is in kubernetes
Jan 22 21:29:02.381: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:29:02.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lw9jv" for this suite.
Jan 22 21:29:24.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:29:24.510: INFO: namespace: e2e-tests-pods-lw9jv, resource: bindings, ignored listing per whitelist
Jan 22 21:29:24.542: INFO: namespace e2e-tests-pods-lw9jv deletion completed in 22.158268454s

• [SLOW TEST:24.770 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:29:24.543: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-6h6jv
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6h6jv to expose endpoints map[]
Jan 22 21:29:24.637: INFO: Get endpoints failed (3.175325ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan 22 21:29:25.640: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6h6jv exposes endpoints map[] (1.005827775s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-6h6jv
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6h6jv to expose endpoints map[pod1:[80]]
Jan 22 21:29:27.672: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6h6jv exposes endpoints map[pod1:[80]] (2.023739794s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-6h6jv
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6h6jv to expose endpoints map[pod1:[80] pod2:[80]]
Jan 22 21:29:29.709: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6h6jv exposes endpoints map[pod1:[80] pod2:[80]] (2.032360707s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-6h6jv
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6h6jv to expose endpoints map[pod2:[80]]
Jan 22 21:29:30.728: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6h6jv exposes endpoints map[pod2:[80]] (1.014327728s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-6h6jv
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6h6jv to expose endpoints map[]
Jan 22 21:29:31.740: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6h6jv exposes endpoints map[] (1.007073769s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:29:31.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-6h6jv" for this suite.
Jan 22 21:29:53.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:29:53.794: INFO: namespace: e2e-tests-services-6h6jv, resource: bindings, ignored listing per whitelist
Jan 22 21:29:53.851: INFO: namespace e2e-tests-services-6h6jv deletion completed in 22.090408165s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:29.309 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:29:53.851: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 22 21:29:58.944: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:29:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-d86cw" for this suite.
Jan 22 21:30:21.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:30:22.018: INFO: namespace: e2e-tests-replicaset-d86cw, resource: bindings, ignored listing per whitelist
Jan 22 21:30:22.044: INFO: namespace e2e-tests-replicaset-d86cw deletion completed in 22.083032707s

• [SLOW TEST:28.192 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:30:22.044: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-d8plq
Jan 22 21:30:24.112: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-d8plq
STEP: checking the pod's current state and verifying that restartCount is present
Jan 22 21:30:24.115: INFO: Initial restart count of pod liveness-http is 0
Jan 22 21:30:38.143: INFO: Restart count of pod e2e-tests-container-probe-d8plq/liveness-http is now 1 (14.027529925s elapsed)
Jan 22 21:30:58.189: INFO: Restart count of pod e2e-tests-container-probe-d8plq/liveness-http is now 2 (34.074386958s elapsed)
Jan 22 21:31:18.229: INFO: Restart count of pod e2e-tests-container-probe-d8plq/liveness-http is now 3 (54.113960876s elapsed)
Jan 22 21:31:38.269: INFO: Restart count of pod e2e-tests-container-probe-d8plq/liveness-http is now 4 (1m14.1541111s elapsed)
Jan 22 21:32:52.427: INFO: Restart count of pod e2e-tests-container-probe-d8plq/liveness-http is now 5 (2m28.311840595s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:32:52.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-d8plq" for this suite.
Jan 22 21:32:58.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:32:58.521: INFO: namespace: e2e-tests-container-probe-d8plq, resource: bindings, ignored listing per whitelist
Jan 22 21:32:58.532: INFO: namespace e2e-tests-container-probe-d8plq deletion completed in 6.084438465s

• [SLOW TEST:156.489 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:32:58.532: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 22 21:32:58.592: INFO: Waiting up to 5m0s for pod "pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-2jg46" to be "success or failure"
Jan 22 21:32:58.594: INFO: Pod "pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198591ms
Jan 22 21:33:00.600: INFO: Pod "pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007570668s
STEP: Saw pod success
Jan 22 21:33:00.600: INFO: Pod "pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:33:00.602: INFO: Trying to get logs from node secconf-node pod pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:33:00.623: INFO: Waiting for pod pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:33:00.627: INFO: Pod pod-499ed9c3-1e8d-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:33:00.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2jg46" for this suite.
Jan 22 21:33:06.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:33:06.759: INFO: namespace: e2e-tests-emptydir-2jg46, resource: bindings, ignored listing per whitelist
Jan 22 21:33:06.764: INFO: namespace e2e-tests-emptydir-2jg46 deletion completed in 6.131737081s

• [SLOW TEST:8.232 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:33:06.764: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 22 21:33:16.850: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:33:16.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-fvdvs" for this suite.
Jan 22 21:33:22.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:33:22.936: INFO: namespace: e2e-tests-gc-fvdvs, resource: bindings, ignored listing per whitelist
Jan 22 21:33:22.948: INFO: namespace e2e-tests-gc-fvdvs deletion completed in 6.094249398s

• [SLOW TEST:16.184 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:33:22.948: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 22 21:33:23.025: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-w9r2c,SelfLink:/api/v1/namespaces/e2e-tests-watch-w9r2c/configmaps/e2e-watch-test-resource-version,UID:582d631c-1e8d-11e9-945c-000c293afc9e,ResourceVersion:33547,Generation:0,CreationTimestamp:2019-01-22 21:33:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 22 21:33:23.026: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-w9r2c,SelfLink:/api/v1/namespaces/e2e-tests-watch-w9r2c/configmaps/e2e-watch-test-resource-version,UID:582d631c-1e8d-11e9-945c-000c293afc9e,ResourceVersion:33548,Generation:0,CreationTimestamp:2019-01-22 21:33:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:33:23.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-w9r2c" for this suite.
Jan 22 21:33:29.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:33:29.100: INFO: namespace: e2e-tests-watch-w9r2c, resource: bindings, ignored listing per whitelist
Jan 22 21:33:29.123: INFO: namespace e2e-tests-watch-w9r2c deletion completed in 6.092106495s

• [SLOW TEST:6.175 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:33:29.123: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-rwczr
Jan 22 21:33:31.189: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-rwczr
STEP: checking the pod's current state and verifying that restartCount is present
Jan 22 21:33:31.193: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:37:31.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rwczr" for this suite.
Jan 22 21:37:37.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:37:37.799: INFO: namespace: e2e-tests-container-probe-rwczr, resource: bindings, ignored listing per whitelist
Jan 22 21:37:37.809: INFO: namespace e2e-tests-container-probe-rwczr deletion completed in 6.092165907s

• [SLOW TEST:248.686 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:37:37.809: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Jan 22 21:37:39.905: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:38:03.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-sd5f4" for this suite.
Jan 22 21:38:09.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:38:09.972: INFO: namespace: e2e-tests-namespaces-sd5f4, resource: bindings, ignored listing per whitelist
Jan 22 21:38:10.028: INFO: namespace e2e-tests-namespaces-sd5f4 deletion completed in 6.078210355s
STEP: Destroying namespace "e2e-tests-nsdeletetest-jtlr9" for this suite.
Jan 22 21:38:10.031: INFO: Namespace e2e-tests-nsdeletetest-jtlr9 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-wddm6" for this suite.
Jan 22 21:38:16.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:38:16.087: INFO: namespace: e2e-tests-nsdeletetest-wddm6, resource: bindings, ignored listing per whitelist
Jan 22 21:38:16.109: INFO: namespace e2e-tests-nsdeletetest-wddm6 deletion completed in 6.078443014s

• [SLOW TEST:38.300 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:38:16.109: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 22 21:38:56.202: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:38:56.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-ff2g4" for this suite.
Jan 22 21:39:02.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:39:02.305: INFO: namespace: e2e-tests-gc-ff2g4, resource: bindings, ignored listing per whitelist
Jan 22 21:39:02.380: INFO: namespace e2e-tests-gc-ff2g4 deletion completed in 6.174022085s

• [SLOW TEST:46.270 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:39:02.380: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Jan 22 21:39:02.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:02.754: INFO: stderr: ""
Jan 22 21:39:02.754: INFO: stdout: "pod/pause created\n"
Jan 22 21:39:02.754: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 22 21:39:02.754: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-w9zjn" to be "running and ready"
Jan 22 21:39:02.758: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109344ms
Jan 22 21:39:04.763: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008946196s
Jan 22 21:39:04.763: INFO: Pod "pause" satisfied condition "running and ready"
Jan 22 21:39:04.763: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 22 21:39:04.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:04.841: INFO: stderr: ""
Jan 22 21:39:04.841: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 22 21:39:04.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pod pause -L testing-label --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:04.904: INFO: stderr: ""
Jan 22 21:39:04.904: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 22 21:39:04.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 label pods pause testing-label- --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:04.972: INFO: stderr: ""
Jan 22 21:39:04.972: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 22 21:39:04.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pod pause -L testing-label --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:05.044: INFO: stderr: ""
Jan 22 21:39:05.044: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Jan 22 21:39:05.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:05.116: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:39:05.116: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 22 21:39:05.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-w9zjn'
Jan 22 21:39:05.207: INFO: stderr: "No resources found.\n"
Jan 22 21:39:05.207: INFO: stdout: ""
Jan 22 21:39:05.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -l name=pause --namespace=e2e-tests-kubectl-w9zjn -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 22 21:39:05.288: INFO: stderr: ""
Jan 22 21:39:05.288: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:39:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w9zjn" for this suite.
Jan 22 21:39:11.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:39:11.329: INFO: namespace: e2e-tests-kubectl-w9zjn, resource: bindings, ignored listing per whitelist
Jan 22 21:39:11.378: INFO: namespace e2e-tests-kubectl-w9zjn deletion completed in 6.086312144s

• [SLOW TEST:8.998 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:39:11.378: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:39:11.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-wp92l" to be "success or failure"
Jan 22 21:39:11.443: INFO: Pod "downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.269119ms
Jan 22 21:39:13.446: INFO: Pod "downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006689879s
STEP: Saw pod success
Jan 22 21:39:13.446: INFO: Pod "downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:39:13.449: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:39:13.472: INFO: Waiting for pod downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:39:13.477: INFO: Pod downwardapi-volume-27dab0a5-1e8e-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:39:13.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wp92l" for this suite.
Jan 22 21:39:19.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:39:19.514: INFO: namespace: e2e-tests-downward-api-wp92l, resource: bindings, ignored listing per whitelist
Jan 22 21:39:19.581: INFO: namespace e2e-tests-downward-api-wp92l deletion completed in 6.099130789s

• [SLOW TEST:8.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:39:19.581: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 21:39:19.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-z976p'
Jan 22 21:39:19.720: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 22 21:39:19.720: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Jan 22 21:39:19.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-z976p'
Jan 22 21:39:19.820: INFO: stderr: ""
Jan 22 21:39:19.820: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:39:19.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z976p" for this suite.
Jan 22 21:39:41.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:39:41.862: INFO: namespace: e2e-tests-kubectl-z976p, resource: bindings, ignored listing per whitelist
Jan 22 21:39:41.921: INFO: namespace e2e-tests-kubectl-z976p deletion completed in 22.094298572s

• [SLOW TEST:22.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:39:41.921: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-l2bk2
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l2bk2 to expose endpoints map[]
Jan 22 21:39:41.986: INFO: Get endpoints failed (2.825886ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 22 21:39:42.990: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l2bk2 exposes endpoints map[] (1.007410637s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-l2bk2
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l2bk2 to expose endpoints map[pod1:[100]]
Jan 22 21:39:45.026: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l2bk2 exposes endpoints map[pod1:[100]] (2.027708668s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-l2bk2
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l2bk2 to expose endpoints map[pod2:[101] pod1:[100]]
Jan 22 21:39:47.064: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l2bk2 exposes endpoints map[pod1:[100] pod2:[101]] (2.03288731s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-l2bk2
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l2bk2 to expose endpoints map[pod2:[101]]
Jan 22 21:39:48.087: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l2bk2 exposes endpoints map[pod2:[101]] (1.019146564s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-l2bk2
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-l2bk2 to expose endpoints map[]
Jan 22 21:39:49.100: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-l2bk2 exposes endpoints map[] (1.007991231s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:39:49.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-l2bk2" for this suite.
Jan 22 21:39:55.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:39:55.193: INFO: namespace: e2e-tests-services-l2bk2, resource: bindings, ignored listing per whitelist
Jan 22 21:39:55.209: INFO: namespace e2e-tests-services-l2bk2 deletion completed in 6.087328745s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:13.288 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:39:55.209: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-41fc5149-1e8e-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:39:57.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zsstj" for this suite.
Jan 22 21:40:19.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:40:19.367: INFO: namespace: e2e-tests-configmap-zsstj, resource: bindings, ignored listing per whitelist
Jan 22 21:40:19.415: INFO: namespace e2e-tests-configmap-zsstj deletion completed in 22.104237936s

• [SLOW TEST:24.206 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:40:19.415: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 22 21:40:19.511: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:40:23.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-5hbsv" for this suite.
Jan 22 21:40:45.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:40:45.262: INFO: namespace: e2e-tests-init-container-5hbsv, resource: bindings, ignored listing per whitelist
Jan 22 21:40:45.308: INFO: namespace e2e-tests-init-container-5hbsv deletion completed in 22.134931284s

• [SLOW TEST:25.893 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:40:45.309: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 22 21:40:45.380: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 22 21:40:50.384: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:40:51.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-jm9bk" for this suite.
Jan 22 21:40:57.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:40:57.518: INFO: namespace: e2e-tests-replication-controller-jm9bk, resource: bindings, ignored listing per whitelist
Jan 22 21:40:57.539: INFO: namespace e2e-tests-replication-controller-jm9bk deletion completed in 6.119561905s

• [SLOW TEST:12.231 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:40:57.539: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 22 21:41:01.652: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 22 21:41:01.655: INFO: Pod pod-with-poststart-http-hook still exists
Jan 22 21:41:03.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 22 21:41:03.658: INFO: Pod pod-with-poststart-http-hook still exists
Jan 22 21:41:05.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 22 21:41:05.658: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:41:05.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-jz98x" for this suite.
Jan 22 21:41:27.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:41:27.731: INFO: namespace: e2e-tests-container-lifecycle-hook-jz98x, resource: bindings, ignored listing per whitelist
Jan 22 21:41:27.772: INFO: namespace e2e-tests-container-lifecycle-hook-jz98x deletion completed in 22.109713004s

• [SLOW TEST:30.233 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:41:27.772: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 21:41:27.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-jttqt'
Jan 22 21:41:27.943: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 22 21:41:27.943: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Jan 22 21:41:31.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-jttqt'
Jan 22 21:41:32.051: INFO: stderr: ""
Jan 22 21:41:32.051: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:41:32.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jttqt" for this suite.
Jan 22 21:41:54.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:41:54.138: INFO: namespace: e2e-tests-kubectl-jttqt, resource: bindings, ignored listing per whitelist
Jan 22 21:41:54.159: INFO: namespace e2e-tests-kubectl-jttqt deletion completed in 22.103352209s

• [SLOW TEST:26.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:41:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:42:14.240: INFO: Container started at 2019-01-22 21:41:55 +0000 UTC, pod became ready at 2019-01-22 21:42:12 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:42:14.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-67ppm" for this suite.
Jan 22 21:42:36.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:42:36.319: INFO: namespace: e2e-tests-container-probe-67ppm, resource: bindings, ignored listing per whitelist
Jan 22 21:42:36.330: INFO: namespace e2e-tests-container-probe-67ppm deletion completed in 22.085738447s

• [SLOW TEST:42.171 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:42:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Jan 22 21:42:36.396: INFO: Waiting up to 5m0s for pod "var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-var-expansion-d9ksz" to be "success or failure"
Jan 22 21:42:36.399: INFO: Pod "var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.282262ms
Jan 22 21:42:38.403: INFO: Pod "var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007529784s
STEP: Saw pod success
Jan 22 21:42:38.403: INFO: Pod "var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:42:38.407: INFO: Trying to get logs from node secconf-node pod var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 21:42:38.430: INFO: Waiting for pod var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:42:38.436: INFO: Pod var-expansion-a204ab3f-1e8e-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:42:38.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-d9ksz" for this suite.
Jan 22 21:42:44.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:42:44.508: INFO: namespace: e2e-tests-var-expansion-d9ksz, resource: bindings, ignored listing per whitelist
Jan 22 21:42:44.537: INFO: namespace e2e-tests-var-expansion-d9ksz deletion completed in 6.095331217s

• [SLOW TEST:8.207 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:42:44.537: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-49286.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-49286.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-49286.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-49286.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-49286.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-49286.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 22 21:42:48.687: INFO: DNS probes using e2e-tests-dns-49286/dns-test-a6e8e5d7-1e8e-11e9-9284-e2fd7d97dccb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:42:48.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-49286" for this suite.
Jan 22 21:42:54.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:42:54.737: INFO: namespace: e2e-tests-dns-49286, resource: bindings, ignored listing per whitelist
Jan 22 21:42:54.777: INFO: namespace e2e-tests-dns-49286 deletion completed in 6.077746311s

• [SLOW TEST:10.240 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:42:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 22 21:42:54.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:42:54.981: INFO: stderr: ""
Jan 22 21:42:54.981: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 22 21:42:54.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:42:55.074: INFO: stderr: ""
Jan 22 21:42:55.074: INFO: stdout: "update-demo-nautilus-7cjxj update-demo-nautilus-ztrmw "
Jan 22 21:42:55.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:42:55.154: INFO: stderr: ""
Jan 22 21:42:55.154: INFO: stdout: ""
Jan 22 21:42:55.154: INFO: update-demo-nautilus-7cjxj is created but not running
Jan 22 21:43:00.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:00.264: INFO: stderr: ""
Jan 22 21:43:00.264: INFO: stdout: "update-demo-nautilus-7cjxj update-demo-nautilus-ztrmw "
Jan 22 21:43:00.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:00.358: INFO: stderr: ""
Jan 22 21:43:00.358: INFO: stdout: "true"
Jan 22 21:43:00.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:00.454: INFO: stderr: ""
Jan 22 21:43:00.454: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:43:00.454: INFO: validating pod update-demo-nautilus-7cjxj
Jan 22 21:43:00.461: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:43:00.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:43:00.461: INFO: update-demo-nautilus-7cjxj is verified up and running
Jan 22 21:43:00.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-ztrmw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:00.538: INFO: stderr: ""
Jan 22 21:43:00.538: INFO: stdout: "true"
Jan 22 21:43:00.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-ztrmw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:00.616: INFO: stderr: ""
Jan 22 21:43:00.616: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:43:00.616: INFO: validating pod update-demo-nautilus-ztrmw
Jan 22 21:43:00.621: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:43:00.621: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:43:00.621: INFO: update-demo-nautilus-ztrmw is verified up and running
STEP: scaling down the replication controller
Jan 22 21:43:00.622: INFO: scanned /root for discovery docs: <nil>
Jan 22 21:43:00.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:01.728: INFO: stderr: ""
Jan 22 21:43:01.728: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 22 21:43:01.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:01.828: INFO: stderr: ""
Jan 22 21:43:01.828: INFO: stdout: "update-demo-nautilus-7cjxj update-demo-nautilus-ztrmw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 22 21:43:06.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:06.907: INFO: stderr: ""
Jan 22 21:43:06.907: INFO: stdout: "update-demo-nautilus-7cjxj "
Jan 22 21:43:06.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:06.975: INFO: stderr: ""
Jan 22 21:43:06.976: INFO: stdout: "true"
Jan 22 21:43:06.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:07.045: INFO: stderr: ""
Jan 22 21:43:07.045: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:43:07.045: INFO: validating pod update-demo-nautilus-7cjxj
Jan 22 21:43:07.049: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:43:07.049: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:43:07.049: INFO: update-demo-nautilus-7cjxj is verified up and running
STEP: scaling up the replication controller
Jan 22 21:43:07.051: INFO: scanned /root for discovery docs: <nil>
Jan 22 21:43:07.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:08.153: INFO: stderr: ""
Jan 22 21:43:08.153: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 22 21:43:08.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:08.271: INFO: stderr: ""
Jan 22 21:43:08.271: INFO: stdout: "update-demo-nautilus-7cjxj update-demo-nautilus-zpr5r "
Jan 22 21:43:08.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:08.380: INFO: stderr: ""
Jan 22 21:43:08.380: INFO: stdout: "true"
Jan 22 21:43:08.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:08.523: INFO: stderr: ""
Jan 22 21:43:08.524: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:43:08.524: INFO: validating pod update-demo-nautilus-7cjxj
Jan 22 21:43:08.531: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:43:08.531: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:43:08.531: INFO: update-demo-nautilus-7cjxj is verified up and running
Jan 22 21:43:08.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-zpr5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:08.647: INFO: stderr: ""
Jan 22 21:43:08.647: INFO: stdout: ""
Jan 22 21:43:08.647: INFO: update-demo-nautilus-zpr5r is created but not running
Jan 22 21:43:13.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:13.743: INFO: stderr: ""
Jan 22 21:43:13.743: INFO: stdout: "update-demo-nautilus-7cjxj update-demo-nautilus-zpr5r "
Jan 22 21:43:13.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:13.836: INFO: stderr: ""
Jan 22 21:43:13.836: INFO: stdout: "true"
Jan 22 21:43:13.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-7cjxj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:13.930: INFO: stderr: ""
Jan 22 21:43:13.930: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:43:13.930: INFO: validating pod update-demo-nautilus-7cjxj
Jan 22 21:43:13.936: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:43:13.936: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:43:13.936: INFO: update-demo-nautilus-7cjxj is verified up and running
Jan 22 21:43:13.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-zpr5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:14.039: INFO: stderr: ""
Jan 22 21:43:14.039: INFO: stdout: "true"
Jan 22 21:43:14.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-zpr5r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:14.147: INFO: stderr: ""
Jan 22 21:43:14.147: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:43:14.147: INFO: validating pod update-demo-nautilus-zpr5r
Jan 22 21:43:14.155: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:43:14.155: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:43:14.155: INFO: update-demo-nautilus-zpr5r is verified up and running
STEP: using delete to clean up resources
Jan 22 21:43:14.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:14.259: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:43:14.259: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 22 21:43:14.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-6mdps'
Jan 22 21:43:14.423: INFO: stderr: "No resources found.\n"
Jan 22 21:43:14.423: INFO: stdout: ""
Jan 22 21:43:14.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -l name=update-demo --namespace=e2e-tests-kubectl-6mdps -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 22 21:43:14.563: INFO: stderr: ""
Jan 22 21:43:14.563: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:43:14.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6mdps" for this suite.
Jan 22 21:43:20.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:43:20.653: INFO: namespace: e2e-tests-kubectl-6mdps, resource: bindings, ignored listing per whitelist
Jan 22 21:43:20.662: INFO: namespace e2e-tests-kubectl-6mdps deletion completed in 6.09411052s

• [SLOW TEST:25.885 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:43:20.662: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-bc7675b0-1e8e-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:43:20.766: INFO: Waiting up to 5m0s for pod "pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-6fs2f" to be "success or failure"
Jan 22 21:43:20.772: INFO: Pod "pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890917ms
Jan 22 21:43:22.775: INFO: Pod "pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008661005s
STEP: Saw pod success
Jan 22 21:43:22.776: INFO: Pod "pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:43:22.779: INFO: Trying to get logs from node secconf-node pod pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:43:22.801: INFO: Waiting for pod pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:43:22.804: INFO: Pod pod-secrets-bc76f398-1e8e-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:43:22.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6fs2f" for this suite.
Jan 22 21:43:28.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:43:28.852: INFO: namespace: e2e-tests-secrets-6fs2f, resource: bindings, ignored listing per whitelist
Jan 22 21:43:28.898: INFO: namespace e2e-tests-secrets-6fs2f deletion completed in 6.085532941s

• [SLOW TEST:8.236 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:43:28.898: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-z482l/configmap-test-c15891de-1e8e-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:43:28.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-z482l" to be "success or failure"
Jan 22 21:43:28.962: INFO: Pod "pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.077952ms
Jan 22 21:43:30.967: INFO: Pod "pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007887834s
STEP: Saw pod success
Jan 22 21:43:30.967: INFO: Pod "pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:43:30.970: INFO: Trying to get logs from node secconf-node pod pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb container env-test: <nil>
STEP: delete the pod
Jan 22 21:43:30.986: INFO: Waiting for pod pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:43:30.991: INFO: Pod pod-configmaps-c15943a9-1e8e-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:43:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-z482l" for this suite.
Jan 22 21:43:37.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:43:37.058: INFO: namespace: e2e-tests-configmap-z482l, resource: bindings, ignored listing per whitelist
Jan 22 21:43:37.121: INFO: namespace e2e-tests-configmap-z482l deletion completed in 6.125858483s

• [SLOW TEST:8.223 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:43:37.122: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-c6462fe5-1e8e-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:43:37.230: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-n8xcg" to be "success or failure"
Jan 22 21:43:37.233: INFO: Pod "pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973888ms
Jan 22 21:43:39.238: INFO: Pod "pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007843958s
STEP: Saw pod success
Jan 22 21:43:39.238: INFO: Pod "pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:43:39.241: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:43:39.259: INFO: Waiting for pod pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:43:39.262: INFO: Pod pod-projected-configmaps-c646d5d2-1e8e-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:43:39.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n8xcg" for this suite.
Jan 22 21:43:45.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:43:45.346: INFO: namespace: e2e-tests-projected-n8xcg, resource: bindings, ignored listing per whitelist
Jan 22 21:43:45.388: INFO: namespace e2e-tests-projected-n8xcg deletion completed in 6.122293735s

• [SLOW TEST:8.267 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:43:45.388: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-xndb8
I0122 21:43:45.453426      14 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-xndb8, replica count: 1
I0122 21:43:46.505054      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0122 21:43:47.505421      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 22 21:43:47.617: INFO: Created: latency-svc-rl9x5
Jan 22 21:43:47.625: INFO: Got endpoints: latency-svc-rl9x5 [19.785273ms]
Jan 22 21:43:47.641: INFO: Created: latency-svc-j7dn6
Jan 22 21:43:47.645: INFO: Got endpoints: latency-svc-j7dn6 [19.741884ms]
Jan 22 21:43:47.661: INFO: Created: latency-svc-g7pds
Jan 22 21:43:47.667: INFO: Got endpoints: latency-svc-g7pds [41.475054ms]
Jan 22 21:43:47.674: INFO: Created: latency-svc-jf6jm
Jan 22 21:43:47.679: INFO: Got endpoints: latency-svc-jf6jm [52.571408ms]
Jan 22 21:43:47.688: INFO: Created: latency-svc-pdz56
Jan 22 21:43:47.694: INFO: Got endpoints: latency-svc-pdz56 [67.742753ms]
Jan 22 21:43:47.700: INFO: Created: latency-svc-92r9s
Jan 22 21:43:47.704: INFO: Got endpoints: latency-svc-92r9s [77.445432ms]
Jan 22 21:43:47.716: INFO: Created: latency-svc-scwjt
Jan 22 21:43:47.719: INFO: Got endpoints: latency-svc-scwjt [92.541648ms]
Jan 22 21:43:47.728: INFO: Created: latency-svc-d9kh8
Jan 22 21:43:47.732: INFO: Got endpoints: latency-svc-d9kh8 [105.950033ms]
Jan 22 21:43:47.739: INFO: Created: latency-svc-59qzz
Jan 22 21:43:47.743: INFO: Got endpoints: latency-svc-59qzz [116.816879ms]
Jan 22 21:43:47.762: INFO: Created: latency-svc-jm6rw
Jan 22 21:43:47.767: INFO: Got endpoints: latency-svc-jm6rw [140.723399ms]
Jan 22 21:43:47.787: INFO: Created: latency-svc-tb7lx
Jan 22 21:43:47.793: INFO: Got endpoints: latency-svc-tb7lx [166.896185ms]
Jan 22 21:43:47.805: INFO: Created: latency-svc-94zzz
Jan 22 21:43:47.811: INFO: Got endpoints: latency-svc-94zzz [184.866799ms]
Jan 22 21:43:47.824: INFO: Created: latency-svc-6pk6k
Jan 22 21:43:47.828: INFO: Got endpoints: latency-svc-6pk6k [201.759535ms]
Jan 22 21:43:47.838: INFO: Created: latency-svc-86r9n
Jan 22 21:43:47.847: INFO: Got endpoints: latency-svc-86r9n [220.807832ms]
Jan 22 21:43:47.862: INFO: Created: latency-svc-wxb4x
Jan 22 21:43:47.875: INFO: Got endpoints: latency-svc-wxb4x [247.935832ms]
Jan 22 21:43:47.883: INFO: Created: latency-svc-xmdps
Jan 22 21:43:47.894: INFO: Got endpoints: latency-svc-xmdps [267.114971ms]
Jan 22 21:43:47.904: INFO: Created: latency-svc-6z7xf
Jan 22 21:43:47.908: INFO: Got endpoints: latency-svc-6z7xf [262.507631ms]
Jan 22 21:43:47.920: INFO: Created: latency-svc-q665j
Jan 22 21:43:47.925: INFO: Got endpoints: latency-svc-q665j [258.445866ms]
Jan 22 21:43:47.939: INFO: Created: latency-svc-2jwf2
Jan 22 21:43:47.941: INFO: Got endpoints: latency-svc-2jwf2 [261.948128ms]
Jan 22 21:43:47.955: INFO: Created: latency-svc-krmcx
Jan 22 21:43:47.962: INFO: Got endpoints: latency-svc-krmcx [268.529785ms]
Jan 22 21:43:47.975: INFO: Created: latency-svc-cdgf8
Jan 22 21:43:47.998: INFO: Got endpoints: latency-svc-cdgf8 [293.981724ms]
Jan 22 21:43:48.003: INFO: Created: latency-svc-pbsdc
Jan 22 21:43:48.014: INFO: Got endpoints: latency-svc-pbsdc [294.850657ms]
Jan 22 21:43:48.036: INFO: Created: latency-svc-fmlkt
Jan 22 21:43:48.046: INFO: Got endpoints: latency-svc-fmlkt [314.078742ms]
Jan 22 21:43:48.062: INFO: Created: latency-svc-c9kqm
Jan 22 21:43:48.065: INFO: Got endpoints: latency-svc-c9kqm [322.329765ms]
Jan 22 21:43:48.081: INFO: Created: latency-svc-qwdxg
Jan 22 21:43:48.084: INFO: Got endpoints: latency-svc-qwdxg [316.891058ms]
Jan 22 21:43:48.096: INFO: Created: latency-svc-sg8vr
Jan 22 21:43:48.101: INFO: Got endpoints: latency-svc-sg8vr [307.619211ms]
Jan 22 21:43:48.117: INFO: Created: latency-svc-r6srd
Jan 22 21:43:48.123: INFO: Got endpoints: latency-svc-r6srd [311.644537ms]
Jan 22 21:43:48.132: INFO: Created: latency-svc-pfkzs
Jan 22 21:43:48.139: INFO: Got endpoints: latency-svc-pfkzs [310.352612ms]
Jan 22 21:43:48.146: INFO: Created: latency-svc-krlsf
Jan 22 21:43:48.156: INFO: Got endpoints: latency-svc-krlsf [308.826109ms]
Jan 22 21:43:48.167: INFO: Created: latency-svc-f9nq4
Jan 22 21:43:48.171: INFO: Got endpoints: latency-svc-f9nq4 [296.213046ms]
Jan 22 21:43:48.180: INFO: Created: latency-svc-b4xfd
Jan 22 21:43:48.186: INFO: Got endpoints: latency-svc-b4xfd [292.453792ms]
Jan 22 21:43:48.200: INFO: Created: latency-svc-xvs6v
Jan 22 21:43:48.207: INFO: Got endpoints: latency-svc-xvs6v [299.109347ms]
Jan 22 21:43:48.210: INFO: Created: latency-svc-7dxvp
Jan 22 21:43:48.219: INFO: Got endpoints: latency-svc-7dxvp [293.00162ms]
Jan 22 21:43:48.225: INFO: Created: latency-svc-lj65z
Jan 22 21:43:48.235: INFO: Got endpoints: latency-svc-lj65z [294.854026ms]
Jan 22 21:43:48.238: INFO: Created: latency-svc-c94m2
Jan 22 21:43:48.249: INFO: Got endpoints: latency-svc-c94m2 [286.649762ms]
Jan 22 21:43:48.260: INFO: Created: latency-svc-w4hvx
Jan 22 21:43:48.264: INFO: Got endpoints: latency-svc-w4hvx [265.977903ms]
Jan 22 21:43:48.280: INFO: Created: latency-svc-smf7b
Jan 22 21:43:48.287: INFO: Got endpoints: latency-svc-smf7b [272.861992ms]
Jan 22 21:43:48.291: INFO: Created: latency-svc-5cbr5
Jan 22 21:43:48.298: INFO: Got endpoints: latency-svc-5cbr5 [251.216284ms]
Jan 22 21:43:48.318: INFO: Created: latency-svc-2q44g
Jan 22 21:43:48.323: INFO: Got endpoints: latency-svc-2q44g [257.31826ms]
Jan 22 21:43:48.334: INFO: Created: latency-svc-nn9fs
Jan 22 21:43:48.346: INFO: Got endpoints: latency-svc-nn9fs [262.032242ms]
Jan 22 21:43:48.364: INFO: Created: latency-svc-zktn6
Jan 22 21:43:48.367: INFO: Got endpoints: latency-svc-zktn6 [266.190085ms]
Jan 22 21:43:48.388: INFO: Created: latency-svc-zgmmf
Jan 22 21:43:48.399: INFO: Got endpoints: latency-svc-zgmmf [275.719728ms]
Jan 22 21:43:48.436: INFO: Created: latency-svc-snzhx
Jan 22 21:43:48.469: INFO: Got endpoints: latency-svc-snzhx [330.726983ms]
Jan 22 21:43:48.509: INFO: Created: latency-svc-r5chr
Jan 22 21:43:48.518: INFO: Got endpoints: latency-svc-r5chr [361.522765ms]
Jan 22 21:43:48.542: INFO: Created: latency-svc-ftmzt
Jan 22 21:43:48.547: INFO: Got endpoints: latency-svc-ftmzt [375.868374ms]
Jan 22 21:43:48.572: INFO: Created: latency-svc-xjjjz
Jan 22 21:43:48.574: INFO: Got endpoints: latency-svc-xjjjz [387.406345ms]
Jan 22 21:43:48.590: INFO: Created: latency-svc-kb2nr
Jan 22 21:43:48.594: INFO: Got endpoints: latency-svc-kb2nr [387.095873ms]
Jan 22 21:43:48.608: INFO: Created: latency-svc-wd6ln
Jan 22 21:43:48.630: INFO: Got endpoints: latency-svc-wd6ln [411.807803ms]
Jan 22 21:43:48.655: INFO: Created: latency-svc-mrr7q
Jan 22 21:43:48.664: INFO: Got endpoints: latency-svc-mrr7q [69.555666ms]
Jan 22 21:43:48.681: INFO: Created: latency-svc-9fz7h
Jan 22 21:43:48.692: INFO: Got endpoints: latency-svc-9fz7h [456.423801ms]
Jan 22 21:43:48.761: INFO: Created: latency-svc-cfzc4
Jan 22 21:43:48.772: INFO: Got endpoints: latency-svc-cfzc4 [523.057199ms]
Jan 22 21:43:48.781: INFO: Created: latency-svc-zp59m
Jan 22 21:43:48.789: INFO: Got endpoints: latency-svc-zp59m [525.307108ms]
Jan 22 21:43:48.796: INFO: Created: latency-svc-d55k2
Jan 22 21:43:48.804: INFO: Got endpoints: latency-svc-d55k2 [517.551736ms]
Jan 22 21:43:48.816: INFO: Created: latency-svc-298rc
Jan 22 21:43:48.824: INFO: Got endpoints: latency-svc-298rc [526.225101ms]
Jan 22 21:43:48.832: INFO: Created: latency-svc-8vw28
Jan 22 21:43:48.854: INFO: Created: latency-svc-kjls5
Jan 22 21:43:48.871: INFO: Got endpoints: latency-svc-8vw28 [547.99042ms]
Jan 22 21:43:48.874: INFO: Created: latency-svc-jtnjl
Jan 22 21:43:48.889: INFO: Created: latency-svc-zrw7n
Jan 22 21:43:48.902: INFO: Created: latency-svc-nfnh2
Jan 22 21:43:48.912: INFO: Created: latency-svc-wh2lq
Jan 22 21:43:48.922: INFO: Got endpoints: latency-svc-kjls5 [575.574032ms]
Jan 22 21:43:48.923: INFO: Created: latency-svc-wdz2j
Jan 22 21:43:48.938: INFO: Created: latency-svc-jnvz2
Jan 22 21:43:48.951: INFO: Created: latency-svc-nvp4n
Jan 22 21:43:48.966: INFO: Created: latency-svc-tgxl4
Jan 22 21:43:48.977: INFO: Got endpoints: latency-svc-jtnjl [609.47763ms]
Jan 22 21:43:48.981: INFO: Created: latency-svc-cg2kr
Jan 22 21:43:48.992: INFO: Created: latency-svc-x4hjd
Jan 22 21:43:49.010: INFO: Created: latency-svc-v5wh4
Jan 22 21:43:49.022: INFO: Created: latency-svc-xjxqm
Jan 22 21:43:49.022: INFO: Got endpoints: latency-svc-zrw7n [622.882358ms]
Jan 22 21:43:49.030: INFO: Created: latency-svc-vvt8g
Jan 22 21:43:49.040: INFO: Created: latency-svc-jpvwh
Jan 22 21:43:49.057: INFO: Created: latency-svc-66br7
Jan 22 21:43:49.073: INFO: Created: latency-svc-4zsvw
Jan 22 21:43:49.076: INFO: Got endpoints: latency-svc-nfnh2 [606.162586ms]
Jan 22 21:43:49.091: INFO: Created: latency-svc-rgt4c
Jan 22 21:43:49.099: INFO: Created: latency-svc-f6jmg
Jan 22 21:43:49.121: INFO: Got endpoints: latency-svc-wh2lq [603.635659ms]
Jan 22 21:43:49.136: INFO: Created: latency-svc-xvw6n
Jan 22 21:43:49.171: INFO: Got endpoints: latency-svc-wdz2j [623.911075ms]
Jan 22 21:43:49.186: INFO: Created: latency-svc-tbgj9
Jan 22 21:43:49.270: INFO: Got endpoints: latency-svc-jnvz2 [696.674483ms]
Jan 22 21:43:49.283: INFO: Got endpoints: latency-svc-nvp4n [652.173624ms]
Jan 22 21:43:49.297: INFO: Created: latency-svc-mdmms
Jan 22 21:43:49.305: INFO: Created: latency-svc-8q5bd
Jan 22 21:43:49.322: INFO: Got endpoints: latency-svc-tgxl4 [657.8091ms]
Jan 22 21:43:49.338: INFO: Created: latency-svc-2ws6s
Jan 22 21:43:49.371: INFO: Got endpoints: latency-svc-cg2kr [678.478639ms]
Jan 22 21:43:49.386: INFO: Created: latency-svc-z5w6p
Jan 22 21:43:49.423: INFO: Got endpoints: latency-svc-x4hjd [650.494026ms]
Jan 22 21:43:49.438: INFO: Created: latency-svc-74f5l
Jan 22 21:43:49.470: INFO: Got endpoints: latency-svc-v5wh4 [681.209833ms]
Jan 22 21:43:49.484: INFO: Created: latency-svc-kjngz
Jan 22 21:43:49.521: INFO: Got endpoints: latency-svc-xjxqm [716.649144ms]
Jan 22 21:43:49.534: INFO: Created: latency-svc-tl4dw
Jan 22 21:43:49.571: INFO: Got endpoints: latency-svc-vvt8g [746.588551ms]
Jan 22 21:43:49.583: INFO: Created: latency-svc-zw478
Jan 22 21:43:49.622: INFO: Got endpoints: latency-svc-jpvwh [750.799593ms]
Jan 22 21:43:49.638: INFO: Created: latency-svc-rl6b8
Jan 22 21:43:49.672: INFO: Got endpoints: latency-svc-66br7 [750.362408ms]
Jan 22 21:43:49.693: INFO: Created: latency-svc-d248p
Jan 22 21:43:49.722: INFO: Got endpoints: latency-svc-4zsvw [745.188335ms]
Jan 22 21:43:49.737: INFO: Created: latency-svc-wxdzh
Jan 22 21:43:49.770: INFO: Got endpoints: latency-svc-rgt4c [748.380428ms]
Jan 22 21:43:49.782: INFO: Created: latency-svc-b5fvk
Jan 22 21:43:49.819: INFO: Got endpoints: latency-svc-f6jmg [743.633725ms]
Jan 22 21:43:49.829: INFO: Created: latency-svc-qmgk9
Jan 22 21:43:49.869: INFO: Got endpoints: latency-svc-xvw6n [747.853813ms]
Jan 22 21:43:49.880: INFO: Created: latency-svc-5n2vl
Jan 22 21:43:49.920: INFO: Got endpoints: latency-svc-tbgj9 [749.025153ms]
Jan 22 21:43:49.937: INFO: Created: latency-svc-6b668
Jan 22 21:43:49.970: INFO: Got endpoints: latency-svc-mdmms [699.724566ms]
Jan 22 21:43:49.982: INFO: Created: latency-svc-2pq9g
Jan 22 21:43:50.020: INFO: Got endpoints: latency-svc-8q5bd [737.147346ms]
Jan 22 21:43:50.036: INFO: Created: latency-svc-nnvfj
Jan 22 21:43:50.070: INFO: Got endpoints: latency-svc-2ws6s [748.23318ms]
Jan 22 21:43:50.082: INFO: Created: latency-svc-rmj94
Jan 22 21:43:50.120: INFO: Got endpoints: latency-svc-z5w6p [749.44727ms]
Jan 22 21:43:50.133: INFO: Created: latency-svc-gjjxn
Jan 22 21:43:50.171: INFO: Got endpoints: latency-svc-74f5l [747.861442ms]
Jan 22 21:43:50.181: INFO: Created: latency-svc-fwxx7
Jan 22 21:43:50.223: INFO: Got endpoints: latency-svc-kjngz [752.481324ms]
Jan 22 21:43:50.235: INFO: Created: latency-svc-xplk6
Jan 22 21:43:50.270: INFO: Got endpoints: latency-svc-tl4dw [749.143887ms]
Jan 22 21:43:50.280: INFO: Created: latency-svc-2z5g9
Jan 22 21:43:50.319: INFO: Got endpoints: latency-svc-zw478 [748.881662ms]
Jan 22 21:43:50.332: INFO: Created: latency-svc-vvh5x
Jan 22 21:43:50.369: INFO: Got endpoints: latency-svc-rl6b8 [747.67252ms]
Jan 22 21:43:50.378: INFO: Created: latency-svc-fpfl8
Jan 22 21:43:50.419: INFO: Got endpoints: latency-svc-d248p [746.906909ms]
Jan 22 21:43:50.430: INFO: Created: latency-svc-ht5vz
Jan 22 21:43:50.469: INFO: Got endpoints: latency-svc-wxdzh [747.455129ms]
Jan 22 21:43:50.479: INFO: Created: latency-svc-kfgqj
Jan 22 21:43:50.520: INFO: Got endpoints: latency-svc-b5fvk [750.125263ms]
Jan 22 21:43:50.533: INFO: Created: latency-svc-w5hhl
Jan 22 21:43:50.570: INFO: Got endpoints: latency-svc-qmgk9 [750.964295ms]
Jan 22 21:43:50.580: INFO: Created: latency-svc-4xv4n
Jan 22 21:43:50.620: INFO: Got endpoints: latency-svc-5n2vl [750.749975ms]
Jan 22 21:43:50.630: INFO: Created: latency-svc-8zhdd
Jan 22 21:43:50.669: INFO: Got endpoints: latency-svc-6b668 [749.280824ms]
Jan 22 21:43:50.679: INFO: Created: latency-svc-8r9m6
Jan 22 21:43:50.719: INFO: Got endpoints: latency-svc-2pq9g [749.295775ms]
Jan 22 21:43:50.729: INFO: Created: latency-svc-2kbbj
Jan 22 21:43:50.771: INFO: Got endpoints: latency-svc-nnvfj [750.878495ms]
Jan 22 21:43:50.781: INFO: Created: latency-svc-n2w9g
Jan 22 21:43:50.819: INFO: Got endpoints: latency-svc-rmj94 [748.486588ms]
Jan 22 21:43:50.827: INFO: Created: latency-svc-snvt2
Jan 22 21:43:50.870: INFO: Got endpoints: latency-svc-gjjxn [750.114912ms]
Jan 22 21:43:50.883: INFO: Created: latency-svc-67x7v
Jan 22 21:43:50.919: INFO: Got endpoints: latency-svc-fwxx7 [748.767748ms]
Jan 22 21:43:50.930: INFO: Created: latency-svc-zvn7c
Jan 22 21:43:50.969: INFO: Got endpoints: latency-svc-xplk6 [746.481069ms]
Jan 22 21:43:50.981: INFO: Created: latency-svc-9s5rb
Jan 22 21:43:51.019: INFO: Got endpoints: latency-svc-2z5g9 [749.009055ms]
Jan 22 21:43:51.030: INFO: Created: latency-svc-k9c4w
Jan 22 21:43:51.071: INFO: Got endpoints: latency-svc-vvh5x [751.045751ms]
Jan 22 21:43:51.081: INFO: Created: latency-svc-ql69g
Jan 22 21:43:51.119: INFO: Got endpoints: latency-svc-fpfl8 [749.805128ms]
Jan 22 21:43:51.133: INFO: Created: latency-svc-xtjp5
Jan 22 21:43:51.169: INFO: Got endpoints: latency-svc-ht5vz [750.039272ms]
Jan 22 21:43:51.186: INFO: Created: latency-svc-fxdpt
Jan 22 21:43:51.220: INFO: Got endpoints: latency-svc-kfgqj [750.617908ms]
Jan 22 21:43:51.232: INFO: Created: latency-svc-hkv6t
Jan 22 21:43:51.269: INFO: Got endpoints: latency-svc-w5hhl [749.037538ms]
Jan 22 21:43:51.283: INFO: Created: latency-svc-qzplw
Jan 22 21:43:51.320: INFO: Got endpoints: latency-svc-4xv4n [749.122813ms]
Jan 22 21:43:51.331: INFO: Created: latency-svc-xn8f6
Jan 22 21:43:51.370: INFO: Got endpoints: latency-svc-8zhdd [749.602149ms]
Jan 22 21:43:51.381: INFO: Created: latency-svc-lwx89
Jan 22 21:43:51.419: INFO: Got endpoints: latency-svc-8r9m6 [750.099677ms]
Jan 22 21:43:51.432: INFO: Created: latency-svc-b4svn
Jan 22 21:43:51.470: INFO: Got endpoints: latency-svc-2kbbj [750.206122ms]
Jan 22 21:43:51.483: INFO: Created: latency-svc-whdb7
Jan 22 21:43:51.519: INFO: Got endpoints: latency-svc-n2w9g [747.810804ms]
Jan 22 21:43:51.532: INFO: Created: latency-svc-gmfzd
Jan 22 21:43:51.570: INFO: Got endpoints: latency-svc-snvt2 [751.192263ms]
Jan 22 21:43:51.583: INFO: Created: latency-svc-xcq9k
Jan 22 21:43:51.619: INFO: Got endpoints: latency-svc-67x7v [748.981048ms]
Jan 22 21:43:51.630: INFO: Created: latency-svc-frpml
Jan 22 21:43:51.670: INFO: Got endpoints: latency-svc-zvn7c [750.436435ms]
Jan 22 21:43:51.679: INFO: Created: latency-svc-l4vxx
Jan 22 21:43:51.719: INFO: Got endpoints: latency-svc-9s5rb [750.010123ms]
Jan 22 21:43:51.731: INFO: Created: latency-svc-2fw8q
Jan 22 21:43:51.774: INFO: Got endpoints: latency-svc-k9c4w [755.012601ms]
Jan 22 21:43:51.821: INFO: Got endpoints: latency-svc-ql69g [750.747436ms]
Jan 22 21:43:51.828: INFO: Created: latency-svc-fz8mv
Jan 22 21:43:51.835: INFO: Created: latency-svc-4mkbk
Jan 22 21:43:51.870: INFO: Got endpoints: latency-svc-xtjp5 [751.014523ms]
Jan 22 21:43:51.881: INFO: Created: latency-svc-5kqn6
Jan 22 21:43:51.919: INFO: Got endpoints: latency-svc-fxdpt [749.897993ms]
Jan 22 21:43:51.930: INFO: Created: latency-svc-9bc64
Jan 22 21:43:51.970: INFO: Got endpoints: latency-svc-hkv6t [749.459101ms]
Jan 22 21:43:51.982: INFO: Created: latency-svc-66b6k
Jan 22 21:43:52.019: INFO: Got endpoints: latency-svc-qzplw [749.988942ms]
Jan 22 21:43:52.033: INFO: Created: latency-svc-jmc5l
Jan 22 21:43:52.070: INFO: Got endpoints: latency-svc-xn8f6 [750.034686ms]
Jan 22 21:43:52.082: INFO: Created: latency-svc-4vg4p
Jan 22 21:43:52.119: INFO: Got endpoints: latency-svc-lwx89 [749.597058ms]
Jan 22 21:43:52.132: INFO: Created: latency-svc-4sjlk
Jan 22 21:43:52.170: INFO: Got endpoints: latency-svc-b4svn [750.40227ms]
Jan 22 21:43:52.181: INFO: Created: latency-svc-c55c2
Jan 22 21:43:52.219: INFO: Got endpoints: latency-svc-whdb7 [749.466854ms]
Jan 22 21:43:52.230: INFO: Created: latency-svc-k2mdt
Jan 22 21:43:52.271: INFO: Got endpoints: latency-svc-gmfzd [752.022656ms]
Jan 22 21:43:52.287: INFO: Created: latency-svc-8rbpd
Jan 22 21:43:52.320: INFO: Got endpoints: latency-svc-xcq9k [749.906629ms]
Jan 22 21:43:52.333: INFO: Created: latency-svc-kblc2
Jan 22 21:43:52.369: INFO: Got endpoints: latency-svc-frpml [750.229099ms]
Jan 22 21:43:52.382: INFO: Created: latency-svc-dlxxf
Jan 22 21:43:52.420: INFO: Got endpoints: latency-svc-l4vxx [749.787549ms]
Jan 22 21:43:52.434: INFO: Created: latency-svc-j5ckj
Jan 22 21:43:52.470: INFO: Got endpoints: latency-svc-2fw8q [750.53115ms]
Jan 22 21:43:52.488: INFO: Created: latency-svc-w2tx6
Jan 22 21:43:52.519: INFO: Got endpoints: latency-svc-fz8mv [744.580535ms]
Jan 22 21:43:52.533: INFO: Created: latency-svc-dccrz
Jan 22 21:43:52.570: INFO: Got endpoints: latency-svc-4mkbk [748.27778ms]
Jan 22 21:43:52.582: INFO: Created: latency-svc-fxf4z
Jan 22 21:43:52.620: INFO: Got endpoints: latency-svc-5kqn6 [749.794869ms]
Jan 22 21:43:52.633: INFO: Created: latency-svc-ttwcw
Jan 22 21:43:52.670: INFO: Got endpoints: latency-svc-9bc64 [750.197131ms]
Jan 22 21:43:52.683: INFO: Created: latency-svc-mwmsr
Jan 22 21:43:52.720: INFO: Got endpoints: latency-svc-66b6k [750.098955ms]
Jan 22 21:43:52.734: INFO: Created: latency-svc-gc5tb
Jan 22 21:43:52.769: INFO: Got endpoints: latency-svc-jmc5l [749.676289ms]
Jan 22 21:43:52.780: INFO: Created: latency-svc-zdkjc
Jan 22 21:43:52.820: INFO: Got endpoints: latency-svc-4vg4p [750.513183ms]
Jan 22 21:43:52.831: INFO: Created: latency-svc-2h9r8
Jan 22 21:43:52.870: INFO: Got endpoints: latency-svc-4sjlk [750.774996ms]
Jan 22 21:43:52.882: INFO: Created: latency-svc-8d784
Jan 22 21:43:52.920: INFO: Got endpoints: latency-svc-c55c2 [750.523217ms]
Jan 22 21:43:52.930: INFO: Created: latency-svc-tmm4d
Jan 22 21:43:52.972: INFO: Got endpoints: latency-svc-k2mdt [752.225743ms]
Jan 22 21:43:52.984: INFO: Created: latency-svc-6vkq9
Jan 22 21:43:53.019: INFO: Got endpoints: latency-svc-8rbpd [748.479724ms]
Jan 22 21:43:53.037: INFO: Created: latency-svc-rzdfs
Jan 22 21:43:53.071: INFO: Got endpoints: latency-svc-kblc2 [751.245978ms]
Jan 22 21:43:53.082: INFO: Created: latency-svc-4gtkl
Jan 22 21:43:53.120: INFO: Got endpoints: latency-svc-dlxxf [750.796914ms]
Jan 22 21:43:53.130: INFO: Created: latency-svc-pnznt
Jan 22 21:43:53.169: INFO: Got endpoints: latency-svc-j5ckj [749.370577ms]
Jan 22 21:43:53.179: INFO: Created: latency-svc-wrnh7
Jan 22 21:43:53.219: INFO: Got endpoints: latency-svc-w2tx6 [749.175716ms]
Jan 22 21:43:53.228: INFO: Created: latency-svc-6mnkv
Jan 22 21:43:53.270: INFO: Got endpoints: latency-svc-dccrz [750.659128ms]
Jan 22 21:43:53.278: INFO: Created: latency-svc-n6vt4
Jan 22 21:43:53.320: INFO: Got endpoints: latency-svc-fxf4z [750.610353ms]
Jan 22 21:43:53.331: INFO: Created: latency-svc-6x6tt
Jan 22 21:43:53.369: INFO: Got endpoints: latency-svc-ttwcw [749.182337ms]
Jan 22 21:43:53.383: INFO: Created: latency-svc-n5lqt
Jan 22 21:43:53.419: INFO: Got endpoints: latency-svc-mwmsr [749.777798ms]
Jan 22 21:43:53.430: INFO: Created: latency-svc-7t948
Jan 22 21:43:53.470: INFO: Got endpoints: latency-svc-gc5tb [750.448197ms]
Jan 22 21:43:53.480: INFO: Created: latency-svc-dv48d
Jan 22 21:43:53.519: INFO: Got endpoints: latency-svc-zdkjc [750.22089ms]
Jan 22 21:43:53.533: INFO: Created: latency-svc-f2hf7
Jan 22 21:43:53.570: INFO: Got endpoints: latency-svc-2h9r8 [749.973732ms]
Jan 22 21:43:53.580: INFO: Created: latency-svc-4tkxx
Jan 22 21:43:53.619: INFO: Got endpoints: latency-svc-8d784 [749.041139ms]
Jan 22 21:43:53.633: INFO: Created: latency-svc-h89z8
Jan 22 21:43:53.670: INFO: Got endpoints: latency-svc-tmm4d [749.765888ms]
Jan 22 21:43:53.684: INFO: Created: latency-svc-j8xsc
Jan 22 21:43:53.724: INFO: Got endpoints: latency-svc-6vkq9 [752.318314ms]
Jan 22 21:43:53.738: INFO: Created: latency-svc-ndhvz
Jan 22 21:43:53.782: INFO: Got endpoints: latency-svc-rzdfs [762.88092ms]
Jan 22 21:43:53.797: INFO: Created: latency-svc-xt8nt
Jan 22 21:43:53.823: INFO: Got endpoints: latency-svc-4gtkl [751.946427ms]
Jan 22 21:43:53.845: INFO: Created: latency-svc-dvs8j
Jan 22 21:43:53.871: INFO: Got endpoints: latency-svc-pnznt [750.944535ms]
Jan 22 21:43:53.895: INFO: Created: latency-svc-5zrct
Jan 22 21:43:53.921: INFO: Got endpoints: latency-svc-wrnh7 [752.019251ms]
Jan 22 21:43:53.948: INFO: Created: latency-svc-ck2v4
Jan 22 21:43:53.971: INFO: Got endpoints: latency-svc-6mnkv [751.391051ms]
Jan 22 21:43:53.982: INFO: Created: latency-svc-7hp2l
Jan 22 21:43:54.019: INFO: Got endpoints: latency-svc-n6vt4 [749.743636ms]
Jan 22 21:43:54.027: INFO: Created: latency-svc-q2kjr
Jan 22 21:43:54.070: INFO: Got endpoints: latency-svc-6x6tt [749.345162ms]
Jan 22 21:43:54.080: INFO: Created: latency-svc-pzh45
Jan 22 21:43:54.120: INFO: Got endpoints: latency-svc-n5lqt [750.437792ms]
Jan 22 21:43:54.128: INFO: Created: latency-svc-wxpkx
Jan 22 21:43:54.169: INFO: Got endpoints: latency-svc-7t948 [749.916679ms]
Jan 22 21:43:54.177: INFO: Created: latency-svc-ln69w
Jan 22 21:43:54.219: INFO: Got endpoints: latency-svc-dv48d [748.600844ms]
Jan 22 21:43:54.229: INFO: Created: latency-svc-bb24q
Jan 22 21:43:54.271: INFO: Got endpoints: latency-svc-f2hf7 [751.100664ms]
Jan 22 21:43:54.282: INFO: Created: latency-svc-sqbzk
Jan 22 21:43:54.319: INFO: Got endpoints: latency-svc-4tkxx [748.87107ms]
Jan 22 21:43:54.331: INFO: Created: latency-svc-r6jjn
Jan 22 21:43:54.369: INFO: Got endpoints: latency-svc-h89z8 [749.728054ms]
Jan 22 21:43:54.380: INFO: Created: latency-svc-8s6th
Jan 22 21:43:54.421: INFO: Got endpoints: latency-svc-j8xsc [750.523497ms]
Jan 22 21:43:54.432: INFO: Created: latency-svc-v65hl
Jan 22 21:43:54.470: INFO: Got endpoints: latency-svc-ndhvz [746.072508ms]
Jan 22 21:43:54.481: INFO: Created: latency-svc-p72fw
Jan 22 21:43:54.519: INFO: Got endpoints: latency-svc-xt8nt [736.613481ms]
Jan 22 21:43:54.530: INFO: Created: latency-svc-frvkl
Jan 22 21:43:54.569: INFO: Got endpoints: latency-svc-dvs8j [745.926479ms]
Jan 22 21:43:54.580: INFO: Created: latency-svc-l9hhm
Jan 22 21:43:54.620: INFO: Got endpoints: latency-svc-5zrct [748.225314ms]
Jan 22 21:43:54.629: INFO: Created: latency-svc-t7wzn
Jan 22 21:43:54.669: INFO: Got endpoints: latency-svc-ck2v4 [748.257043ms]
Jan 22 21:43:54.678: INFO: Created: latency-svc-qb6ns
Jan 22 21:43:54.720: INFO: Got endpoints: latency-svc-7hp2l [749.407004ms]
Jan 22 21:43:54.731: INFO: Created: latency-svc-qsl4d
Jan 22 21:43:54.769: INFO: Got endpoints: latency-svc-q2kjr [749.758836ms]
Jan 22 21:43:54.778: INFO: Created: latency-svc-l8fb2
Jan 22 21:43:54.820: INFO: Got endpoints: latency-svc-pzh45 [750.223583ms]
Jan 22 21:43:54.830: INFO: Created: latency-svc-jtsfj
Jan 22 21:43:54.869: INFO: Got endpoints: latency-svc-wxpkx [749.617905ms]
Jan 22 21:43:54.879: INFO: Created: latency-svc-lcd8s
Jan 22 21:43:54.920: INFO: Got endpoints: latency-svc-ln69w [750.592731ms]
Jan 22 21:43:54.930: INFO: Created: latency-svc-z2t7d
Jan 22 21:43:54.969: INFO: Got endpoints: latency-svc-bb24q [750.446612ms]
Jan 22 21:43:54.978: INFO: Created: latency-svc-hggfc
Jan 22 21:43:55.019: INFO: Got endpoints: latency-svc-sqbzk [748.857667ms]
Jan 22 21:43:55.032: INFO: Created: latency-svc-5nxgl
Jan 22 21:43:55.070: INFO: Got endpoints: latency-svc-r6jjn [751.241566ms]
Jan 22 21:43:55.083: INFO: Created: latency-svc-nmqr4
Jan 22 21:43:55.120: INFO: Got endpoints: latency-svc-8s6th [750.612372ms]
Jan 22 21:43:55.130: INFO: Created: latency-svc-hmf2z
Jan 22 21:43:55.170: INFO: Got endpoints: latency-svc-v65hl [749.544771ms]
Jan 22 21:43:55.183: INFO: Created: latency-svc-lm4kb
Jan 22 21:43:55.219: INFO: Got endpoints: latency-svc-p72fw [748.718317ms]
Jan 22 21:43:55.229: INFO: Created: latency-svc-mm7mr
Jan 22 21:43:55.270: INFO: Got endpoints: latency-svc-frvkl [750.774943ms]
Jan 22 21:43:55.279: INFO: Created: latency-svc-xr4nx
Jan 22 21:43:55.320: INFO: Got endpoints: latency-svc-l9hhm [750.492581ms]
Jan 22 21:43:55.330: INFO: Created: latency-svc-9dcds
Jan 22 21:43:55.370: INFO: Got endpoints: latency-svc-t7wzn [750.226866ms]
Jan 22 21:43:55.381: INFO: Created: latency-svc-2jsdr
Jan 22 21:43:55.419: INFO: Got endpoints: latency-svc-qb6ns [749.540403ms]
Jan 22 21:43:55.439: INFO: Created: latency-svc-kvf27
Jan 22 21:43:55.472: INFO: Got endpoints: latency-svc-qsl4d [752.210556ms]
Jan 22 21:43:55.526: INFO: Got endpoints: latency-svc-l8fb2 [756.587436ms]
Jan 22 21:43:55.570: INFO: Got endpoints: latency-svc-jtsfj [749.547009ms]
Jan 22 21:43:55.620: INFO: Got endpoints: latency-svc-lcd8s [750.148093ms]
Jan 22 21:43:55.669: INFO: Got endpoints: latency-svc-z2t7d [749.500306ms]
Jan 22 21:43:55.719: INFO: Got endpoints: latency-svc-hggfc [749.560527ms]
Jan 22 21:43:55.770: INFO: Got endpoints: latency-svc-5nxgl [750.492939ms]
Jan 22 21:43:55.821: INFO: Got endpoints: latency-svc-nmqr4 [749.976323ms]
Jan 22 21:43:55.871: INFO: Got endpoints: latency-svc-hmf2z [751.589934ms]
Jan 22 21:43:55.921: INFO: Got endpoints: latency-svc-lm4kb [751.091195ms]
Jan 22 21:43:55.970: INFO: Got endpoints: latency-svc-mm7mr [750.813435ms]
Jan 22 21:43:56.021: INFO: Got endpoints: latency-svc-xr4nx [750.90377ms]
Jan 22 21:43:56.070: INFO: Got endpoints: latency-svc-9dcds [750.246544ms]
Jan 22 21:43:56.120: INFO: Got endpoints: latency-svc-2jsdr [750.087284ms]
Jan 22 21:43:56.170: INFO: Got endpoints: latency-svc-kvf27 [751.299405ms]
Jan 22 21:43:56.171: INFO: Latencies: [19.741884ms 41.475054ms 52.571408ms 67.742753ms 69.555666ms 77.445432ms 92.541648ms 105.950033ms 116.816879ms 140.723399ms 166.896185ms 184.866799ms 201.759535ms 220.807832ms 247.935832ms 251.216284ms 257.31826ms 258.445866ms 261.948128ms 262.032242ms 262.507631ms 265.977903ms 266.190085ms 267.114971ms 268.529785ms 272.861992ms 275.719728ms 286.649762ms 292.453792ms 293.00162ms 293.981724ms 294.850657ms 294.854026ms 296.213046ms 299.109347ms 307.619211ms 308.826109ms 310.352612ms 311.644537ms 314.078742ms 316.891058ms 322.329765ms 330.726983ms 361.522765ms 375.868374ms 387.095873ms 387.406345ms 411.807803ms 456.423801ms 517.551736ms 523.057199ms 525.307108ms 526.225101ms 547.99042ms 575.574032ms 603.635659ms 606.162586ms 609.47763ms 622.882358ms 623.911075ms 650.494026ms 652.173624ms 657.8091ms 678.478639ms 681.209833ms 696.674483ms 699.724566ms 716.649144ms 736.613481ms 737.147346ms 743.633725ms 744.580535ms 745.188335ms 745.926479ms 746.072508ms 746.481069ms 746.588551ms 746.906909ms 747.455129ms 747.67252ms 747.810804ms 747.853813ms 747.861442ms 748.225314ms 748.23318ms 748.257043ms 748.27778ms 748.380428ms 748.479724ms 748.486588ms 748.600844ms 748.718317ms 748.767748ms 748.857667ms 748.87107ms 748.881662ms 748.981048ms 749.009055ms 749.025153ms 749.037538ms 749.041139ms 749.122813ms 749.143887ms 749.175716ms 749.182337ms 749.280824ms 749.295775ms 749.345162ms 749.370577ms 749.407004ms 749.44727ms 749.459101ms 749.466854ms 749.500306ms 749.540403ms 749.544771ms 749.547009ms 749.560527ms 749.597058ms 749.602149ms 749.617905ms 749.676289ms 749.728054ms 749.743636ms 749.758836ms 749.765888ms 749.777798ms 749.787549ms 749.794869ms 749.805128ms 749.897993ms 749.906629ms 749.916679ms 749.973732ms 749.976323ms 749.988942ms 750.010123ms 750.034686ms 750.039272ms 750.087284ms 750.098955ms 750.099677ms 750.114912ms 750.125263ms 750.148093ms 750.197131ms 750.206122ms 750.22089ms 750.223583ms 750.226866ms 750.229099ms 750.246544ms 750.362408ms 750.40227ms 750.436435ms 750.437792ms 750.446612ms 750.448197ms 750.492581ms 750.492939ms 750.513183ms 750.523217ms 750.523497ms 750.53115ms 750.592731ms 750.610353ms 750.612372ms 750.617908ms 750.659128ms 750.747436ms 750.749975ms 750.774943ms 750.774996ms 750.796914ms 750.799593ms 750.813435ms 750.878495ms 750.90377ms 750.944535ms 750.964295ms 751.014523ms 751.045751ms 751.091195ms 751.100664ms 751.192263ms 751.241566ms 751.245978ms 751.299405ms 751.391051ms 751.589934ms 751.946427ms 752.019251ms 752.022656ms 752.210556ms 752.225743ms 752.318314ms 752.481324ms 755.012601ms 756.587436ms 762.88092ms]
Jan 22 21:43:56.171: INFO: 50 %ile: 749.041139ms
Jan 22 21:43:56.171: INFO: 90 %ile: 751.014523ms
Jan 22 21:43:56.171: INFO: 99 %ile: 756.587436ms
Jan 22 21:43:56.171: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:43:56.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-xndb8" for this suite.
Jan 22 21:44:18.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:44:18.209: INFO: namespace: e2e-tests-svc-latency-xndb8, resource: bindings, ignored listing per whitelist
Jan 22 21:44:18.267: INFO: namespace e2e-tests-svc-latency-xndb8 deletion completed in 22.092346153s

• [SLOW TEST:32.878 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:44:18.267: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Jan 22 21:44:18.362: INFO: Waiting up to 5m0s for pod "client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-containers-7kbwv" to be "success or failure"
Jan 22 21:44:18.367: INFO: Pod "client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142324ms
Jan 22 21:44:20.370: INFO: Pod "client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00726318s
STEP: Saw pod success
Jan 22 21:44:20.370: INFO: Pod "client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:44:20.372: INFO: Trying to get logs from node secconf-node pod client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:44:20.388: INFO: Waiting for pod client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:44:20.391: INFO: Pod client-containers-decb6ab5-1e8e-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:44:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-7kbwv" for this suite.
Jan 22 21:44:26.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:44:26.477: INFO: namespace: e2e-tests-containers-7kbwv, resource: bindings, ignored listing per whitelist
Jan 22 21:44:26.484: INFO: namespace e2e-tests-containers-7kbwv deletion completed in 6.087600651s

• [SLOW TEST:8.217 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:44:26.484: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Jan 22 21:44:26.538: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 22 21:44:26.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:44:26.663: INFO: stderr: ""
Jan 22 21:44:26.663: INFO: stdout: "service/redis-slave created\n"
Jan 22 21:44:26.663: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 22 21:44:26.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:44:26.815: INFO: stderr: ""
Jan 22 21:44:26.815: INFO: stdout: "service/redis-master created\n"
Jan 22 21:44:26.815: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 22 21:44:26.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:44:26.972: INFO: stderr: ""
Jan 22 21:44:26.972: INFO: stdout: "service/frontend created\n"
Jan 22 21:44:26.972: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 22 21:44:26.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:44:27.130: INFO: stderr: ""
Jan 22 21:44:27.130: INFO: stdout: "deployment.extensions/frontend created\n"
Jan 22 21:44:27.130: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 22 21:44:27.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:44:27.285: INFO: stderr: ""
Jan 22 21:44:27.285: INFO: stdout: "deployment.extensions/redis-master created\n"
Jan 22 21:44:27.285: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 22 21:44:27.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:44:27.423: INFO: stderr: ""
Jan 22 21:44:27.423: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Jan 22 21:44:27.423: INFO: Waiting for all frontend pods to be Running.
Jan 22 21:44:32.474: INFO: Waiting for frontend to serve content.
Jan 22 21:44:32.509: INFO: Trying to add a new entry to the guestbook.
Jan 22 21:44:32.520: INFO: Verifying that added entry can be retrieved.
Jan 22 21:44:32.533: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:44:37.550: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:44:42.566: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:44:47.581: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:44:52.597: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:44:57.612: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:45:02.628: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:45:07.646: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:45:12.658: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:45:17.674: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:45:22.688: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jan 22 21:45:27.702: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jan 22 21:45:32.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:45:32.807: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:45:32.807: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 22 21:45:32.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:45:32.901: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:45:32.901: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 22 21:45:32.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:45:33.001: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:45:33.001: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 22 21:45:33.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:45:33.097: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:45:33.097: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 22 21:45:33.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:45:33.229: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:45:33.229: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 22 21:45:33.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gdwjb'
Jan 22 21:45:33.397: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 21:45:33.397: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:45:33.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gdwjb" for this suite.
Jan 22 21:46:13.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:46:13.458: INFO: namespace: e2e-tests-kubectl-gdwjb, resource: bindings, ignored listing per whitelist
Jan 22 21:46:13.509: INFO: namespace e2e-tests-kubectl-gdwjb deletion completed in 40.104753024s

• [SLOW TEST:107.025 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:46:13.509: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:46:13.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-w2vch" to be "success or failure"
Jan 22 21:46:13.572: INFO: Pod "downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.312318ms
Jan 22 21:46:15.576: INFO: Pod "downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007573995s
STEP: Saw pod success
Jan 22 21:46:15.576: INFO: Pod "downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:46:15.579: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:46:15.603: INFO: Waiting for pod downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:46:15.608: INFO: Pod downwardapi-volume-2376bcf0-1e8f-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:46:15.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w2vch" for this suite.
Jan 22 21:46:21.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:46:21.663: INFO: namespace: e2e-tests-projected-w2vch, resource: bindings, ignored listing per whitelist
Jan 22 21:46:21.695: INFO: namespace e2e-tests-projected-w2vch deletion completed in 6.080571278s

• [SLOW TEST:8.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:46:21.695: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 22 21:46:52.289: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:46:52.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-c8wst" for this suite.
Jan 22 21:46:58.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:46:58.335: INFO: namespace: e2e-tests-gc-c8wst, resource: bindings, ignored listing per whitelist
Jan 22 21:46:58.385: INFO: namespace e2e-tests-gc-c8wst deletion completed in 6.092342013s

• [SLOW TEST:36.690 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:46:58.386: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:47:00.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-hh2wb" for this suite.
Jan 22 21:47:44.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:47:44.550: INFO: namespace: e2e-tests-kubelet-test-hh2wb, resource: bindings, ignored listing per whitelist
Jan 22 21:47:44.578: INFO: namespace e2e-tests-kubelet-test-hh2wb deletion completed in 44.088985254s

• [SLOW TEST:46.193 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:47:44.578: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:47:44.653: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 22 21:47:49.656: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 22 21:47:49.656: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 22 21:47:51.660: INFO: Creating deployment "test-rollover-deployment"
Jan 22 21:47:51.668: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 22 21:47:53.674: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 22 21:47:53.680: INFO: Ensure that both replica sets have 1 created replica
Jan 22 21:47:53.685: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 22 21:47:53.692: INFO: Updating deployment test-rollover-deployment
Jan 22 21:47:53.692: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 22 21:47:55.698: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 22 21:47:55.703: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 22 21:47:55.707: INFO: all replica sets need to contain the pod-template-hash label
Jan 22 21:47:55.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790475, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 22 21:47:57.714: INFO: all replica sets need to contain the pod-template-hash label
Jan 22 21:47:57.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790475, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 22 21:47:59.713: INFO: all replica sets need to contain the pod-template-hash label
Jan 22 21:47:59.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790475, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 22 21:48:01.715: INFO: all replica sets need to contain the pod-template-hash label
Jan 22 21:48:01.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790475, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 22 21:48:03.714: INFO: all replica sets need to contain the pod-template-hash label
Jan 22 21:48:03.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790475, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790471, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 22 21:48:05.714: INFO: 
Jan 22 21:48:05.714: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 22 21:48:05.722: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-jps4t,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jps4t/deployments/test-rollover-deployment,UID:5def5bb8-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37247,Generation:2,CreationTimestamp:2019-01-22 21:47:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-22 21:47:51 +0000 UTC 2019-01-22 21:47:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-22 21:48:05 +0000 UTC 2019-01-22 21:47:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 22 21:48:05.726: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-jps4t,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jps4t/replicasets/test-rollover-deployment-6b7f9d6597,UID:5f254d9f-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37238,Generation:2,CreationTimestamp:2019-01-22 21:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5def5bb8-1e8f-11e9-945c-000c293afc9e 0xc001ea6ca7 0xc001ea6ca8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 22 21:48:05.726: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 22 21:48:05.726: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-jps4t,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jps4t/replicasets/test-rollover-controller,UID:59c11b41-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37246,Generation:2,CreationTimestamp:2019-01-22 21:47:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5def5bb8-1e8f-11e9-945c-000c293afc9e 0xc001ea6aa7 0xc001ea6aa8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 22 21:48:05.726: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-jps4t,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-jps4t/replicasets/test-rollover-deployment-6586df867b,UID:5df1a8fc-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37214,Generation:2,CreationTimestamp:2019-01-22 21:47:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5def5bb8-1e8f-11e9-945c-000c293afc9e 0xc001ea6b67 0xc001ea6b68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 22 21:48:05.730: INFO: Pod "test-rollover-deployment-6b7f9d6597-q58k8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-q58k8,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-jps4t,SelfLink:/api/v1/namespaces/e2e-tests-deployment-jps4t/pods/test-rollover-deployment-6b7f9d6597-q58k8,UID:5f27ef47-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37222,Generation:0,CreationTimestamp:2019-01-22 21:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.226/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 5f254d9f-1e8f-11e9-945c-000c293afc9e 0xc002002707 0xc002002708}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-676g9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-676g9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-676g9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002002790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002002a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:47:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:47:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:47:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:47:53 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.226,StartTime:2019-01-22 21:47:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-22 21:47:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://d4ed4dfba0e58764c6d24f40accef4372e4f30bae4b77423d31bef619f5e6a68}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:48:05.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-jps4t" for this suite.
Jan 22 21:48:11.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:48:11.805: INFO: namespace: e2e-tests-deployment-jps4t, resource: bindings, ignored listing per whitelist
Jan 22 21:48:11.826: INFO: namespace e2e-tests-deployment-jps4t deletion completed in 6.092592712s

• [SLOW TEST:27.248 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:48:11.826: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 21:48:11.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-jgjgv'
Jan 22 21:48:11.972: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 22 21:48:11.972: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Jan 22 21:48:11.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-jgjgv'
Jan 22 21:48:12.061: INFO: stderr: ""
Jan 22 21:48:12.061: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:48:12.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jgjgv" for this suite.
Jan 22 21:48:34.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:48:34.136: INFO: namespace: e2e-tests-kubectl-jgjgv, resource: bindings, ignored listing per whitelist
Jan 22 21:48:34.147: INFO: namespace e2e-tests-kubectl-jgjgv deletion completed in 22.082732493s

• [SLOW TEST:22.321 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:48:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 22 21:48:34.202: INFO: namespace e2e-tests-kubectl-rg5r2
Jan 22 21:48:34.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-rg5r2'
Jan 22 21:48:34.329: INFO: stderr: ""
Jan 22 21:48:34.329: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 22 21:48:35.332: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 21:48:35.332: INFO: Found 1 / 1
Jan 22 21:48:35.332: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 22 21:48:35.335: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 21:48:35.335: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 22 21:48:35.335: INFO: wait on redis-master startup in e2e-tests-kubectl-rg5r2 
Jan 22 21:48:35.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-jvk6z redis-master --namespace=e2e-tests-kubectl-rg5r2'
Jan 22 21:48:35.420: INFO: stderr: ""
Jan 22 21:48:35.420: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n1:M 22 Jan 21:48:35.118 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jan 21:48:35.118 # Server started, Redis version 3.2.12\n1:M 22 Jan 21:48:35.118 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jan 21:48:35.118 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 22 21:48:35.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-rg5r2'
Jan 22 21:48:35.513: INFO: stderr: ""
Jan 22 21:48:35.513: INFO: stdout: "service/rm2 exposed\n"
Jan 22 21:48:35.517: INFO: Service rm2 in namespace e2e-tests-kubectl-rg5r2 found.
STEP: exposing service
Jan 22 21:48:37.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-rg5r2'
Jan 22 21:48:37.610: INFO: stderr: ""
Jan 22 21:48:37.610: INFO: stdout: "service/rm3 exposed\n"
Jan 22 21:48:37.614: INFO: Service rm3 in namespace e2e-tests-kubectl-rg5r2 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:48:39.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rg5r2" for this suite.
Jan 22 21:49:01.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:49:01.710: INFO: namespace: e2e-tests-kubectl-rg5r2, resource: bindings, ignored listing per whitelist
Jan 22 21:49:01.712: INFO: namespace e2e-tests-kubectl-rg5r2 deletion completed in 22.08785223s

• [SLOW TEST:27.565 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:49:01.713: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-87b977f7-1e8f-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:49:01.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-2xztn" to be "success or failure"
Jan 22 21:49:01.786: INFO: Pod "pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.867059ms
Jan 22 21:49:03.789: INFO: Pod "pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006137808s
STEP: Saw pod success
Jan 22 21:49:03.789: INFO: Pod "pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:49:03.792: INFO: Trying to get logs from node secconf-node pod pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:49:03.807: INFO: Waiting for pod pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:49:03.813: INFO: Pod pod-configmaps-87b9ef7c-1e8f-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:49:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2xztn" for this suite.
Jan 22 21:49:09.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:49:09.883: INFO: namespace: e2e-tests-configmap-2xztn, resource: bindings, ignored listing per whitelist
Jan 22 21:49:09.922: INFO: namespace e2e-tests-configmap-2xztn deletion completed in 6.103770632s

• [SLOW TEST:8.209 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:49:09.922: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Jan 22 21:49:09.986: INFO: Waiting up to 5m0s for pod "var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-var-expansion-rqlq4" to be "success or failure"
Jan 22 21:49:09.989: INFO: Pod "var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690697ms
Jan 22 21:49:11.992: INFO: Pod "var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006163407s
STEP: Saw pod success
Jan 22 21:49:11.992: INFO: Pod "var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:49:11.995: INFO: Trying to get logs from node secconf-node pod var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 21:49:12.011: INFO: Waiting for pod var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:49:12.014: INFO: Pod var-expansion-8c9db618-1e8f-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:49:12.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-rqlq4" for this suite.
Jan 22 21:49:18.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:49:18.085: INFO: namespace: e2e-tests-var-expansion-rqlq4, resource: bindings, ignored listing per whitelist
Jan 22 21:49:18.101: INFO: namespace e2e-tests-var-expansion-rqlq4 deletion completed in 6.083343092s

• [SLOW TEST:8.179 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:49:18.101: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 22 21:49:20.688: INFO: Successfully updated pod "pod-update-activedeadlineseconds-917e1775-1e8f-11e9-9284-e2fd7d97dccb"
Jan 22 21:49:20.688: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-917e1775-1e8f-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-pods-sv69d" to be "terminated due to deadline exceeded"
Jan 22 21:49:20.696: INFO: Pod "pod-update-activedeadlineseconds-917e1775-1e8f-11e9-9284-e2fd7d97dccb": Phase="Running", Reason="", readiness=true. Elapsed: 8.473893ms
Jan 22 21:49:22.700: INFO: Pod "pod-update-activedeadlineseconds-917e1775-1e8f-11e9-9284-e2fd7d97dccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01156786s
Jan 22 21:49:24.703: INFO: Pod "pod-update-activedeadlineseconds-917e1775-1e8f-11e9-9284-e2fd7d97dccb": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015414784s
Jan 22 21:49:24.703: INFO: Pod "pod-update-activedeadlineseconds-917e1775-1e8f-11e9-9284-e2fd7d97dccb" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:49:24.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-sv69d" for this suite.
Jan 22 21:49:30.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:49:30.770: INFO: namespace: e2e-tests-pods-sv69d, resource: bindings, ignored listing per whitelist
Jan 22 21:49:30.790: INFO: namespace e2e-tests-pods-sv69d deletion completed in 6.083003696s

• [SLOW TEST:12.688 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:49:30.790: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:49:30.847: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 22 21:49:30.853: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 22 21:49:35.859: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 22 21:49:35.859: INFO: Creating deployment "test-rolling-update-deployment"
Jan 22 21:49:35.863: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 22 21:49:35.869: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 22 21:49:37.875: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 22 21:49:37.878: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 22 21:49:37.886: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-6lm76,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6lm76/deployments/test-rolling-update-deployment,UID:9c0ac995-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37612,Generation:1,CreationTimestamp:2019-01-22 21:49:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-22 21:49:35 +0000 UTC 2019-01-22 21:49:35 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-22 21:49:37 +0000 UTC 2019-01-22 21:49:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 22 21:49:37.889: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-6lm76,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6lm76/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:9c0cc758-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37603,Generation:1,CreationTimestamp:2019-01-22 21:49:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9c0ac995-1e8f-11e9-945c-000c293afc9e 0xc001dacc87 0xc001dacc88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 22 21:49:37.889: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 22 21:49:37.889: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-6lm76,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6lm76/replicasets/test-rolling-update-controller,UID:990e09b2-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37611,Generation:2,CreationTimestamp:2019-01-22 21:49:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9c0ac995-1e8f-11e9-945c-000c293afc9e 0xc001dacbc7 0xc001dacbc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 22 21:49:37.892: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-rmxsq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-rmxsq,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-6lm76,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6lm76/pods/test-rolling-update-deployment-68b55d7bc6-rmxsq,UID:9c0d3e90-1e8f-11e9-945c-000c293afc9e,ResourceVersion:37602,Generation:0,CreationTimestamp:2019-01-22 21:49:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.232/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 9c0cc758-1e8f-11e9-945c-000c293afc9e 0xc001c9bb67 0xc001c9bb68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6qzmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6qzmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6qzmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c9bbe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c9bc00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:49:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:49:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:49:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:49:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.232,StartTime:2019-01-22 21:49:35 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-22 21:49:36 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1f90d030a1420a2b8d624b156d0b9247454ae62e0f205d4847f61b4afcdd3572}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:49:37.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-6lm76" for this suite.
Jan 22 21:49:43.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:49:43.923: INFO: namespace: e2e-tests-deployment-6lm76, resource: bindings, ignored listing per whitelist
Jan 22 21:49:43.986: INFO: namespace e2e-tests-deployment-6lm76 deletion completed in 6.090947888s

• [SLOW TEST:13.197 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:49:43.986: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-lnjr
STEP: Creating a pod to test atomic-volume-subpath
Jan 22 21:49:44.057: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lnjr" in namespace "e2e-tests-subpath-slfhr" to be "success or failure"
Jan 22 21:49:44.060: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.057665ms
Jan 22 21:49:46.063: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006474922s
Jan 22 21:49:48.068: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 4.010626127s
Jan 22 21:49:50.073: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 6.015876068s
Jan 22 21:49:52.076: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 8.019352844s
Jan 22 21:49:54.081: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 10.024091281s
Jan 22 21:49:56.086: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 12.028796986s
Jan 22 21:49:58.091: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 14.034003966s
Jan 22 21:50:00.094: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 16.03702034s
Jan 22 21:50:02.098: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 18.040946809s
Jan 22 21:50:04.101: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 20.044420662s
Jan 22 21:50:06.107: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Running", Reason="", readiness=false. Elapsed: 22.050269065s
Jan 22 21:50:08.113: INFO: Pod "pod-subpath-test-configmap-lnjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055537869s
STEP: Saw pod success
Jan 22 21:50:08.113: INFO: Pod "pod-subpath-test-configmap-lnjr" satisfied condition "success or failure"
Jan 22 21:50:08.115: INFO: Trying to get logs from node secconf-node pod pod-subpath-test-configmap-lnjr container test-container-subpath-configmap-lnjr: <nil>
STEP: delete the pod
Jan 22 21:50:08.137: INFO: Waiting for pod pod-subpath-test-configmap-lnjr to disappear
Jan 22 21:50:08.143: INFO: Pod pod-subpath-test-configmap-lnjr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lnjr
Jan 22 21:50:08.143: INFO: Deleting pod "pod-subpath-test-configmap-lnjr" in namespace "e2e-tests-subpath-slfhr"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:50:08.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-slfhr" for this suite.
Jan 22 21:50:14.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:50:14.176: INFO: namespace: e2e-tests-subpath-slfhr, resource: bindings, ignored listing per whitelist
Jan 22 21:50:14.227: INFO: namespace e2e-tests-subpath-slfhr deletion completed in 6.077473315s

• [SLOW TEST:30.241 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:50:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Jan 22 21:50:14.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:14.498: INFO: stderr: ""
Jan 22 21:50:14.498: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 22 21:50:14.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:14.586: INFO: stderr: ""
Jan 22 21:50:14.587: INFO: stdout: "update-demo-nautilus-pqb2r update-demo-nautilus-srsfd "
Jan 22 21:50:14.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-pqb2r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:14.655: INFO: stderr: ""
Jan 22 21:50:14.655: INFO: stdout: ""
Jan 22 21:50:14.655: INFO: update-demo-nautilus-pqb2r is created but not running
Jan 22 21:50:19.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:19.734: INFO: stderr: ""
Jan 22 21:50:19.734: INFO: stdout: "update-demo-nautilus-pqb2r update-demo-nautilus-srsfd "
Jan 22 21:50:19.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-pqb2r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:19.801: INFO: stderr: ""
Jan 22 21:50:19.801: INFO: stdout: "true"
Jan 22 21:50:19.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-pqb2r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:19.868: INFO: stderr: ""
Jan 22 21:50:19.868: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:50:19.868: INFO: validating pod update-demo-nautilus-pqb2r
Jan 22 21:50:19.874: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:50:19.874: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:50:19.874: INFO: update-demo-nautilus-pqb2r is verified up and running
Jan 22 21:50:19.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-srsfd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:19.938: INFO: stderr: ""
Jan 22 21:50:19.938: INFO: stdout: "true"
Jan 22 21:50:19.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-srsfd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:20.005: INFO: stderr: ""
Jan 22 21:50:20.005: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 21:50:20.005: INFO: validating pod update-demo-nautilus-srsfd
Jan 22 21:50:20.010: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 21:50:20.010: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 21:50:20.010: INFO: update-demo-nautilus-srsfd is verified up and running
STEP: rolling-update to new replication controller
Jan 22 21:50:20.011: INFO: scanned /root for discovery docs: <nil>
Jan 22 21:50:20.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:42.306: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 22 21:50:42.306: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 22 21:50:42.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:42.389: INFO: stderr: ""
Jan 22 21:50:42.389: INFO: stdout: "update-demo-kitten-gs2wg update-demo-kitten-rgcsh "
Jan 22 21:50:42.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-kitten-gs2wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:42.461: INFO: stderr: ""
Jan 22 21:50:42.461: INFO: stdout: "true"
Jan 22 21:50:42.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-kitten-gs2wg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:42.530: INFO: stderr: ""
Jan 22 21:50:42.530: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 22 21:50:42.530: INFO: validating pod update-demo-kitten-gs2wg
Jan 22 21:50:42.535: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 22 21:50:42.535: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 22 21:50:42.535: INFO: update-demo-kitten-gs2wg is verified up and running
Jan 22 21:50:42.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-kitten-rgcsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:42.603: INFO: stderr: ""
Jan 22 21:50:42.603: INFO: stdout: "true"
Jan 22 21:50:42.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-kitten-rgcsh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fskb9'
Jan 22 21:50:42.671: INFO: stderr: ""
Jan 22 21:50:42.671: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 22 21:50:42.671: INFO: validating pod update-demo-kitten-rgcsh
Jan 22 21:50:42.677: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 22 21:50:42.677: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 22 21:50:42.677: INFO: update-demo-kitten-rgcsh is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:50:42.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fskb9" for this suite.
Jan 22 21:51:04.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:51:04.739: INFO: namespace: e2e-tests-kubectl-fskb9, resource: bindings, ignored listing per whitelist
Jan 22 21:51:04.763: INFO: namespace e2e-tests-kubectl-fskb9 deletion completed in 22.082670932s

• [SLOW TEST:50.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:51:04.763: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 22 21:51:07.351: INFO: Successfully updated pod "labelsupdated110ba12-1e8f-11e9-9284-e2fd7d97dccb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:51:11.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6xw9v" for this suite.
Jan 22 21:51:33.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:51:33.403: INFO: namespace: e2e-tests-downward-api-6xw9v, resource: bindings, ignored listing per whitelist
Jan 22 21:51:33.462: INFO: namespace e2e-tests-downward-api-6xw9v deletion completed in 22.082702114s

• [SLOW TEST:28.700 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:51:33.462: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 22 21:51:33.519: INFO: PodSpec: initContainers in spec.initContainers
Jan 22 21:52:16.433: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e22bda4c-1e8f-11e9-9284-e2fd7d97dccb", GenerateName:"", Namespace:"e2e-tests-init-container-4vhjg", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-4vhjg/pods/pod-init-e22bda4c-1e8f-11e9-9284-e2fd7d97dccb", UID:"e22c9a4f-1e8f-11e9-945c-000c293afc9e", ResourceVersion:"38078", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63683790693, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"519224825"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.100.1.239/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-2nqj4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001addf00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2nqj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2nqj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2nqj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000db5c08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"secconf-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001735ec0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000db5c90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000db5cb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000db5cb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000db5cbc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790693, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790693, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790693, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683790693, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.43.101", PodIP:"100.100.1.239", StartTime:(*v1.Time)(0xc001a99100), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006d69a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006d70a0)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://6b218d575bc1b4634a17e8949f3bc05f5b739fe3c507136b31ef6a462429b878"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a99160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a99140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:52:16.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4vhjg" for this suite.
Jan 22 21:52:38.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:52:38.519: INFO: namespace: e2e-tests-init-container-4vhjg, resource: bindings, ignored listing per whitelist
Jan 22 21:52:38.526: INFO: namespace e2e-tests-init-container-4vhjg deletion completed in 22.088041325s

• [SLOW TEST:65.064 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:52:38.527: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 21:52:38.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-h76lz'
Jan 22 21:52:38.658: INFO: stderr: ""
Jan 22 21:52:38.658: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 22 21:52:43.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-h76lz -o json'
Jan 22 21:52:43.783: INFO: stderr: ""
Jan 22 21:52:43.783: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.100.1.240/32\"\n        },\n        \"creationTimestamp\": \"2019-01-22T21:52:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-h76lz\",\n        \"resourceVersion\": \"38137\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-h76lz/pods/e2e-test-nginx-pod\",\n        \"uid\": \"08fd4f04-1e90-11e9-945c-000c293afc9e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5n6rp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"secconf-node\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5n6rp\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5n6rp\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-22T21:52:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-22T21:52:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-22T21:52:39Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-22T21:52:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a1c38da79039e2575aa11726480d1c7e394ba083cc9f4f514ba766190c2f3344\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-01-22T21:52:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.43.101\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.100.1.240\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-01-22T21:52:38Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 22 21:52:43.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 replace -f - --namespace=e2e-tests-kubectl-h76lz'
Jan 22 21:52:43.903: INFO: stderr: ""
Jan 22 21:52:43.903: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Jan 22 21:52:43.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-h76lz'
Jan 22 21:52:51.967: INFO: stderr: ""
Jan 22 21:52:51.967: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:52:51.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h76lz" for this suite.
Jan 22 21:52:57.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:52:58.046: INFO: namespace: e2e-tests-kubectl-h76lz, resource: bindings, ignored listing per whitelist
Jan 22 21:52:58.048: INFO: namespace e2e-tests-kubectl-h76lz deletion completed in 6.076065476s

• [SLOW TEST:19.521 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:52:58.048: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1495c1a2-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:52:58.106: INFO: Waiting up to 5m0s for pod "pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-q29kn" to be "success or failure"
Jan 22 21:52:58.109: INFO: Pod "pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.481984ms
Jan 22 21:53:00.112: INFO: Pod "pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006041071s
STEP: Saw pod success
Jan 22 21:53:00.112: INFO: Pod "pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:53:00.115: INFO: Trying to get logs from node secconf-node pod pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:53:00.135: INFO: Waiting for pod pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:53:00.139: INFO: Pod pod-secrets-14962555-1e90-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:53:00.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-q29kn" for this suite.
Jan 22 21:53:06.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:53:06.200: INFO: namespace: e2e-tests-secrets-q29kn, resource: bindings, ignored listing per whitelist
Jan 22 21:53:06.232: INFO: namespace e2e-tests-secrets-q29kn deletion completed in 6.089300716s

• [SLOW TEST:8.184 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:53:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-1976e947-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 21:53:06.293: INFO: Waiting up to 5m0s for pod "pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-xttst" to be "success or failure"
Jan 22 21:53:06.303: INFO: Pod "pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.402779ms
Jan 22 21:53:08.309: INFO: Pod "pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015576441s
STEP: Saw pod success
Jan 22 21:53:08.309: INFO: Pod "pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:53:08.312: INFO: Trying to get logs from node secconf-node pod pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 21:53:08.329: INFO: Waiting for pod pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:53:08.332: INFO: Pod pod-configmaps-1977705a-1e90-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:53:08.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xttst" for this suite.
Jan 22 21:53:14.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:53:14.403: INFO: namespace: e2e-tests-configmap-xttst, resource: bindings, ignored listing per whitelist
Jan 22 21:53:14.422: INFO: namespace e2e-tests-configmap-xttst deletion completed in 6.085147798s

• [SLOW TEST:8.189 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:53:14.422: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:53:14.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-94w4l" to be "success or failure"
Jan 22 21:53:14.487: INFO: Pod "downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.114306ms
Jan 22 21:53:16.494: INFO: Pod "downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01011647s
STEP: Saw pod success
Jan 22 21:53:16.494: INFO: Pod "downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:53:16.497: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:53:16.524: INFO: Waiting for pod downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:53:16.527: INFO: Pod downwardapi-volume-1e592e58-1e90-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:53:16.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-94w4l" for this suite.
Jan 22 21:53:22.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:53:22.575: INFO: namespace: e2e-tests-downward-api-94w4l, resource: bindings, ignored listing per whitelist
Jan 22 21:53:22.617: INFO: namespace e2e-tests-downward-api-94w4l deletion completed in 6.084240144s

• [SLOW TEST:8.195 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:53:22.617: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 22 21:53:22.687: INFO: Waiting up to 5m0s for pod "pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-z4ggx" to be "success or failure"
Jan 22 21:53:22.692: INFO: Pod "pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.087206ms
Jan 22 21:53:24.696: INFO: Pod "pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008807828s
STEP: Saw pod success
Jan 22 21:53:24.696: INFO: Pod "pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:53:24.699: INFO: Trying to get logs from node secconf-node pod pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 21:53:24.715: INFO: Waiting for pod pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:53:24.718: INFO: Pod pod-233ca14f-1e90-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:53:24.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z4ggx" for this suite.
Jan 22 21:53:30.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:53:30.765: INFO: namespace: e2e-tests-emptydir-z4ggx, resource: bindings, ignored listing per whitelist
Jan 22 21:53:30.802: INFO: namespace e2e-tests-emptydir-z4ggx deletion completed in 6.08118986s

• [SLOW TEST:8.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:53:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 21:53:30.863: INFO: Waiting up to 5m0s for pod "downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-pt7tv" to be "success or failure"
Jan 22 21:53:30.866: INFO: Pod "downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.28488ms
Jan 22 21:53:32.870: INFO: Pod "downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006812513s
STEP: Saw pod success
Jan 22 21:53:32.870: INFO: Pod "downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:53:32.872: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 21:53:32.891: INFO: Waiting for pod downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:53:32.896: INFO: Pod downwardapi-volume-281c71f9-1e90-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:53:32.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pt7tv" for this suite.
Jan 22 21:53:38.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:53:38.930: INFO: namespace: e2e-tests-projected-pt7tv, resource: bindings, ignored listing per whitelist
Jan 22 21:53:38.981: INFO: namespace e2e-tests-projected-pt7tv deletion completed in 6.080213529s

• [SLOW TEST:8.179 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:53:38.982: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-2cfbae7b-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 21:53:39.045: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-5zhg7" to be "success or failure"
Jan 22 21:53:39.049: INFO: Pod "pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.178096ms
Jan 22 21:53:41.054: INFO: Pod "pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009222768s
STEP: Saw pod success
Jan 22 21:53:41.054: INFO: Pod "pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:53:41.057: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 22 21:53:41.075: INFO: Waiting for pod pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:53:41.078: INFO: Pod pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 21:53:41.078: INFO: Unexpected error occurred: expected "content of file \"/etc/projected-secret-volume/new-path-data-1\": value-1" in container output: Expected
    <string>: 
to contain substring
    <string>: content of file "/etc/projected-secret-volume/new-path-data-1": value-1
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-5zhg7".
STEP: Found 4 events.
Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-5zhg7/pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 21:53:41.094: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 21:53:41.094: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 21:53:41.094: INFO: 
Jan 22 21:53:41.098: INFO: 
Logging node info for node secconf-master
Jan 22 21:53:41.100: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:38343,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 21:53:32 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 21:53:32 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 21:53:32 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 21:53:32 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 21:53:41.101: INFO: 
Logging kubelet events for node secconf-master
Jan 22 21:53:41.104: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 21:53:41.154: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 21:53:41.154: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 21:53:41.154: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 21:53:41.154: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 21:53:41.154: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 21:53:41.154: INFO: 	Container coredns ready: true, restart count 2
Jan 22 21:53:41.154: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 21:53:41.154: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 21:53:41.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 21:53:41.154: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 21:53:41.154: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 21:53:41.154: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 21:53:41.154: INFO: 	Container coredns ready: true, restart count 2
Jan 22 21:53:41.154: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 21:53:41.154: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 21:53:41.154: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 21:53:41.188: INFO: 
Latency metrics for node secconf-master
Jan 22 21:53:41.188: INFO: 
Logging node info for node secconf-node
Jan 22 21:53:41.191: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:38372,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 21:53:41.191: INFO: 
Logging kubelet events for node secconf-node
Jan 22 21:53:41.193: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 21:53:41.198: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 21:53:41.198: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 21:53:41.199: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 21:53:41.199: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 21:53:41.199: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 21:53:41.199: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 21:53:41.199: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 21:53:41.199: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 21:53:41.199: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 21:53:41.199: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 21:53:41.199: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 21:53:41.199: INFO: 	Container e2e ready: true, restart count 0
Jan 22 21:53:41.199: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 21:53:41.225: INFO: 
Latency metrics for node secconf-node
Jan 22 21:53:41.225: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.07586s}
Jan 22 21:53:41.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5zhg7" for this suite.
Jan 22 21:53:47.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:53:47.291: INFO: namespace: e2e-tests-projected-5zhg7, resource: bindings, ignored listing per whitelist
Jan 22 21:53:47.308: INFO: namespace e2e-tests-projected-5zhg7 deletion completed in 6.079748227s

• Failure [8.326 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc000b7e290>: {
          s: "expected \"content of file \\\"/etc/projected-secret-volume/new-path-data-1\\\": value-1\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: content of file \"/etc/projected-secret-volume/new-path-data-1\": value-1",
      }
      expected "content of file \"/etc/projected-secret-volume/new-path-data-1\": value-1" in container output: Expected
          <string>: 
      to contain substring
          <string>: content of file "/etc/projected-secret-volume/new-path-data-1": value-1
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:53:47.308: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-tm5q
STEP: Creating a pod to test atomic-volume-subpath
Jan 22 21:53:47.381: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tm5q" in namespace "e2e-tests-subpath-v8grq" to be "success or failure"
Jan 22 21:53:47.384: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376157ms
Jan 22 21:53:49.387: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006389003s
Jan 22 21:53:51.390: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 4.009485901s
Jan 22 21:53:53.394: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 6.01365232s
Jan 22 21:53:55.398: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 8.017082088s
Jan 22 21:53:57.401: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 10.019985273s
Jan 22 21:53:59.405: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 12.024133066s
Jan 22 21:54:01.409: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 14.028743742s
Jan 22 21:54:03.413: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 16.032877605s
Jan 22 21:54:05.417: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 18.036509509s
Jan 22 21:54:07.421: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 20.04091495s
Jan 22 21:54:09.425: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Running", Reason="", readiness=false. Elapsed: 22.04467708s
Jan 22 21:54:11.429: INFO: Pod "pod-subpath-test-downwardapi-tm5q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.048781951s
STEP: Saw pod success
Jan 22 21:54:11.431: INFO: Pod "pod-subpath-test-downwardapi-tm5q" satisfied condition "success or failure"
Jan 22 21:54:11.434: INFO: Trying to get logs from node secconf-node pod pod-subpath-test-downwardapi-tm5q container test-container-subpath-downwardapi-tm5q: <nil>
STEP: delete the pod
Jan 22 21:54:11.452: INFO: Waiting for pod pod-subpath-test-downwardapi-tm5q to disappear
Jan 22 21:54:11.456: INFO: Pod pod-subpath-test-downwardapi-tm5q no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tm5q
Jan 22 21:54:11.456: INFO: Deleting pod "pod-subpath-test-downwardapi-tm5q" in namespace "e2e-tests-subpath-v8grq"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:54:11.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-v8grq" for this suite.
Jan 22 21:54:17.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:54:17.491: INFO: namespace: e2e-tests-subpath-v8grq, resource: bindings, ignored listing per whitelist
Jan 22 21:54:17.566: INFO: namespace e2e-tests-subpath-v8grq deletion completed in 6.101867988s

• [SLOW TEST:30.258 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:54:17.566: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 21:54:17.648: INFO: (0) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 20.592977ms)
Jan 22 21:54:17.652: INFO: (1) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.617628ms)
Jan 22 21:54:17.656: INFO: (2) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.792476ms)
Jan 22 21:54:17.660: INFO: (3) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.897798ms)
Jan 22 21:54:17.664: INFO: (4) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.108206ms)
Jan 22 21:54:17.667: INFO: (5) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.270139ms)
Jan 22 21:54:17.671: INFO: (6) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.549703ms)
Jan 22 21:54:17.674: INFO: (7) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.056537ms)
Jan 22 21:54:17.677: INFO: (8) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.460903ms)
Jan 22 21:54:17.683: INFO: (9) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.680813ms)
Jan 22 21:54:17.688: INFO: (10) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.217389ms)
Jan 22 21:54:17.692: INFO: (11) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.552895ms)
Jan 22 21:54:17.695: INFO: (12) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.554787ms)
Jan 22 21:54:17.698: INFO: (13) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.065133ms)
Jan 22 21:54:17.702: INFO: (14) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.266468ms)
Jan 22 21:54:17.705: INFO: (15) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.450577ms)
Jan 22 21:54:17.708: INFO: (16) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.268277ms)
Jan 22 21:54:17.712: INFO: (17) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.093669ms)
Jan 22 21:54:17.715: INFO: (18) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.416577ms)
Jan 22 21:54:17.719: INFO: (19) /api/v1/nodes/secconf-node:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.495734ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:54:17.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-ft927" for this suite.
Jan 22 21:54:23.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:54:23.792: INFO: namespace: e2e-tests-proxy-ft927, resource: bindings, ignored listing per whitelist
Jan 22 21:54:23.799: INFO: namespace e2e-tests-proxy-ft927 deletion completed in 6.077797365s

• [SLOW TEST:6.233 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:54:23.799: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4nvq5
Jan 22 21:54:25.866: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4nvq5
STEP: checking the pod's current state and verifying that restartCount is present
Jan 22 21:54:25.868: INFO: Initial restart count of pod liveness-http is 0
Jan 22 21:54:43.912: INFO: Restart count of pod e2e-tests-container-probe-4nvq5/liveness-http is now 1 (18.043290298s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:54:43.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4nvq5" for this suite.
Jan 22 21:54:49.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:54:50.010: INFO: namespace: e2e-tests-container-probe-4nvq5, resource: bindings, ignored listing per whitelist
Jan 22 21:54:50.020: INFO: namespace e2e-tests-container-probe-4nvq5 deletion completed in 6.094946554s

• [SLOW TEST:26.221 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:54:50.020: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 22 21:54:50.092: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tl74w,SelfLink:/api/v1/namespaces/e2e-tests-watch-tl74w/configmaps/e2e-watch-test-label-changed,UID:5754cfb9-1e90-11e9-945c-000c293afc9e,ResourceVersion:38574,Generation:0,CreationTimestamp:2019-01-22 21:54:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 22 21:54:50.092: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tl74w,SelfLink:/api/v1/namespaces/e2e-tests-watch-tl74w/configmaps/e2e-watch-test-label-changed,UID:5754cfb9-1e90-11e9-945c-000c293afc9e,ResourceVersion:38575,Generation:0,CreationTimestamp:2019-01-22 21:54:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 22 21:54:50.092: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tl74w,SelfLink:/api/v1/namespaces/e2e-tests-watch-tl74w/configmaps/e2e-watch-test-label-changed,UID:5754cfb9-1e90-11e9-945c-000c293afc9e,ResourceVersion:38576,Generation:0,CreationTimestamp:2019-01-22 21:54:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 22 21:55:00.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tl74w,SelfLink:/api/v1/namespaces/e2e-tests-watch-tl74w/configmaps/e2e-watch-test-label-changed,UID:5754cfb9-1e90-11e9-945c-000c293afc9e,ResourceVersion:38591,Generation:0,CreationTimestamp:2019-01-22 21:54:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 22 21:55:00.119: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tl74w,SelfLink:/api/v1/namespaces/e2e-tests-watch-tl74w/configmaps/e2e-watch-test-label-changed,UID:5754cfb9-1e90-11e9-945c-000c293afc9e,ResourceVersion:38592,Generation:0,CreationTimestamp:2019-01-22 21:54:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 22 21:55:00.120: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tl74w,SelfLink:/api/v1/namespaces/e2e-tests-watch-tl74w/configmaps/e2e-watch-test-label-changed,UID:5754cfb9-1e90-11e9-945c-000c293afc9e,ResourceVersion:38593,Generation:0,CreationTimestamp:2019-01-22 21:54:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:55:00.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-tl74w" for this suite.
Jan 22 21:55:06.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:55:06.193: INFO: namespace: e2e-tests-watch-tl74w, resource: bindings, ignored listing per whitelist
Jan 22 21:55:06.209: INFO: namespace e2e-tests-watch-tl74w deletion completed in 6.084075355s

• [SLOW TEST:16.188 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:55:06.209: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 22 21:55:10.324: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:10.327: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:12.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:12.332: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:14.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:14.332: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:16.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:16.333: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:18.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:18.333: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:20.329: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:20.334: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:22.329: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:22.334: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:24.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:24.331: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:26.327: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:26.331: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:28.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:28.332: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 22 21:55:30.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 22 21:55:30.331: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:55:30.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-58n68" for this suite.
Jan 22 21:55:52.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:55:52.404: INFO: namespace: e2e-tests-container-lifecycle-hook-58n68, resource: bindings, ignored listing per whitelist
Jan 22 21:55:52.417: INFO: namespace e2e-tests-container-lifecycle-hook-58n68 deletion completed in 22.082939379s

• [SLOW TEST:46.209 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:55:52.418: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-7c84ec05-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating secret with name secret-projected-all-test-volume-7c84ebf4-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 22 21:55:52.482: INFO: Waiting up to 5m0s for pod "projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-kh2pb" to be "success or failure"
Jan 22 21:55:52.485: INFO: Pod "projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917412ms
Jan 22 21:55:54.489: INFO: Pod "projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006377552s
STEP: Saw pod success
Jan 22 21:55:54.489: INFO: Pod "projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 21:55:54.492: INFO: Trying to get logs from node secconf-node pod projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 22 21:55:54.510: INFO: Waiting for pod projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb to disappear
Jan 22 21:55:54.513: INFO: Pod projected-volume-7c84ebc1-1e90-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:55:54.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kh2pb" for this suite.
Jan 22 21:56:00.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:56:00.588: INFO: namespace: e2e-tests-projected-kh2pb, resource: bindings, ignored listing per whitelist
Jan 22 21:56:00.603: INFO: namespace e2e-tests-projected-kh2pb deletion completed in 6.086226864s

• [SLOW TEST:8.185 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:56:00.603: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-grg5n
Jan 22 21:56:02.676: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-grg5n
STEP: checking the pod's current state and verifying that restartCount is present
Jan 22 21:56:02.679: INFO: Initial restart count of pod liveness-exec is 0
Jan 22 21:56:56.806: INFO: Restart count of pod e2e-tests-container-probe-grg5n/liveness-exec is now 1 (54.127807992s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:56:56.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-grg5n" for this suite.
Jan 22 21:57:02.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:57:02.862: INFO: namespace: e2e-tests-container-probe-grg5n, resource: bindings, ignored listing per whitelist
Jan 22 21:57:02.909: INFO: namespace e2e-tests-container-probe-grg5n deletion completed in 6.087457944s

• [SLOW TEST:62.306 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:57:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-a68a9593-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating configMap with name cm-test-opt-upd-a68a95b9-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a68a9593-1e90-11e9-9284-e2fd7d97dccb
STEP: Updating configmap cm-test-opt-upd-a68a95b9-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating configMap with name cm-test-opt-create-a68a95c5-1e90-11e9-9284-e2fd7d97dccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:57:07.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gts8m" for this suite.
Jan 22 21:57:29.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:57:29.123: INFO: namespace: e2e-tests-projected-gts8m, resource: bindings, ignored listing per whitelist
Jan 22 21:57:29.139: INFO: namespace e2e-tests-projected-gts8m deletion completed in 22.080224516s

• [SLOW TEST:26.230 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:57:29.139: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 22 21:57:29.195: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 22 21:57:29.200: INFO: Waiting for terminating namespaces to be deleted...
Jan 22 21:57:29.202: INFO: 
Logging pods the kubelet thinks is on node secconf-node before test
Jan 22 21:57:29.207: INFO: kube-proxy-6m8wc from kube-system started at 2019-01-21 09:37:56 +0000 UTC (1 container statuses recorded)
Jan 22 21:57:29.207: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 21:57:29.207: INFO: calico-node-6j2nx from kube-system started at 2019-01-21 09:37:56 +0000 UTC (2 container statuses recorded)
Jan 22 21:57:29.207: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 21:57:29.207: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 21:57:29.207: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn from heptio-sonobuoy started at 2019-01-22 21:07:14 +0000 UTC (2 container statuses recorded)
Jan 22 21:57:29.207: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 21:57:29.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 21:57:29.207: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (1 container statuses recorded)
Jan 22 21:57:29.207: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 21:57:29.207: INFO: sonobuoy-e2e-job-98d427a1d3c0481f from heptio-sonobuoy started at 2019-01-22 21:07:14 +0000 UTC (2 container statuses recorded)
Jan 22 21:57:29.207: INFO: 	Container e2e ready: true, restart count 0
Jan 22 21:57:29.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b7625856-1e90-11e9-9284-e2fd7d97dccb 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b7625856-1e90-11e9-9284-e2fd7d97dccb off the node secconf-node
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b7625856-1e90-11e9-9284-e2fd7d97dccb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:57:33.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-n7p2z" for this suite.
Jan 22 21:57:41.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:57:41.347: INFO: namespace: e2e-tests-sched-pred-n7p2z, resource: bindings, ignored listing per whitelist
Jan 22 21:57:41.372: INFO: namespace e2e-tests-sched-pred-n7p2z deletion completed in 8.08345598s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.233 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:57:41.372: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-bd765406-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating secret with name s-test-opt-upd-bd76543f-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bd765406-1e90-11e9-9284-e2fd7d97dccb
STEP: Updating secret s-test-opt-upd-bd76543f-1e90-11e9-9284-e2fd7d97dccb
STEP: Creating secret with name s-test-opt-create-bd76544d-1e90-11e9-9284-e2fd7d97dccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:59:05.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v4b22" for this suite.
Jan 22 21:59:27.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:59:27.956: INFO: namespace: e2e-tests-projected-v4b22, resource: bindings, ignored listing per whitelist
Jan 22 21:59:27.997: INFO: namespace e2e-tests-projected-v4b22 deletion completed in 22.08925649s

• [SLOW TEST:106.625 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:59:27.997: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-c48cm in namespace e2e-tests-proxy-97gmk
I0122 21:59:28.072318      14 runners.go:184] Created replication controller with name: proxy-service-c48cm, namespace: e2e-tests-proxy-97gmk, replica count: 1
I0122 21:59:29.123642      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0122 21:59:30.124467      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:31.124832      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:32.125707      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:33.128982      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:34.130192      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:35.130662      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:36.131436      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:37.133449      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0122 21:59:38.135847      14 runners.go:184] proxy-service-c48cm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 22 21:59:38.139: INFO: setup took 10.083380449s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 22 21:59:38.147: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.238587ms)
Jan 22 21:59:38.149: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 9.633546ms)
Jan 22 21:59:38.156: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 15.932294ms)
Jan 22 21:59:38.156: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 15.856766ms)
Jan 22 21:59:38.156: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 16.102192ms)
Jan 22 21:59:38.156: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 15.7522ms)
Jan 22 21:59:38.157: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 16.20237ms)
Jan 22 21:59:38.157: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 16.533054ms)
Jan 22 21:59:38.158: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 17.306112ms)
Jan 22 21:59:38.159: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 18.872803ms)
Jan 22 21:59:38.160: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 21.290379ms)
Jan 22 21:59:38.176: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 36.244924ms)
Jan 22 21:59:38.176: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 36.068018ms)
Jan 22 21:59:38.176: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 36.486008ms)
Jan 22 21:59:38.176: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 35.323393ms)
Jan 22 21:59:38.176: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 35.580726ms)
Jan 22 21:59:38.180: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 4.334969ms)
Jan 22 21:59:38.180: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 4.47701ms)
Jan 22 21:59:38.183: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 6.781098ms)
Jan 22 21:59:38.183: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 7.245651ms)
Jan 22 21:59:38.184: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.972046ms)
Jan 22 21:59:38.184: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 8.049767ms)
Jan 22 21:59:38.184: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 7.919335ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 9.372053ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 9.421068ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 9.956811ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 9.65544ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 9.490692ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 9.728266ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 9.999367ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 9.642156ms)
Jan 22 21:59:38.186: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 10.186482ms)
Jan 22 21:59:38.193: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 6.055667ms)
Jan 22 21:59:38.193: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.637032ms)
Jan 22 21:59:38.193: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.5161ms)
Jan 22 21:59:38.194: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 7.261176ms)
Jan 22 21:59:38.195: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.394453ms)
Jan 22 21:59:38.195: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 8.77587ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 7.342635ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.369085ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 7.333188ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 7.478227ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 8.481196ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 8.269916ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 9.240751ms)
Jan 22 21:59:38.196: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 8.445398ms)
Jan 22 21:59:38.198: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 10.297558ms)
Jan 22 21:59:38.198: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 10.182273ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 8.899584ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 8.802397ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 9.080374ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.777234ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.86471ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 9.039473ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 9.071029ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 8.894126ms)
Jan 22 21:59:38.207: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 9.120308ms)
Jan 22 21:59:38.208: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 10.262615ms)
Jan 22 21:59:38.209: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 10.307139ms)
Jan 22 21:59:38.209: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 10.529695ms)
Jan 22 21:59:38.209: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 10.565829ms)
Jan 22 21:59:38.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 13.062633ms)
Jan 22 21:59:38.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 13.16783ms)
Jan 22 21:59:38.211: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 13.161167ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.052781ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 7.948856ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 8.122291ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 8.559239ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 8.103328ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.857342ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 8.993427ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 8.309008ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 8.122041ms)
Jan 22 21:59:38.220: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 8.068996ms)
Jan 22 21:59:38.221: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 8.160193ms)
Jan 22 21:59:38.221: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 8.461328ms)
Jan 22 21:59:38.221: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 9.369973ms)
Jan 22 21:59:38.221: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 9.393002ms)
Jan 22 21:59:38.221: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 9.821628ms)
Jan 22 21:59:38.221: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 9.712653ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 8.023699ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 8.677703ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 9.481071ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 9.838347ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 9.051766ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 9.282252ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 8.626695ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 8.738646ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 9.736279ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.973783ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 9.827152ms)
Jan 22 21:59:38.231: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 9.572834ms)
Jan 22 21:59:38.232: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 9.863449ms)
Jan 22 21:59:38.232: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 10.136692ms)
Jan 22 21:59:38.232: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 10.543007ms)
Jan 22 21:59:38.232: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 10.074338ms)
Jan 22 21:59:38.239: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 6.970276ms)
Jan 22 21:59:38.240: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 6.894025ms)
Jan 22 21:59:38.240: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 7.204287ms)
Jan 22 21:59:38.240: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.259124ms)
Jan 22 21:59:38.241: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 8.984924ms)
Jan 22 21:59:38.242: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 8.915216ms)
Jan 22 21:59:38.242: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 8.845356ms)
Jan 22 21:59:38.242: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.943127ms)
Jan 22 21:59:38.242: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 8.862655ms)
Jan 22 21:59:38.243: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 10.624871ms)
Jan 22 21:59:38.243: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 10.727611ms)
Jan 22 21:59:38.243: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 10.780878ms)
Jan 22 21:59:38.243: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 10.497729ms)
Jan 22 21:59:38.245: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 11.975929ms)
Jan 22 21:59:38.245: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 12.120952ms)
Jan 22 21:59:38.245: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 12.230032ms)
Jan 22 21:59:38.253: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 7.455205ms)
Jan 22 21:59:38.253: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.1435ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 9.686991ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 10.023986ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 10.026015ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 10.079055ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 10.564539ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 10.313397ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 10.282477ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 10.399581ms)
Jan 22 21:59:38.255: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 10.266448ms)
Jan 22 21:59:38.256: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 10.806241ms)
Jan 22 21:59:38.256: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 10.678432ms)
Jan 22 21:59:38.257: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 11.620077ms)
Jan 22 21:59:38.257: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 11.855195ms)
Jan 22 21:59:38.260: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 14.856152ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 6.995733ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 6.953853ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 5.895385ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.991479ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.147102ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.282362ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 5.945467ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 6.721827ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 7.475064ms)
Jan 22 21:59:38.267: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 6.827526ms)
Jan 22 21:59:38.269: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.663495ms)
Jan 22 21:59:38.272: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 11.437604ms)
Jan 22 21:59:38.272: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 11.012014ms)
Jan 22 21:59:38.272: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 11.827196ms)
Jan 22 21:59:38.272: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 11.901605ms)
Jan 22 21:59:38.274: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 12.767204ms)
Jan 22 21:59:38.277: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 3.364598ms)
Jan 22 21:59:38.280: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.641827ms)
Jan 22 21:59:38.280: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 5.740745ms)
Jan 22 21:59:38.281: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 6.441462ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 7.877696ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 7.888614ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 7.999783ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.965854ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 8.029671ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 8.011382ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 8.022935ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 8.004507ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 8.095343ms)
Jan 22 21:59:38.282: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 8.473919ms)
Jan 22 21:59:38.283: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 8.725515ms)
Jan 22 21:59:38.283: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 8.898264ms)
Jan 22 21:59:38.290: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 5.955797ms)
Jan 22 21:59:38.290: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 7.048938ms)
Jan 22 21:59:38.290: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 5.835936ms)
Jan 22 21:59:38.290: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 7.182121ms)
Jan 22 21:59:38.291: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 7.567891ms)
Jan 22 21:59:38.291: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 6.880858ms)
Jan 22 21:59:38.291: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 7.822392ms)
Jan 22 21:59:38.291: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.82144ms)
Jan 22 21:59:38.291: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 7.549646ms)
Jan 22 21:59:38.291: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.438995ms)
Jan 22 21:59:38.296: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 11.761093ms)
Jan 22 21:59:38.296: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 12.158124ms)
Jan 22 21:59:38.296: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 11.375079ms)
Jan 22 21:59:38.296: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 12.477985ms)
Jan 22 21:59:38.296: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 12.153015ms)
Jan 22 21:59:38.296: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 10.872475ms)
Jan 22 21:59:38.303: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.361848ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 8.976636ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 9.21917ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 9.31074ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 9.197529ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 9.232712ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 9.025631ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 9.099959ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 9.068381ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 9.188627ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 9.247014ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 9.388486ms)
Jan 22 21:59:38.305: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 9.373165ms)
Jan 22 21:59:38.306: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 9.711201ms)
Jan 22 21:59:38.306: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 9.928313ms)
Jan 22 21:59:38.306: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 9.959484ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 6.062784ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 5.205375ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.539397ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 5.824374ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 5.757173ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.056809ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.498103ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 6.255413ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 5.560972ms)
Jan 22 21:59:38.312: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 6.687353ms)
Jan 22 21:59:38.315: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.238138ms)
Jan 22 21:59:38.315: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 8.508032ms)
Jan 22 21:59:38.316: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 10.224296ms)
Jan 22 21:59:38.316: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 10.332269ms)
Jan 22 21:59:38.316: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 9.96779ms)
Jan 22 21:59:38.316: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 10.06714ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.55083ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 6.891942ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.885306ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 6.975902ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 6.831923ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.672667ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.829594ms)
Jan 22 21:59:38.323: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 6.94112ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 8.049104ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 8.691813ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 8.623529ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 8.462513ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 8.537657ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 8.53855ms)
Jan 22 21:59:38.325: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 8.597565ms)
Jan 22 21:59:38.326: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 9.545899ms)
Jan 22 21:59:38.331: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 4.812506ms)
Jan 22 21:59:38.331: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 4.771094ms)
Jan 22 21:59:38.332: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 5.671835ms)
Jan 22 21:59:38.332: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.783766ms)
Jan 22 21:59:38.332: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 5.628042ms)
Jan 22 21:59:38.334: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 7.44559ms)
Jan 22 21:59:38.334: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.446704ms)
Jan 22 21:59:38.334: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 7.482623ms)
Jan 22 21:59:38.334: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 7.543552ms)
Jan 22 21:59:38.334: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 7.460373ms)
Jan 22 21:59:38.335: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 8.565339ms)
Jan 22 21:59:38.335: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 8.758734ms)
Jan 22 21:59:38.335: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 8.756148ms)
Jan 22 21:59:38.335: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 8.713582ms)
Jan 22 21:59:38.335: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 8.695195ms)
Jan 22 21:59:38.335: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 8.600684ms)
Jan 22 21:59:38.341: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.818736ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 5.43448ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 5.779137ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 6.284369ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 5.397122ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 5.637867ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 5.457114ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 5.415652ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 6.294524ms)
Jan 22 21:59:38.342: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 6.216256ms)
Jan 22 21:59:38.343: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 6.600925ms)
Jan 22 21:59:38.343: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 7.414401ms)
Jan 22 21:59:38.343: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 7.561105ms)
Jan 22 21:59:38.343: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 7.158629ms)
Jan 22 21:59:38.343: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 7.914197ms)
Jan 22 21:59:38.344: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 8.734648ms)
Jan 22 21:59:38.351: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.033695ms)
Jan 22 21:59:38.351: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 5.790702ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 8.515727ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 9.876082ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 9.128754ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 9.659697ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 8.999378ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 9.135558ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 9.527422ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 9.058251ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 9.871764ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 9.687092ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 10.113907ms)
Jan 22 21:59:38.354: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 9.150282ms)
Jan 22 21:59:38.355: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 9.534522ms)
Jan 22 21:59:38.355: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 9.642507ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 6.40772ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 5.805816ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 6.563906ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.180096ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 7.019208ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 6.734548ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 7.344238ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 6.558852ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.199208ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 6.972791ms)
Jan 22 21:59:38.362: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.290884ms)
Jan 22 21:59:38.363: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 7.36273ms)
Jan 22 21:59:38.363: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 8.166895ms)
Jan 22 21:59:38.363: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 6.960729ms)
Jan 22 21:59:38.363: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 7.746705ms)
Jan 22 21:59:38.363: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 7.248497ms)
Jan 22 21:59:38.368: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 4.848979ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 5.801855ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 5.7959ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.311442ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 5.854908ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.450781ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 6.13296ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 6.11338ms)
Jan 22 21:59:38.370: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 5.944586ms)
Jan 22 21:59:38.375: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 10.977369ms)
Jan 22 21:59:38.375: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 11.696601ms)
Jan 22 21:59:38.375: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 11.032668ms)
Jan 22 21:59:38.375: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 11.842554ms)
Jan 22 21:59:38.375: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 10.914907ms)
Jan 22 21:59:38.375: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 10.987331ms)
Jan 22 21:59:38.376: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 12.465935ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.538874ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.817901ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:160/proxy/: foo (200; 6.863945ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:462/proxy/: tls qux (200; 6.769186ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:1080/proxy/... (200; 6.597795ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw:1080/proxy/rewri... (200; 6.734168ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/proxy-service-c48cm-lftsw/proxy/rewriteme"... (200; 6.702909ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname1/proxy/: foo (200; 6.6948ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/http:proxy-service-c48cm-lftsw:162/proxy/: bar (200; 6.757605ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname1/proxy/: foo (200; 6.82489ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname2/proxy/: tls qux (200; 6.874742ms)
Jan 22 21:59:38.383: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:460/proxy/: tls baz (200; 6.693602ms)
Jan 22 21:59:38.384: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/https:proxy-service-c48cm:tlsportname1/proxy/: tls baz (200; 7.335876ms)
Jan 22 21:59:38.384: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/http:proxy-service-c48cm:portname2/proxy/: bar (200; 7.40212ms)
Jan 22 21:59:38.384: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-97gmk/pods/https:proxy-service-c48cm-lftsw:443/proxy/... (200; 7.471246ms)
Jan 22 21:59:38.384: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-97gmk/services/proxy-service-c48cm:portname2/proxy/: bar (200; 7.512985ms)
STEP: deleting ReplicationController proxy-service-c48cm in namespace e2e-tests-proxy-97gmk, will wait for the garbage collector to delete the pods
Jan 22 21:59:38.444: INFO: Deleting ReplicationController proxy-service-c48cm took: 5.012005ms
Jan 22 21:59:38.544: INFO: Terminating ReplicationController proxy-service-c48cm pods took: 100.256021ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:59:40.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-97gmk" for this suite.
Jan 22 21:59:46.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:59:46.585: INFO: namespace: e2e-tests-proxy-97gmk, resource: bindings, ignored listing per whitelist
Jan 22 21:59:46.638: INFO: namespace e2e-tests-proxy-97gmk deletion completed in 6.088833335s

• [SLOW TEST:18.641 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:59:46.638: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jan 22 21:59:47.732: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 21:59:47.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-z2fgn" for this suite.
Jan 22 21:59:53.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 21:59:53.792: INFO: namespace: e2e-tests-gc-z2fgn, resource: bindings, ignored listing per whitelist
Jan 22 21:59:53.818: INFO: namespace e2e-tests-gc-z2fgn deletion completed in 6.082467271s

• [SLOW TEST:7.180 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 21:59:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 22 21:59:53.890: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:59:53.893: INFO: Number of nodes with available pods: 0
Jan 22 21:59:53.893: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:59:54.898: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:59:54.902: INFO: Number of nodes with available pods: 0
Jan 22 21:59:54.902: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:59:55.898: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:59:55.901: INFO: Number of nodes with available pods: 1
Jan 22 21:59:55.901: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 22 21:59:55.916: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:59:55.921: INFO: Number of nodes with available pods: 0
Jan 22 21:59:55.921: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:59:56.925: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:59:56.928: INFO: Number of nodes with available pods: 0
Jan 22 21:59:56.928: INFO: Node secconf-node is running more than one daemon pod
Jan 22 21:59:57.927: INFO: DaemonSet pods can't tolerate node secconf-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 22 21:59:57.930: INFO: Number of nodes with available pods: 1
Jan 22 21:59:57.930: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-b4x4t, will wait for the garbage collector to delete the pods
Jan 22 21:59:57.993: INFO: Deleting DaemonSet.extensions daemon-set took: 4.810983ms
Jan 22 21:59:58.094: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.924647ms
Jan 22 22:00:42.000: INFO: Number of nodes with available pods: 0
Jan 22 22:00:42.000: INFO: Number of running nodes: 0, number of available pods: 0
Jan 22 22:00:42.004: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-b4x4t/daemonsets","resourceVersion":"39423"},"items":null}

Jan 22 22:00:42.006: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-b4x4t/pods","resourceVersion":"39423"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:00:42.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-b4x4t" for this suite.
Jan 22 22:00:48.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:00:48.073: INFO: namespace: e2e-tests-daemonsets-b4x4t, resource: bindings, ignored listing per whitelist
Jan 22 22:00:48.098: INFO: namespace e2e-tests-daemonsets-b4x4t deletion completed in 6.082908566s

• [SLOW TEST:54.281 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:00:48.099: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:00:48.160: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 22 22:00:48.165: INFO: Number of nodes with available pods: 0
Jan 22 22:00:48.165: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 22 22:00:48.180: INFO: Number of nodes with available pods: 0
Jan 22 22:00:48.180: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:49.185: INFO: Number of nodes with available pods: 1
Jan 22 22:00:49.185: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 22 22:00:49.201: INFO: Number of nodes with available pods: 1
Jan 22 22:00:49.201: INFO: Number of running nodes: 0, number of available pods: 1
Jan 22 22:00:50.205: INFO: Number of nodes with available pods: 0
Jan 22 22:00:50.205: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 22 22:00:50.212: INFO: Number of nodes with available pods: 0
Jan 22 22:00:50.212: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:51.218: INFO: Number of nodes with available pods: 0
Jan 22 22:00:51.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:52.218: INFO: Number of nodes with available pods: 0
Jan 22 22:00:52.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:53.217: INFO: Number of nodes with available pods: 0
Jan 22 22:00:53.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:54.216: INFO: Number of nodes with available pods: 0
Jan 22 22:00:54.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:55.218: INFO: Number of nodes with available pods: 0
Jan 22 22:00:55.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:56.216: INFO: Number of nodes with available pods: 0
Jan 22 22:00:56.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:57.217: INFO: Number of nodes with available pods: 0
Jan 22 22:00:57.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:58.215: INFO: Number of nodes with available pods: 0
Jan 22 22:00:58.215: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:00:59.216: INFO: Number of nodes with available pods: 0
Jan 22 22:00:59.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:00.215: INFO: Number of nodes with available pods: 0
Jan 22 22:01:00.215: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:01.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:01.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:02.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:02.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:03.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:03.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:04.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:04.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:05.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:05.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:06.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:06.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:07.215: INFO: Number of nodes with available pods: 0
Jan 22 22:01:07.215: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:08.215: INFO: Number of nodes with available pods: 0
Jan 22 22:01:08.215: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:09.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:09.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:10.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:10.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:11.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:11.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:12.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:12.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:13.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:13.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:14.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:14.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:15.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:15.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:16.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:16.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:17.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:17.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:18.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:18.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:19.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:19.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:20.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:20.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:21.215: INFO: Number of nodes with available pods: 0
Jan 22 22:01:21.215: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:22.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:22.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:23.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:23.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:24.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:24.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:25.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:25.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:26.215: INFO: Number of nodes with available pods: 0
Jan 22 22:01:26.215: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:27.216: INFO: Number of nodes with available pods: 0
Jan 22 22:01:27.216: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:28.219: INFO: Number of nodes with available pods: 0
Jan 22 22:01:28.219: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:29.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:29.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:30.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:30.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:31.217: INFO: Number of nodes with available pods: 0
Jan 22 22:01:31.217: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:32.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:32.218: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:33.218: INFO: Number of nodes with available pods: 0
Jan 22 22:01:33.219: INFO: Node secconf-node is running more than one daemon pod
Jan 22 22:01:34.217: INFO: Number of nodes with available pods: 1
Jan 22 22:01:34.217: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-t9vr6, will wait for the garbage collector to delete the pods
Jan 22 22:01:34.283: INFO: Deleting DaemonSet.extensions daemon-set took: 6.119328ms
Jan 22 22:01:34.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.520044ms
Jan 22 22:02:11.988: INFO: Number of nodes with available pods: 0
Jan 22 22:02:11.988: INFO: Number of running nodes: 0, number of available pods: 0
Jan 22 22:02:11.991: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-t9vr6/daemonsets","resourceVersion":"39612"},"items":null}

Jan 22 22:02:11.994: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-t9vr6/pods","resourceVersion":"39612"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:02:12.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-t9vr6" for this suite.
Jan 22 22:02:18.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:02:18.041: INFO: namespace: e2e-tests-daemonsets-t9vr6, resource: bindings, ignored listing per whitelist
Jan 22 22:02:18.095: INFO: namespace e2e-tests-daemonsets-t9vr6 deletion completed in 6.084540465s

• [SLOW TEST:89.997 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:02:18.095: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Jan 22 22:02:18.155: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-tpn2x" to be "success or failure"
Jan 22 22:02:18.158: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.077349ms
Jan 22 22:02:20.165: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010172298s
STEP: Saw pod success
Jan 22 22:02:20.165: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 22 22:02:20.167: INFO: Trying to get logs from node secconf-node pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 22 22:02:20.187: INFO: Waiting for pod pod-host-path-test to disappear
Jan 22 22:02:20.190: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:02:20.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-tpn2x" for this suite.
Jan 22 22:02:26.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:02:26.239: INFO: namespace: e2e-tests-hostpath-tpn2x, resource: bindings, ignored listing per whitelist
Jan 22 22:02:26.276: INFO: namespace e2e-tests-hostpath-tpn2x deletion completed in 6.082873066s

• [SLOW TEST:8.181 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:02:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 22 22:02:26.328: INFO: Waiting up to 5m0s for pod "pod-6745e982-1e91-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-jz52l" to be "success or failure"
Jan 22 22:02:26.330: INFO: Pod "pod-6745e982-1e91-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87295ms
Jan 22 22:02:28.334: INFO: Pod "pod-6745e982-1e91-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006805185s
STEP: Saw pod success
Jan 22 22:02:28.334: INFO: Pod "pod-6745e982-1e91-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:02:28.337: INFO: Trying to get logs from node secconf-node pod pod-6745e982-1e91-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:02:28.354: INFO: Waiting for pod pod-6745e982-1e91-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:02:28.357: INFO: Pod pod-6745e982-1e91-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:02:28.357: INFO: Unexpected error occurred: expected "perms of file \"/test-volume/test-file\": -rw-r--r--" in container output: Expected
    <string>: 
to contain substring
    <string>: perms of file "/test-volume/test-file": -rw-r--r--
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-emptydir-jz52l".
STEP: Found 4 events.
Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:26 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-emptydir-jz52l/pod-6745e982-1e91-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:26 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0" already present on machine
Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:26 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:27 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:02:28.370: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:02:28.370: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:02:28.370: INFO: 
Jan 22 22:02:28.373: INFO: 
Logging node info for node secconf-master
Jan 22 22:02:28.375: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:39667,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:02:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:02:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:02:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:02:23 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:02:28.375: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:02:28.378: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:02:28.387: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:02:28.387: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:02:28.387: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:02:28.387: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:02:28.387: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:02:28.387: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:02:28.387: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:02:28.387: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:02:28.387: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:02:28.387: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:02:28.387: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:02:28.387: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:02:28.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:02:28.387: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:02:28.387: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:02:28.387: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:02:28.408: INFO: 
Latency metrics for node secconf-master
Jan 22 22:02:28.408: INFO: 
Logging node info for node secconf-node
Jan 22 22:02:28.412: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:39658,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:02:28.412: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:02:28.415: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:02:28.420: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:02:28.420: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:02:28.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:02:28.420: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:02:28.420: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:02:28.420: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:02:28.420: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:02:28.420: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:02:28.420: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:02:28.420: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:02:28.420: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:02:28.420: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:02:28.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:02:28.441: INFO: 
Latency metrics for node secconf-node
Jan 22 22:02:28.441: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.066745s}
Jan 22 22:02:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jz52l" for this suite.
Jan 22 22:02:34.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:02:34.476: INFO: namespace: e2e-tests-emptydir-jz52l, resource: bindings, ignored listing per whitelist
Jan 22 22:02:34.529: INFO: namespace e2e-tests-emptydir-jz52l deletion completed in 6.084446906s

• Failure [8.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc001192200>: {
          s: "expected \"perms of file \\\"/test-volume/test-file\\\": -rw-r--r--\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: perms of file \"/test-volume/test-file\": -rw-r--r--",
      }
      expected "perms of file \"/test-volume/test-file\": -rw-r--r--" in container output: Expected
          <string>: 
      to contain substring
          <string>: perms of file "/test-volume/test-file": -rw-r--r--
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:02:34.529: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-87xlk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 22 22:02:34.583: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 22 22:02:50.670: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 100.100.1.11 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-87xlk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:02:50.670: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:02:51.759: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:02:51.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-87xlk" for this suite.
Jan 22 22:03:13.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:03:13.815: INFO: namespace: e2e-tests-pod-network-test-87xlk, resource: bindings, ignored listing per whitelist
Jan 22 22:03:13.879: INFO: namespace e2e-tests-pod-network-test-87xlk deletion completed in 22.116301241s

• [SLOW TEST:39.350 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:03:13.879: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-czv7z
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 22 22:03:13.936: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 22 22:03:29.982: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.100.1.14:8080/dial?request=hostName&protocol=http&host=100.100.1.13&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-czv7z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:03:29.982: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:03:30.068: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:03:30.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-czv7z" for this suite.
Jan 22 22:03:52.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:03:52.126: INFO: namespace: e2e-tests-pod-network-test-czv7z, resource: bindings, ignored listing per whitelist
Jan 22 22:03:52.150: INFO: namespace e2e-tests-pod-network-test-czv7z deletion completed in 22.079136723s

• [SLOW TEST:38.271 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:03:52.150: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-9a7661c6-1e91-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:03:52.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-b88j6" to be "success or failure"
Jan 22 22:03:52.215: INFO: Pod "pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905317ms
Jan 22 22:03:54.219: INFO: Pod "pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006941624s
STEP: Saw pod success
Jan 22 22:03:54.219: INFO: Pod "pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:03:54.222: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 22 22:03:54.237: INFO: Waiting for pod pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:03:54.240: INFO: Pod pod-projected-secrets-9a76d1ca-1e91-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:03:54.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b88j6" for this suite.
Jan 22 22:04:00.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:04:00.311: INFO: namespace: e2e-tests-projected-b88j6, resource: bindings, ignored listing per whitelist
Jan 22 22:04:00.329: INFO: namespace e2e-tests-projected-b88j6 deletion completed in 6.084358788s

• [SLOW TEST:8.179 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:04:00.329: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-9f5654cf-1e91-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:04:00.391: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-dtljd" to be "success or failure"
Jan 22 22:04:00.393: INFO: Pod "pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039844ms
Jan 22 22:04:02.396: INFO: Pod "pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005162027s
STEP: Saw pod success
Jan 22 22:04:02.396: INFO: Pod "pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:04:02.399: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 22 22:04:02.415: INFO: Waiting for pod pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:04:02.419: INFO: Pod pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:04:02.419: INFO: Unexpected error occurred: expected "content of file \"/etc/projected-secret-volume/data-1\": value-1" in container output: Expected
    <string>: 
to contain substring
    <string>: content of file "/etc/projected-secret-volume/data-1": value-1
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-dtljd".
STEP: Found 4 events.
Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:00 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-dtljd/pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:01 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:01 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:01 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:04:02.433: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:04:02.433: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:02.433: INFO: 
Jan 22 22:04:02.436: INFO: 
Logging node info for node secconf-master
Jan 22 22:04:02.438: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:39943,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:03:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:03:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:03:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:03:53 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:04:02.438: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:04:02.441: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:04:02.447: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:02.447: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:04:02.447: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:02.447: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:04:02.447: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:02.447: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:04:02.447: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:04:02.447: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:02.447: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:02.447: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:02.447: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:02.447: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:02.447: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:04:02.447: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:02.447: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:04:02.447: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:02.472: INFO: 
Latency metrics for node secconf-master
Jan 22 22:04:02.472: INFO: 
Logging node info for node secconf-node
Jan 22 22:04:02.475: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:39964,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:04:02.475: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:04:02.477: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:04:02.483: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:02.483: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:04:02.483: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:02.483: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:02.483: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:04:02.483: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:04:02.483: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:02.483: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:04:02.483: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:02.483: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:04:02.483: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:02.483: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:04:02.483: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:02.508: INFO: 
Latency metrics for node secconf-node
Jan 22 22:04:02.508: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}
Jan 22 22:04:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dtljd" for this suite.
Jan 22 22:04:08.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:04:08.572: INFO: namespace: e2e-tests-projected-dtljd, resource: bindings, ignored listing per whitelist
Jan 22 22:04:08.588: INFO: namespace e2e-tests-projected-dtljd deletion completed in 6.076923955s

• Failure [8.259 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc0023fdbc0>: {
          s: "expected \"content of file \\\"/etc/projected-secret-volume/data-1\\\": value-1\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: content of file \"/etc/projected-secret-volume/data-1\": value-1",
      }
      expected "content of file \"/etc/projected-secret-volume/data-1\": value-1" in container output: Expected
          <string>: 
      to contain substring
          <string>: content of file "/etc/projected-secret-volume/data-1": value-1
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:04:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Jan 22 22:04:08.648: INFO: Waiting up to 5m0s for pod "var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-var-expansion-7vn4k" to be "success or failure"
Jan 22 22:04:08.653: INFO: Pod "var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.449141ms
Jan 22 22:04:10.656: INFO: Pod "var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007908989s
STEP: Saw pod success
Jan 22 22:04:10.656: INFO: Pod "var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:04:10.658: INFO: Trying to get logs from node secconf-node pod var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 22:04:10.673: INFO: Waiting for pod var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:04:10.677: INFO: Pod var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:04:10.677: INFO: Unexpected error occurred: expected "FOO=foo-value" in container output: Expected
    <string>: 
to contain substring
    <string>: FOO=foo-value
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-var-expansion-7vn4k".
STEP: Found 4 events.
Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:08 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-var-expansion-7vn4k/var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:09 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "docker.io/library/busybox:1.29" already present on machine
Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:09 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:09 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:04:10.689: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:04:10.689: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:10.689: INFO: 
Jan 22 22:04:10.692: INFO: 
Logging node info for node secconf-master
Jan 22 22:04:10.694: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:39986,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:03 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:03 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:03 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:03 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:04:10.694: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:04:10.696: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:04:10.701: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:10.701: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:10.701: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:10.701: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:10.701: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:10.701: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:04:10.701: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:10.701: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:04:10.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:10.701: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:10.701: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:04:10.701: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:10.701: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:04:10.701: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:10.701: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:04:10.701: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:04:10.719: INFO: 
Latency metrics for node secconf-master
Jan 22 22:04:10.719: INFO: 
Logging node info for node secconf-node
Jan 22 22:04:10.722: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:40016,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:04:10.722: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:04:10.724: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:04:10.728: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:10.728: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:04:10.728: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:10.728: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:04:10.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:10.728: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:10.728: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:04:10.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:10.728: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:10.728: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:04:10.728: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:04:10.728: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:10.728: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:04:10.747: INFO: 
Latency metrics for node secconf-node
Jan 22 22:04:10.747: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}
Jan 22 22:04:10.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-7vn4k" for this suite.
Jan 22 22:04:16.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:04:16.821: INFO: namespace: e2e-tests-var-expansion-7vn4k, resource: bindings, ignored listing per whitelist
Jan 22 22:04:16.833: INFO: namespace e2e-tests-var-expansion-7vn4k deletion completed in 6.082491672s

• Failure [8.245 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc00230e450>: {
          s: "expected \"FOO=foo-value\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: FOO=foo-value",
      }
      expected "FOO=foo-value" in container output: Expected
          <string>: 
      to contain substring
          <string>: FOO=foo-value
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:04:16.833: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-a92c678b-1e91-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:04:16.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-9ljbq" to be "success or failure"
Jan 22 22:04:16.895: INFO: Pod "pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728191ms
Jan 22 22:04:18.901: INFO: Pod "pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008386278s
STEP: Saw pod success
Jan 22 22:04:18.901: INFO: Pod "pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:04:18.905: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 22:04:18.938: INFO: Waiting for pod pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:04:18.941: INFO: Pod pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:04:18.941: INFO: Unexpected error occurred: expected "content of file \"/etc/projected-configmap-volume/data-1\": value-1" in container output: Expected
    <string>: 
to contain substring
    <string>: content of file "/etc/projected-configmap-volume/data-1": value-1
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-9ljbq".
STEP: Found 4 events.
Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:16 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-9ljbq/pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:17 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:17 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:17 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:04:18.956: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:04:18.956: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:18.956: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:18.957: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:18.957: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:18.957: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:04:18.957: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:04:18.957: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:04:18.957: INFO: 
Jan 22 22:04:18.959: INFO: 
Logging node info for node secconf-master
Jan 22 22:04:18.962: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:40025,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:13 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:04:18.962: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:04:18.965: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:04:18.973: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:18.973: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:04:18.973: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:18.973: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:04:18.973: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:18.973: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:18.973: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:18.973: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:18.973: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:04:18.973: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:18.973: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:04:18.973: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:18.973: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:04:18.973: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:18.973: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:04:18.973: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:04:18.987: INFO: 
Latency metrics for node secconf-master
Jan 22 22:04:18.987: INFO: 
Logging node info for node secconf-node
Jan 22 22:04:18.989: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:40016,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:04:18.990: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:04:18.992: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:04:18.997: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:18.997: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:04:18.997: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:18.997: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:18.997: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:04:18.997: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:18.997: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:04:18.997: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:04:18.997: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:04:18.997: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:04:18.997: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:04:18.997: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:04:18.997: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:04:19.017: INFO: 
Latency metrics for node secconf-node
Jan 22 22:04:19.017: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}
Jan 22 22:04:19.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9ljbq" for this suite.
Jan 22 22:04:25.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:04:25.051: INFO: namespace: e2e-tests-projected-9ljbq, resource: bindings, ignored listing per whitelist
Jan 22 22:04:25.102: INFO: namespace e2e-tests-projected-9ljbq deletion completed in 6.081769831s

• Failure [8.269 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc002a299a0>: {
          s: "expected \"content of file \\\"/etc/projected-configmap-volume/data-1\\\": value-1\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: content of file \"/etc/projected-configmap-volume/data-1\": value-1",
      }
      expected "content of file \"/etc/projected-configmap-volume/data-1\": value-1" in container output: Expected
          <string>: 
      to contain substring
          <string>: content of file "/etc/projected-configmap-volume/data-1": value-1
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:04:25.102: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 22:04:25.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:25.334: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 22 22:04:25.334: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jan 22 22:04:25.339: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jan 22 22:04:25.340: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 22 22:04:25.351: INFO: scanned /root for discovery docs: <nil>
Jan 22 22:04:25.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:41.121: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 22 22:04:41.121: INFO: stdout: "Created e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5\nScaling up e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jan 22 22:04:41.121: INFO: stdout: "Created e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5\nScaling up e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 22 22:04:41.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:41.199: INFO: stderr: ""
Jan 22 22:04:41.199: INFO: stdout: "e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5-2lpwf e2e-test-nginx-rc-swssf "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jan 22 22:04:46.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:46.273: INFO: stderr: ""
Jan 22 22:04:46.273: INFO: stdout: "e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5-2lpwf "
Jan 22 22:04:46.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5-2lpwf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:46.337: INFO: stderr: ""
Jan 22 22:04:46.337: INFO: stdout: "true"
Jan 22 22:04:46.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5-2lpwf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:46.406: INFO: stderr: ""
Jan 22 22:04:46.406: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jan 22 22:04:46.406: INFO: e2e-test-nginx-rc-bfa2afab91f4f94afcec65ec678af6a5-2lpwf is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Jan 22 22:04:46.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qzjx8'
Jan 22 22:04:46.477: INFO: stderr: ""
Jan 22 22:04:46.477: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:04:46.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qzjx8" for this suite.
Jan 22 22:05:02.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:05:02.526: INFO: namespace: e2e-tests-kubectl-qzjx8, resource: bindings, ignored listing per whitelist
Jan 22 22:05:02.610: INFO: namespace e2e-tests-kubectl-qzjx8 deletion completed in 16.129613997s

• [SLOW TEST:37.508 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:05:02.610: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:05:02.673: INFO: Creating ReplicaSet my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb
Jan 22 22:05:02.680: INFO: Pod name my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb: Found 0 pods out of 1
Jan 22 22:05:07.685: INFO: Pod name my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb: Found 1 pods out of 1
Jan 22 22:05:07.685: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb" is running
Jan 22 22:05:07.688: INFO: Pod "my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb-mwdkk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:05:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:05:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:05:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:05:02 +0000 UTC Reason: Message:}])
Jan 22 22:05:07.688: INFO: Trying to dial the pod
Jan 22 22:05:12.698: INFO: Controller my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb: Got expected result from replica 1 [my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb-mwdkk]: "my-hostname-basic-c4770c09-1e91-11e9-9284-e2fd7d97dccb-mwdkk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:05:12.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-k49qp" for this suite.
Jan 22 22:05:18.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:05:18.782: INFO: namespace: e2e-tests-replicaset-k49qp, resource: bindings, ignored listing per whitelist
Jan 22 22:05:18.800: INFO: namespace e2e-tests-replicaset-k49qp deletion completed in 6.099008023s

• [SLOW TEST:16.190 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:05:18.800: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-x64qm/configmap-test-ce1e2589-1e91-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:05:18.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-x64qm" to be "success or failure"
Jan 22 22:05:18.881: INFO: Pod "pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.348834ms
Jan 22 22:05:20.885: INFO: Pod "pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009422434s
STEP: Saw pod success
Jan 22 22:05:20.885: INFO: Pod "pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:05:20.888: INFO: Trying to get logs from node secconf-node pod pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb container env-test: <nil>
STEP: delete the pod
Jan 22 22:05:20.947: INFO: Waiting for pod pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:05:20.952: INFO: Pod pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:05:20.952: INFO: Unexpected error occurred: expected "CONFIG_DATA_1=value-1" in container output: Expected
    <string>: 
to contain substring
    <string>: CONFIG_DATA_1=value-1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-configmap-x64qm".
STEP: Found 4 events.
Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:18 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-configmap-x64qm/pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:19 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "docker.io/library/busybox:1.29" already present on machine
Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:19 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:19 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:05:20.967: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:05:20.967: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:05:20.967: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:05:20.968: INFO: 
Jan 22 22:05:20.971: INFO: 
Logging node info for node secconf-master
Jan 22 22:05:20.973: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:40242,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:05:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:05:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:05:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:05:13 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:05:20.974: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:05:20.976: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:05:20.981: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:05:20.981: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:05:20.981: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:05:20.981: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:05:20.981: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:05:20.981: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:05:20.981: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:05:20.981: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:05:20.981: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:05:20.981: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:05:20.981: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:05:20.981: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:05:20.981: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:05:20.981: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:05:20.981: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:05:20.981: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:05:20.999: INFO: 
Latency metrics for node secconf-master
Jan 22 22:05:20.999: INFO: 
Logging node info for node secconf-node
Jan 22 22:05:21.002: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:40277,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:05:21.002: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:05:21.005: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:05:21.011: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:05:21.011: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:05:21.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:05:21.011: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:05:21.011: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:05:21.011: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:05:21.011: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:05:21.011: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:05:21.011: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:05:21.011: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:05:21.011: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:05:21.011: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 22 22:05:21.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:05:21.035: INFO: 
Latency metrics for node secconf-node
Jan 22 22:05:21.035: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}
Jan 22 22:05:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x64qm" for this suite.
Jan 22 22:05:27.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:05:27.071: INFO: namespace: e2e-tests-configmap-x64qm, resource: bindings, ignored listing per whitelist
Jan 22 22:05:27.118: INFO: namespace e2e-tests-configmap-x64qm deletion completed in 6.079896198s

• Failure [8.318 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc002b203d0>: {
          s: "expected \"CONFIG_DATA_1=value-1\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: CONFIG_DATA_1=value-1",
      }
      expected "CONFIG_DATA_1=value-1" in container output: Expected
          <string>: 
      to contain substring
          <string>: CONFIG_DATA_1=value-1
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:05:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 22 22:05:27.386: INFO: Pod name wrapped-volume-race-d3250dd9-1e91-11e9-9284-e2fd7d97dccb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d3250dd9-1e91-11e9-9284-e2fd7d97dccb in namespace e2e-tests-emptydir-wrapper-6d76g, will wait for the garbage collector to delete the pods
Jan 22 22:05:41.492: INFO: Deleting ReplicationController wrapped-volume-race-d3250dd9-1e91-11e9-9284-e2fd7d97dccb took: 5.267887ms
Jan 22 22:05:41.594: INFO: Terminating ReplicationController wrapped-volume-race-d3250dd9-1e91-11e9-9284-e2fd7d97dccb pods took: 101.544505ms
STEP: Creating RC which spawns configmap-volume pods
Jan 22 22:06:22.107: INFO: Pod name wrapped-volume-race-f3cdae03-1e91-11e9-9284-e2fd7d97dccb: Found 0 pods out of 5
Jan 22 22:06:27.113: INFO: Pod name wrapped-volume-race-f3cdae03-1e91-11e9-9284-e2fd7d97dccb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f3cdae03-1e91-11e9-9284-e2fd7d97dccb in namespace e2e-tests-emptydir-wrapper-6d76g, will wait for the garbage collector to delete the pods
Jan 22 22:06:37.193: INFO: Deleting ReplicationController wrapped-volume-race-f3cdae03-1e91-11e9-9284-e2fd7d97dccb took: 8.204277ms
Jan 22 22:06:37.294: INFO: Terminating ReplicationController wrapped-volume-race-f3cdae03-1e91-11e9-9284-e2fd7d97dccb pods took: 100.378557ms
STEP: Creating RC which spawns configmap-volume pods
Jan 22 22:07:12.709: INFO: Pod name wrapped-volume-race-11f6a4bd-1e92-11e9-9284-e2fd7d97dccb: Found 0 pods out of 5
Jan 22 22:07:17.715: INFO: Pod name wrapped-volume-race-11f6a4bd-1e92-11e9-9284-e2fd7d97dccb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-11f6a4bd-1e92-11e9-9284-e2fd7d97dccb in namespace e2e-tests-emptydir-wrapper-6d76g, will wait for the garbage collector to delete the pods
Jan 22 22:07:27.792: INFO: Deleting ReplicationController wrapped-volume-race-11f6a4bd-1e92-11e9-9284-e2fd7d97dccb took: 6.050549ms
Jan 22 22:07:27.893: INFO: Terminating ReplicationController wrapped-volume-race-11f6a4bd-1e92-11e9-9284-e2fd7d97dccb pods took: 101.323152ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:08:12.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-6d76g" for this suite.
Jan 22 22:08:18.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:08:18.676: INFO: namespace: e2e-tests-emptydir-wrapper-6d76g, resource: bindings, ignored listing per whitelist
Jan 22 22:08:18.683: INFO: namespace e2e-tests-emptydir-wrapper-6d76g deletion completed in 6.111031122s

• [SLOW TEST:171.564 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:08:18.683: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:08:18.756: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-gq7qj" to be "success or failure"
Jan 22 22:08:18.760: INFO: Pod "downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159017ms
Jan 22 22:08:20.766: INFO: Pod "downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009860565s
STEP: Saw pod success
Jan 22 22:08:20.766: INFO: Pod "downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:08:20.768: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:08:20.786: INFO: Waiting for pod downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:08:20.789: INFO: Pod downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:08:20.789: INFO: Unexpected error occurred: expected "[1-9]" in container output: Expected
    <string>: 
to match regular expression
    <string>: [1-9]
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-gq7qj".
STEP: Found 4 events.
Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:18 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-gq7qj/downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:19 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:19 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:19 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:08:20.803: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:08:20.803: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:08:20.803: INFO: 
Jan 22 22:08:20.806: INFO: 
Logging node info for node secconf-master
Jan 22 22:08:20.811: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41217,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:08:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:08:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:08:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:08:13 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:08:20.811: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:08:20.813: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:08:20.819: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:08:20.819: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:08:20.819: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:08:20.819: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:08:20.819: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:08:20.819: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:08:20.819: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:08:20.819: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:08:20.819: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:08:20.819: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:08:20.819: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:08:20.819: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:08:20.819: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:08:20.819: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:08:20.819: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:08:20.819: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:08:20.840: INFO: 
Latency metrics for node secconf-master
Jan 22 22:08:20.840: INFO: 
Logging node info for node secconf-node
Jan 22 22:08:20.842: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41422,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:08:20.843: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:08:20.845: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:08:20.852: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:08:20.852: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:08:20.852: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:08:20.852: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:08:20.852: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:08:20.852: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:08:20.852: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:08:20.852: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:08:20.852: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:08:20.852: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:08:20.852: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:08:20.852: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:08:20.852: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:08:20.876: INFO: 
Latency metrics for node secconf-node
Jan 22 22:08:20.876: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.140318s}
Jan 22 22:08:20.876: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}
Jan 22 22:08:20.876: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.996776s}
Jan 22 22:08:20.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gq7qj" for this suite.
Jan 22 22:08:26.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:08:26.934: INFO: namespace: e2e-tests-projected-gq7qj, resource: bindings, ignored listing per whitelist
Jan 22 22:08:26.955: INFO: namespace e2e-tests-projected-gq7qj deletion completed in 6.075832604s

• Failure [8.271 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc001d88340>: {
          s: "expected \"[1-9]\" in container output: Expected\n    <string>: \nto match regular expression\n    <string>: [1-9]",
      }
      expected "[1-9]" in container output: Expected
          <string>: 
      to match regular expression
          <string>: [1-9]
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:08:26.955: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:08:31.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-s4mnr" for this suite.
Jan 22 22:08:53.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:08:53.087: INFO: namespace: e2e-tests-replication-controller-s4mnr, resource: bindings, ignored listing per whitelist
Jan 22 22:08:53.126: INFO: namespace e2e-tests-replication-controller-s4mnr deletion completed in 22.074716662s

• [SLOW TEST:26.172 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:08:53.127: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 22 22:08:53.186: INFO: Waiting up to 5m0s for pod "pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-8mzdw" to be "success or failure"
Jan 22 22:08:53.190: INFO: Pod "pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946112ms
Jan 22 22:08:55.196: INFO: Pod "pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010078105s
STEP: Saw pod success
Jan 22 22:08:55.196: INFO: Pod "pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:08:55.200: INFO: Trying to get logs from node secconf-node pod pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:08:55.214: INFO: Waiting for pod pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:08:55.218: INFO: Pod pod-4ddb728f-1e92-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:08:55.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8mzdw" for this suite.
Jan 22 22:09:01.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:09:01.272: INFO: namespace: e2e-tests-emptydir-8mzdw, resource: bindings, ignored listing per whitelist
Jan 22 22:09:01.308: INFO: namespace e2e-tests-emptydir-8mzdw deletion completed in 6.086191192s

• [SLOW TEST:8.181 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:09:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:09:01.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-88brm" for this suite.
Jan 22 22:09:23.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:09:23.459: INFO: namespace: e2e-tests-pods-88brm, resource: bindings, ignored listing per whitelist
Jan 22 22:09:23.510: INFO: namespace e2e-tests-pods-88brm deletion completed in 22.137991565s

• [SLOW TEST:22.202 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:09:23.510: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 22 22:09:23.566: INFO: Waiting up to 5m0s for pod "downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-8h44h" to be "success or failure"
Jan 22 22:09:23.569: INFO: Pod "downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904484ms
Jan 22 22:09:25.571: INFO: Pod "downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005398817s
STEP: Saw pod success
Jan 22 22:09:25.572: INFO: Pod "downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:09:25.574: INFO: Trying to get logs from node secconf-node pod downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 22:09:25.589: INFO: Waiting for pod downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:09:25.591: INFO: Pod downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:09:25.592: INFO: Unexpected error occurred: expected "HOST_IP=(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)" in container output: Expected
    <string>: 
to match regular expression
    <string>: HOST_IP=(?:\d+)\.(?:\d+)\.(?:\d+)\.(?:\d+)
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-downward-api-8h44h".
STEP: Found 4 events.
Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:23 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-8h44h/downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:24 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "docker.io/library/busybox:1.29" already present on machine
Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:24 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:24 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:09:25.604: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:09:25.604: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:25.604: INFO: 
Jan 22 22:09:25.607: INFO: 
Logging node info for node secconf-master
Jan 22 22:09:25.609: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41604,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:23 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:25.609: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:09:25.611: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:09:25.616: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:25.616: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:25.616: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:25.616: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:25.616: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:25.616: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:25.616: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:25.616: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:25.616: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:25.616: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:25.616: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:25.616: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:25.616: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:25.616: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:25.616: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:25.616: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:25.636: INFO: 
Latency metrics for node secconf-master
Jan 22 22:09:25.636: INFO: 
Logging node info for node secconf-node
Jan 22 22:09:25.639: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41589,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:25.639: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:09:25.641: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:09:25.646: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:25.646: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:09:25.646: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:09:25.646: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:25.646: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:25.646: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:25.646: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:25.646: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:25.646: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:25.646: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:09:25.646: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:25.646: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:25.646: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:25.664: INFO: 
Latency metrics for node secconf-node
Jan 22 22:09:25.664: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.140318s}
Jan 22 22:09:25.664: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}
Jan 22 22:09:25.664: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.996776s}
Jan 22 22:09:25.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8h44h" for this suite.
Jan 22 22:09:31.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:09:31.728: INFO: namespace: e2e-tests-downward-api-8h44h, resource: bindings, ignored listing per whitelist
Jan 22 22:09:31.747: INFO: namespace e2e-tests-downward-api-8h44h deletion completed in 6.080147403s

• Failure [8.237 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc001a96c30>: {
          s: "expected \"HOST_IP=(?:\\\\d+)\\\\.(?:\\\\d+)\\\\.(?:\\\\d+)\\\\.(?:\\\\d+)\" in container output: Expected\n    <string>: \nto match regular expression\n    <string>: HOST_IP=(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)",
      }
      expected "HOST_IP=(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)" in container output: Expected
          <string>: 
      to match regular expression
          <string>: HOST_IP=(?:\d+)\.(?:\d+)\.(?:\d+)\.(?:\d+)
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:09:31.748: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 22 22:09:31.804: INFO: Waiting up to 5m0s for pod "pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-vbw9p" to be "success or failure"
Jan 22 22:09:31.811: INFO: Pod "pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.778225ms
Jan 22 22:09:33.815: INFO: Pod "pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010387528s
STEP: Saw pod success
Jan 22 22:09:33.815: INFO: Pod "pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:09:33.818: INFO: Trying to get logs from node secconf-node pod pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:09:33.839: INFO: Waiting for pod pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:09:33.843: INFO: Pod pod-64e077e9-1e92-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:09:33.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vbw9p" for this suite.
Jan 22 22:09:39.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:09:39.916: INFO: namespace: e2e-tests-emptydir-vbw9p, resource: bindings, ignored listing per whitelist
Jan 22 22:09:39.938: INFO: namespace e2e-tests-emptydir-vbw9p deletion completed in 6.090861263s

• [SLOW TEST:8.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:09:39.938: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:09:39.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-ksmjc" to be "success or failure"
Jan 22 22:09:40.003: INFO: Pod "downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122948ms
Jan 22 22:09:42.007: INFO: Pod "downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007790777s
STEP: Saw pod success
Jan 22 22:09:42.007: INFO: Pod "downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:09:42.010: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:09:42.024: INFO: Waiting for pod downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:09:42.027: INFO: Pod downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:09:42.027: INFO: Unexpected error occurred: expected "mode of file \"/etc/podinfo/podname\": -r--------" in container output: Expected
    <string>: 
to contain substring
    <string>: mode of file "/etc/podinfo/podname": -r--------
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-downward-api-ksmjc".
STEP: Found 4 events.
Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-ksmjc/downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:09:42.039: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:09:42.039: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:42.039: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:42.040: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:42.040: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:42.040: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:42.040: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:42.040: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:42.040: INFO: 
Jan 22 22:09:42.043: INFO: 
Logging node info for node secconf-master
Jan 22 22:09:42.045: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41647,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:33 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:33 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:33 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:33 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:42.046: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:09:42.048: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:09:42.052: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:42.052: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:42.052: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:42.052: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:42.052: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:42.052: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:42.052: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:42.052: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:42.052: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:42.052: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:42.052: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:42.052: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:42.052: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:42.052: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:42.052: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:42.052: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:42.066: INFO: 
Latency metrics for node secconf-master
Jan 22 22:09:42.066: INFO: 
Logging node info for node secconf-node
Jan 22 22:09:42.068: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41674,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:42.068: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:09:42.071: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:09:42.076: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:42.076: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:42.076: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:42.076: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:42.076: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:42.076: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:42.076: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:09:42.076: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:42.076: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:42.076: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:42.076: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:42.076: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:09:42.076: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:09:42.094: INFO: 
Latency metrics for node secconf-node
Jan 22 22:09:42.094: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}
Jan 22 22:09:42.094: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}
Jan 22 22:09:42.094: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.996776s}
Jan 22 22:09:42.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ksmjc" for this suite.
Jan 22 22:09:48.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:09:48.159: INFO: namespace: e2e-tests-downward-api-ksmjc, resource: bindings, ignored listing per whitelist
Jan 22 22:09:48.174: INFO: namespace e2e-tests-downward-api-ksmjc deletion completed in 6.077394683s

• Failure [8.236 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc0008df280>: {
          s: "expected \"mode of file \\\"/etc/podinfo/podname\\\": -r--------\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: mode of file \"/etc/podinfo/podname\": -r--------",
      }
      expected "mode of file \"/etc/podinfo/podname\": -r--------" in container output: Expected
          <string>: 
      to contain substring
          <string>: mode of file "/etc/podinfo/podname": -r--------
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:09:48.175: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 22 22:09:48.236: INFO: Waiting up to 5m0s for pod "pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-prhcl" to be "success or failure"
Jan 22 22:09:48.239: INFO: Pod "pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.610966ms
Jan 22 22:09:50.244: INFO: Pod "pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007333575s
STEP: Saw pod success
Jan 22 22:09:50.244: INFO: Pod "pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:09:50.246: INFO: Trying to get logs from node secconf-node pod pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:09:50.264: INFO: Waiting for pod pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:09:50.266: INFO: Pod pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:09:50.266: INFO: Unexpected error occurred: expected "perms of file \"/test-volume/test-file\": -rw-rw-rw-" in container output: Expected
    <string>: 
to contain substring
    <string>: perms of file "/test-volume/test-file": -rw-rw-rw-
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-emptydir-prhcl".
STEP: Found 4 events.
Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-emptydir-prhcl/pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0" already present on machine
Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:09:50.277: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:09:50.277: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:50.278: INFO: 
Jan 22 22:09:50.280: INFO: 
Logging node info for node secconf-master
Jan 22 22:09:50.283: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41687,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:43 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:43 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:43 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:43 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:50.283: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:09:50.285: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:09:50.290: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:50.290: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:50.290: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:50.290: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:50.290: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:50.290: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:50.290: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:50.290: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:50.290: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:50.290: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:50.290: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:50.290: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:50.290: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:50.290: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:50.290: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:50.290: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:50.303: INFO: 
Latency metrics for node secconf-master
Jan 22 22:09:50.303: INFO: 
Logging node info for node secconf-node
Jan 22 22:09:50.306: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41674,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:50.306: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:09:50.308: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:09:50.313: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:50.313: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:09:50.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:09:50.313: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:50.313: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:50.313: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:50.313: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:50.313: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:50.313: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:50.313: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:09:50.313: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:50.313: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:50.313: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:50.334: INFO: 
Latency metrics for node secconf-node
Jan 22 22:09:50.334: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}
Jan 22 22:09:50.334: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}
Jan 22 22:09:50.334: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.959052s}
Jan 22 22:09:50.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-prhcl" for this suite.
Jan 22 22:09:56.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:09:56.378: INFO: namespace: e2e-tests-emptydir-prhcl, resource: bindings, ignored listing per whitelist
Jan 22 22:09:56.417: INFO: namespace e2e-tests-emptydir-prhcl deletion completed in 6.080276937s

• Failure [8.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc002b83a40>: {
          s: "expected \"perms of file \\\"/test-volume/test-file\\\": -rw-rw-rw-\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: perms of file \"/test-volume/test-file\": -rw-rw-rw-",
      }
      expected "perms of file \"/test-volume/test-file\": -rw-rw-rw-" in container output: Expected
          <string>: 
      to contain substring
          <string>: perms of file "/test-volume/test-file": -rw-rw-rw-
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:09:56.417: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:09:56.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-kstc9" to be "success or failure"
Jan 22 22:09:56.481: INFO: Pod "downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719634ms
Jan 22 22:09:58.484: INFO: Pod "downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006654943s
STEP: Saw pod success
Jan 22 22:09:58.484: INFO: Pod "downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:09:58.486: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:09:58.501: INFO: Waiting for pod downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:09:58.503: INFO: Pod downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:09:58.503: INFO: Unexpected error occurred: expected "[1-9]" in container output: Expected
    <string>: 
to match regular expression
    <string>: [1-9]
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-kstc9".
STEP: Found 4 events.
Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:56 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-kstc9/downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:57 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:57 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:57 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:09:58.517: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:09:58.517: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:09:58.517: INFO: 
Jan 22 22:09:58.520: INFO: 
Logging node info for node secconf-master
Jan 22 22:09:58.522: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41725,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:53 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:58.522: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:09:58.525: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:09:58.530: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:58.530: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:58.530: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:58.530: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:09:58.530: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:58.530: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:58.530: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:58.530: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:58.530: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:58.530: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:58.530: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:58.530: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:58.530: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:09:58.530: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:58.530: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:58.530: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:58.545: INFO: 
Latency metrics for node secconf-master
Jan 22 22:09:58.545: INFO: 
Logging node info for node secconf-node
Jan 22 22:09:58.547: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41720,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:09:58.548: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:09:58.550: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:09:58.555: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:58.555: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:09:58.555: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:09:58.555: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:58.555: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:09:58.555: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:09:58.555: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:58.555: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:09:58.555: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:09:58.555: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:09:58.555: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:09:58.555: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:09:58.555: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:09:58.572: INFO: 
Latency metrics for node secconf-node
Jan 22 22:09:58.572: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}
Jan 22 22:09:58.572: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}
Jan 22 22:09:58.572: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.959052s}
Jan 22 22:09:58.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kstc9" for this suite.
Jan 22 22:10:04.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:10:04.625: INFO: namespace: e2e-tests-projected-kstc9, resource: bindings, ignored listing per whitelist
Jan 22 22:10:04.656: INFO: namespace e2e-tests-projected-kstc9 deletion completed in 6.081337408s

• Failure [8.239 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc0003c7d40>: {
          s: "expected \"[1-9]\" in container output: Expected\n    <string>: \nto match regular expression\n    <string>: [1-9]",
      }
      expected "[1-9]" in container output: Expected
          <string>: 
      to match regular expression
          <string>: [1-9]
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:10:04.656: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-rhl96/secret-test-787f43ba-1e92-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:10:04.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-rhl96" to be "success or failure"
Jan 22 22:10:04.728: INFO: Pod "pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.112605ms
Jan 22 22:10:06.733: INFO: Pod "pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007953666s
STEP: Saw pod success
Jan 22 22:10:06.733: INFO: Pod "pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:10:06.737: INFO: Trying to get logs from node secconf-node pod pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb container env-test: <nil>
STEP: delete the pod
Jan 22 22:10:06.759: INFO: Waiting for pod pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:10:06.762: INFO: Pod pod-configmaps-787fba5c-1e92-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:10:06.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rhl96" for this suite.
Jan 22 22:10:12.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:10:12.825: INFO: namespace: e2e-tests-secrets-rhl96, resource: bindings, ignored listing per whitelist
Jan 22 22:10:12.852: INFO: namespace e2e-tests-secrets-rhl96 deletion completed in 6.084964439s

• [SLOW TEST:8.195 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:10:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 22 22:10:22.984: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:10:22.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-r5tfk" for this suite.
Jan 22 22:10:29.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:10:29.049: INFO: namespace: e2e-tests-gc-r5tfk, resource: bindings, ignored listing per whitelist
Jan 22 22:10:29.085: INFO: namespace e2e-tests-gc-r5tfk deletion completed in 6.097037469s

• [SLOW TEST:16.233 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:10:29.085: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:10:29.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-krkq5" to be "success or failure"
Jan 22 22:10:29.149: INFO: Pod "downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.490355ms
Jan 22 22:10:31.152: INFO: Pod "downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006315817s
STEP: Saw pod success
Jan 22 22:10:31.152: INFO: Pod "downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:10:31.155: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:10:31.173: INFO: Waiting for pod downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:10:31.176: INFO: Pod downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:10:31.176: INFO: Unexpected error occurred: expected "mode of file \"/etc/podinfo/podname\": -r--------" in container output: Expected
    <string>: 
to contain substring
    <string>: mode of file "/etc/podinfo/podname": -r--------
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-krkq5".
STEP: Found 4 events.
Jan 22 22:10:31.179: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-krkq5/downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:10:31.180: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:10:31.180: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:10:31.180: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:10:31.187: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:10:31.187: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:10:31.187: INFO: 
Jan 22 22:10:31.190: INFO: 
Logging node info for node secconf-master
Jan 22 22:10:31.192: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41978,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:10:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:10:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:10:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:10:23 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:10:31.193: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:10:31.195: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:10:31.200: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:10:31.200: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:10:31.200: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:10:31.200: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:10:31.200: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:10:31.200: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:10:31.200: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:10:31.200: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:10:31.200: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:10:31.200: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:10:31.200: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:10:31.200: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:10:31.200: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:10:31.200: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:10:31.200: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:10:31.200: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:10:31.218: INFO: 
Latency metrics for node secconf-master
Jan 22 22:10:31.218: INFO: 
Logging node info for node secconf-node
Jan 22 22:10:31.221: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:42071,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:10:31.221: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:10:31.223: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:10:31.229: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:10:31.229: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:10:31.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:10:31.229: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:10:31.229: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:10:31.229: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:10:31.229: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:10:31.229: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:10:31.229: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:10:31.229: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:10:31.229: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:10:31.229: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:10:31.229: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:10:31.250: INFO: 
Latency metrics for node secconf-node
Jan 22 22:10:31.250: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}
Jan 22 22:10:31.250: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}
Jan 22 22:10:31.250: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.954125s}
Jan 22 22:10:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-krkq5" for this suite.
Jan 22 22:10:37.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:10:37.317: INFO: namespace: e2e-tests-projected-krkq5, resource: bindings, ignored listing per whitelist
Jan 22 22:10:37.340: INFO: namespace e2e-tests-projected-krkq5 deletion completed in 6.086546629s

• Failure [8.255 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc002a79700>: {
          s: "expected \"mode of file \\\"/etc/podinfo/podname\\\": -r--------\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: mode of file \"/etc/podinfo/podname\": -r--------",
      }
      expected "mode of file \"/etc/podinfo/podname\": -r--------" in container output: Expected
          <string>: 
      to contain substring
          <string>: mode of file "/etc/podinfo/podname": -r--------
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:10:37.340: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:10:37.407: INFO: Creating deployment "nginx-deployment"
Jan 22 22:10:37.412: INFO: Waiting for observed generation 1
Jan 22 22:10:39.422: INFO: Waiting for all required pods to come up
Jan 22 22:10:39.427: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 22 22:10:45.436: INFO: Waiting for deployment "nginx-deployment" to complete
Jan 22 22:10:45.442: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jan 22 22:10:45.448: INFO: Updating deployment nginx-deployment
Jan 22 22:10:45.448: INFO: Waiting for observed generation 2
Jan 22 22:10:47.454: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 22 22:10:47.458: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 22 22:10:47.461: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 22 22:10:47.509: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 22 22:10:47.509: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 22 22:10:47.511: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 22 22:10:47.515: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jan 22 22:10:47.515: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jan 22 22:10:47.520: INFO: Updating deployment nginx-deployment
Jan 22 22:10:47.520: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jan 22 22:10:47.525: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 22 22:10:47.527: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 22 22:10:47.538: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9gzrt/deployments/nginx-deployment,UID:8bfbd889-1e92-11e9-945c-000c293afc9e,ResourceVersion:42304,Generation:3,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-01-22 22:10:45 +0000 UTC 2019-01-22 22:10:37 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-01-22 22:10:47 +0000 UTC 2019-01-22 22:10:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jan 22 22:10:47.551: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9gzrt/replicasets/nginx-deployment-65bbdb5f8,UID:90c6b2d4-1e92-11e9-945c-000c293afc9e,ResourceVersion:42303,Generation:3,CreationTimestamp:2019-01-22 22:10:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 8bfbd889-1e92-11e9-945c-000c293afc9e 0xc0009f70b7 0xc0009f70b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 22 22:10:47.551: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jan 22 22:10:47.551: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9gzrt/replicasets/nginx-deployment-555b55d965,UID:8bfe1219-1e92-11e9-945c-000c293afc9e,ResourceVersion:42301,Generation:3,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 8bfbd889-1e92-11e9-945c-000c293afc9e 0xc0009f6da7 0xc0009f6da8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jan 22 22:10:47.578: INFO: Pod "nginx-deployment-555b55d965-5m79f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5m79f,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-5m79f,UID:92050baf-1e92-11e9-945c-000c293afc9e,ResourceVersion:42313,Generation:0,CreationTimestamp:2019-01-22 22:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc000055957 0xc000055958}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000036710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000352090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.578: INFO: Pod "nginx-deployment-555b55d965-7rrkl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-7rrkl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-7rrkl,UID:8c02d259-1e92-11e9-945c-000c293afc9e,ResourceVersion:42218,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.65/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc000352810 0xc000352811}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003bccf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003bce10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.65,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://e1a1c5898aedaa617968e6b234b55f38301685d901246b581cca682aca551742}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.578: INFO: Pod "nginx-deployment-555b55d965-88hnh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-88hnh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-88hnh,UID:8c050b34-1e92-11e9-945c-000c293afc9e,ResourceVersion:42223,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.68/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc0003bd277 0xc0003bd278}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003bd300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003bd330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.68,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://9168ae73155d667109445582d2647602fbb120d504a26e362b5e639045d88d30}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-c6t4h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-c6t4h,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-c6t4h,UID:8c0521b5-1e92-11e9-945c-000c293afc9e,ResourceVersion:42201,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.64/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc0003bd727 0xc0003bd728}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003bd920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003bdfa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.64,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://0510e372c473f5458c8336b106f9ab6f8f2aef94416364550f40c25231e7e31e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-dzh9x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dzh9x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-dzh9x,UID:8c02e448-1e92-11e9-945c-000c293afc9e,ResourceVersion:42206,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.66/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001e1ca97 0xc001e1ca98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e1ced0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e1d100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.66,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://8397497cc19353634ad1105cec7f2d6064887be9f3bfc520588db2f7fd4a5141}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-f9bwz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f9bwz,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-f9bwz,UID:92060e34-1e92-11e9-945c-000c293afc9e,ResourceVersion:42318,Generation:0,CreationTimestamp:2019-01-22 22:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001e1d827 0xc001e1d828}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e1d8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e1d8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-qz87b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qz87b,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-qz87b,UID:8bffd8e2-1e92-11e9-945c-000c293afc9e,ResourceVersion:42232,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.62/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001dda200 0xc001dda201}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001dda280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001dda7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.62,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://a74f03e0b7747c64f1e5e79377ba7f5f0408763649a7bd75a93ec0d40c209d46}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-rwpnl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-rwpnl,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-rwpnl,UID:8c02a75b-1e92-11e9-945c-000c293afc9e,ResourceVersion:42213,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.63/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001dda9d7 0xc001dda9d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ddae80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ddaea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.63,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://9b2794cb5c03e86f39e15e0e746d1310f2e15b3847c5745e6944e7a650c4a87b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-t92j9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-t92j9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-t92j9,UID:9205eacb-1e92-11e9-945c-000c293afc9e,ResourceVersion:42316,Generation:0,CreationTimestamp:2019-01-22 22:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001ddba27 0xc001ddba28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ddbaa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ddbac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.579: INFO: Pod "nginx-deployment-555b55d965-x8frh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-x8frh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-x8frh,UID:8c02e9c0-1e92-11e9-945c-000c293afc9e,ResourceVersion:42184,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.59/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001ddbdd0 0xc001ddbdd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ddbe40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ddbe60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.59,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://b4e37d45ec7a624d69dc388f0d75b83f624abfe30e72d26a9593b3d68f9b097e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.580: INFO: Pod "nginx-deployment-555b55d965-zjq5n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zjq5n,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-555b55d965-zjq5n,UID:8c05022e-1e92-11e9-945c-000c293afc9e,ResourceVersion:42189,Generation:0,CreationTimestamp:2019-01-22 22:10:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.67/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 8bfe1219-1e92-11e9-945c-000c293afc9e 0xc001ddbf67 0xc001ddbf68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a6c3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a6c400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:37 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.67,StartTime:2019-01-22 22:10:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-22 22:10:39 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://6d1212ed30e7ff3e1aac5a39a8a2f112e1dd34a74ca60863d0b41664b0e7e941}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.580: INFO: Pod "nginx-deployment-65bbdb5f8-4vgdc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-4vgdc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-4vgdc,UID:920579d7-1e92-11e9-945c-000c293afc9e,ResourceVersion:42312,Generation:0,CreationTimestamp:2019-01-22 22:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc001a6cd17 0xc001a6cd18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a6cf10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a6cfc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.580: INFO: Pod "nginx-deployment-65bbdb5f8-c2bz6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-c2bz6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-c2bz6,UID:90c73810-1e92-11e9-945c-000c293afc9e,ResourceVersion:42289,Generation:0,CreationTimestamp:2019-01-22 22:10:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.70/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc001a6d3b0 0xc001a6d3b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a6d7f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a6d8f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:,StartTime:2019-01-22 22:10:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.580: INFO: Pod "nginx-deployment-65bbdb5f8-dh258" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-dh258,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-dh258,UID:920640a4-1e92-11e9-945c-000c293afc9e,ResourceVersion:42317,Generation:0,CreationTimestamp:2019-01-22 22:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc001a6df00 0xc001a6df01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001940890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001940950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.580: INFO: Pod "nginx-deployment-65bbdb5f8-gf65m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gf65m,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-gf65m,UID:90c7e21c-1e92-11e9-945c-000c293afc9e,ResourceVersion:42294,Generation:0,CreationTimestamp:2019-01-22 22:10:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.72/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc001940ae0 0xc001940ae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001940d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001940d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:,StartTime:2019-01-22 22:10:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.581: INFO: Pod "nginx-deployment-65bbdb5f8-jrfdr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-jrfdr,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-jrfdr,UID:90ce4add-1e92-11e9-945c-000c293afc9e,ResourceVersion:42298,Generation:0,CreationTimestamp:2019-01-22 22:10:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.71/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc001941020 0xc001941021}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001941110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001941150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:,StartTime:2019-01-22 22:10:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.581: INFO: Pod "nginx-deployment-65bbdb5f8-mdx2j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-mdx2j,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-mdx2j,UID:90c7d4d0-1e92-11e9-945c-000c293afc9e,ResourceVersion:42288,Generation:0,CreationTimestamp:2019-01-22 22:10:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.69/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc0019415e0 0xc0019415e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001941870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001941890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:,StartTime:2019-01-22 22:10:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.581: INFO: Pod "nginx-deployment-65bbdb5f8-v62mv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-v62mv,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-v62mv,UID:90cd7f25-1e92-11e9-945c-000c293afc9e,ResourceVersion:42296,Generation:0,CreationTimestamp:2019-01-22 22:10:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.73/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc001941d20 0xc001941d21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3e190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3e1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:,StartTime:2019-01-22 22:10:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 22 22:10:47.581: INFO: Pod "nginx-deployment-65bbdb5f8-xs7br" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-xs7br,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-9gzrt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9gzrt/pods/nginx-deployment-65bbdb5f8-xs7br,UID:92066243-1e92-11e9-945c-000c293afc9e,ResourceVersion:42319,Generation:0,CreationTimestamp:2019-01-22 22:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 90c6b2d4-1e92-11e9-945c-000c293afc9e 0xc000f3eb60 0xc000f3eb61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8mfzd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8mfzd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8mfzd true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3f530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3f550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:10:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:10:47.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9gzrt" for this suite.
Jan 22 22:10:53.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:10:53.795: INFO: namespace: e2e-tests-deployment-9gzrt, resource: bindings, ignored listing per whitelist
Jan 22 22:10:53.818: INFO: namespace e2e-tests-deployment-9gzrt deletion completed in 6.22411863s

• [SLOW TEST:16.479 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:10:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 22 22:11:04.496: INFO: Successfully updated pod "labelsupdate95d8e7dd-1e92-11e9-9284-e2fd7d97dccb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:11:06.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-62npn" for this suite.
Jan 22 22:11:28.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:11:28.576: INFO: namespace: e2e-tests-projected-62npn, resource: bindings, ignored listing per whitelist
Jan 22 22:11:28.629: INFO: namespace e2e-tests-projected-62npn deletion completed in 22.101105068s

• [SLOW TEST:34.811 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:11:28.630: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:11:28.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-wwcqq" for this suite.
Jan 22 22:11:34.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:11:34.744: INFO: namespace: e2e-tests-services-wwcqq, resource: bindings, ignored listing per whitelist
Jan 22 22:11:34.777: INFO: namespace e2e-tests-services-wwcqq deletion completed in 6.079766568s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.147 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:11:34.777: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 22 22:11:34.836: INFO: Waiting up to 5m0s for pod "downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-j9rvv" to be "success or failure"
Jan 22 22:11:34.839: INFO: Pod "downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.306824ms
Jan 22 22:11:36.845: INFO: Pod "downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009039282s
STEP: Saw pod success
Jan 22 22:11:36.845: INFO: Pod "downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:11:36.848: INFO: Trying to get logs from node secconf-node pod downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 22:11:36.866: INFO: Waiting for pod downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:11:36.870: INFO: Pod downward-api-ae358a18-1e92-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:11:36.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-j9rvv" for this suite.
Jan 22 22:11:42.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:11:42.903: INFO: namespace: e2e-tests-downward-api-j9rvv, resource: bindings, ignored listing per whitelist
Jan 22 22:11:42.956: INFO: namespace e2e-tests-downward-api-j9rvv deletion completed in 6.082425062s

• [SLOW TEST:8.179 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:11:42.956: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 22 22:12:03.644: INFO: Successfully updated pod "annotationupdateb316778a-1e92-11e9-9284-e2fd7d97dccb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:12:05.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dp2cc" for this suite.
Jan 22 22:12:27.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:12:27.696: INFO: namespace: e2e-tests-downward-api-dp2cc, resource: bindings, ignored listing per whitelist
Jan 22 22:12:27.754: INFO: namespace e2e-tests-downward-api-dp2cc deletion completed in 22.082843207s

• [SLOW TEST:44.797 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:12:27.754: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Jan 22 22:12:27.808: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-259822818 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:12:27.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sx87z" for this suite.
Jan 22 22:12:33.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:12:33.953: INFO: namespace: e2e-tests-kubectl-sx87z, resource: bindings, ignored listing per whitelist
Jan 22 22:12:33.984: INFO: namespace e2e-tests-kubectl-sx87z deletion completed in 6.08831647s

• [SLOW TEST:6.230 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:12:33.984: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d17ffa16-1e92-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:12:34.067: INFO: Waiting up to 5m0s for pod "pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-x9h2d" to be "success or failure"
Jan 22 22:12:34.070: INFO: Pod "pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868535ms
Jan 22 22:12:36.075: INFO: Pod "pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007955919s
STEP: Saw pod success
Jan 22 22:12:36.075: INFO: Pod "pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:12:36.078: INFO: Trying to get logs from node secconf-node pod pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 22:12:36.096: INFO: Waiting for pod pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:12:36.099: INFO: Pod pod-secrets-d183710b-1e92-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:12:36.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-x9h2d" for this suite.
Jan 22 22:12:42.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:12:42.175: INFO: namespace: e2e-tests-secrets-x9h2d, resource: bindings, ignored listing per whitelist
Jan 22 22:12:42.182: INFO: namespace e2e-tests-secrets-x9h2d deletion completed in 6.079029713s
STEP: Destroying namespace "e2e-tests-secret-namespace-pwsxn" for this suite.
Jan 22 22:12:48.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:12:48.211: INFO: namespace: e2e-tests-secret-namespace-pwsxn, resource: bindings, ignored listing per whitelist
Jan 22 22:12:48.262: INFO: namespace e2e-tests-secret-namespace-pwsxn deletion completed in 6.079746641s

• [SLOW TEST:14.277 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:12:48.262: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-pmwmn
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Jan 22 22:12:48.331: INFO: Found 0 stateful pods, waiting for 3
Jan 22 22:12:58.336: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:12:58.336: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:12:58.336: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 22 22:12:58.361: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 22 22:13:08.391: INFO: Updating stateful set ss2
Jan 22 22:13:08.397: INFO: Waiting for Pod e2e-tests-statefulset-pmwmn/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jan 22 22:13:18.441: INFO: Found 2 stateful pods, waiting for 3
Jan 22 22:13:28.445: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:13:28.446: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:13:28.446: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 22 22:13:28.467: INFO: Updating stateful set ss2
Jan 22 22:13:28.474: INFO: Waiting for Pod e2e-tests-statefulset-pmwmn/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 22 22:13:38.497: INFO: Updating stateful set ss2
Jan 22 22:13:38.502: INFO: Waiting for StatefulSet e2e-tests-statefulset-pmwmn/ss2 to complete update
Jan 22 22:13:38.502: INFO: Waiting for Pod e2e-tests-statefulset-pmwmn/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 22 22:13:48.507: INFO: Deleting all statefulset in ns e2e-tests-statefulset-pmwmn
Jan 22 22:13:48.510: INFO: Scaling statefulset ss2 to 0
Jan 22 22:14:08.522: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 22:14:08.525: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:14:08.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-pmwmn" for this suite.
Jan 22 22:14:14.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:14:14.617: INFO: namespace: e2e-tests-statefulset-pmwmn, resource: bindings, ignored listing per whitelist
Jan 22 22:14:14.622: INFO: namespace e2e-tests-statefulset-pmwmn deletion completed in 6.084872243s

• [SLOW TEST:86.361 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:14:14.622: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-9nlbq
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-9nlbq
STEP: Deleting pre-stop pod
Jan 22 22:14:23.721: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:14:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-9nlbq" for this suite.
Jan 22 22:15:01.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:15:01.787: INFO: namespace: e2e-tests-prestop-9nlbq, resource: bindings, ignored listing per whitelist
Jan 22 22:15:01.808: INFO: namespace e2e-tests-prestop-9nlbq deletion completed in 38.078828014s

• [SLOW TEST:47.186 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:15:01.809: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 22 22:15:01.869: INFO: Waiting up to 5m0s for pod "pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-msf6g" to be "success or failure"
Jan 22 22:15:01.871: INFO: Pod "pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476483ms
Jan 22 22:15:03.874: INFO: Pod "pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005029132s
STEP: Saw pod success
Jan 22 22:15:03.874: INFO: Pod "pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:15:03.876: INFO: Trying to get logs from node secconf-node pod pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:15:03.898: INFO: Waiting for pod pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:15:03.901: INFO: Pod pod-299c32c5-1e93-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:15:03.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-msf6g" for this suite.
Jan 22 22:15:09.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:15:09.971: INFO: namespace: e2e-tests-emptydir-msf6g, resource: bindings, ignored listing per whitelist
Jan 22 22:15:09.997: INFO: namespace e2e-tests-emptydir-msf6g deletion completed in 6.093171083s

• [SLOW TEST:8.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:15:09.998: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-2e7cf117-1e93-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:15:10.056: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-nn5fn" to be "success or failure"
Jan 22 22:15:10.061: INFO: Pod "pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.313559ms
Jan 22 22:15:12.064: INFO: Pod "pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008176994s
STEP: Saw pod success
Jan 22 22:15:12.064: INFO: Pod "pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:15:12.066: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 22:15:12.080: INFO: Waiting for pod pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:15:12.084: INFO: Pod pod-projected-configmaps-2e7d748a-1e93-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:15:12.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nn5fn" for this suite.
Jan 22 22:15:18.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:15:18.176: INFO: namespace: e2e-tests-projected-nn5fn, resource: bindings, ignored listing per whitelist
Jan 22 22:15:18.176: INFO: namespace e2e-tests-projected-nn5fn deletion completed in 6.088462475s

• [SLOW TEST:8.178 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:15:18.176: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Jan 22 22:15:18.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:18.465: INFO: stderr: ""
Jan 22 22:15:18.465: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Jan 22 22:15:19.468: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 22:15:19.468: INFO: Found 0 / 1
Jan 22 22:15:20.469: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 22:15:20.469: INFO: Found 1 / 1
Jan 22 22:15:20.469: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 22 22:15:20.473: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 22:15:20.473: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 22 22:15:20.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:20.540: INFO: stderr: ""
Jan 22 22:15:20.540: INFO: stdout: ""
Jan 22 22:15:22.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:22.609: INFO: stderr: ""
Jan 22 22:15:22.609: INFO: stdout: ""
Jan 22 22:15:24.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:24.689: INFO: stderr: ""
Jan 22 22:15:24.689: INFO: stdout: ""
Jan 22 22:15:26.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:26.758: INFO: stderr: ""
Jan 22 22:15:26.758: INFO: stdout: ""
Jan 22 22:15:28.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:28.827: INFO: stderr: ""
Jan 22 22:15:28.827: INFO: stdout: ""
Jan 22 22:15:30.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:30.898: INFO: stderr: ""
Jan 22 22:15:30.898: INFO: stdout: ""
Jan 22 22:15:32.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:32.973: INFO: stderr: ""
Jan 22 22:15:32.973: INFO: stdout: ""
Jan 22 22:15:34.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:35.043: INFO: stderr: ""
Jan 22 22:15:35.043: INFO: stdout: ""
Jan 22 22:15:37.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:37.113: INFO: stderr: ""
Jan 22 22:15:37.113: INFO: stdout: ""
Jan 22 22:15:39.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:39.189: INFO: stderr: ""
Jan 22 22:15:39.189: INFO: stdout: ""
Jan 22 22:15:41.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:41.260: INFO: stderr: ""
Jan 22 22:15:41.260: INFO: stdout: ""
Jan 22 22:15:43.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:43.334: INFO: stderr: ""
Jan 22 22:15:43.334: INFO: stdout: ""
Jan 22 22:15:45.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:45.404: INFO: stderr: ""
Jan 22 22:15:45.404: INFO: stdout: ""
Jan 22 22:15:47.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:47.495: INFO: stderr: ""
Jan 22 22:15:47.495: INFO: stdout: ""
Jan 22 22:15:49.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:49.568: INFO: stderr: ""
Jan 22 22:15:49.568: INFO: stdout: ""
Jan 22 22:15:51.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:51.641: INFO: stderr: ""
Jan 22 22:15:51.641: INFO: stdout: ""
Jan 22 22:15:53.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:53.712: INFO: stderr: ""
Jan 22 22:15:53.712: INFO: stdout: ""
Jan 22 22:15:55.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:55.788: INFO: stderr: ""
Jan 22 22:15:55.788: INFO: stdout: ""
Jan 22 22:15:57.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:57.865: INFO: stderr: ""
Jan 22 22:15:57.865: INFO: stdout: ""
Jan 22 22:15:59.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:15:59.936: INFO: stderr: ""
Jan 22 22:15:59.936: INFO: stdout: ""
Jan 22 22:16:01.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:02.008: INFO: stderr: ""
Jan 22 22:16:02.008: INFO: stdout: ""
Jan 22 22:16:04.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:04.088: INFO: stderr: ""
Jan 22 22:16:04.088: INFO: stdout: ""
Jan 22 22:16:06.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:06.159: INFO: stderr: ""
Jan 22 22:16:06.159: INFO: stdout: ""
Jan 22 22:16:08.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:08.226: INFO: stderr: ""
Jan 22 22:16:08.226: INFO: stdout: ""
Jan 22 22:16:10.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:10.297: INFO: stderr: ""
Jan 22 22:16:10.297: INFO: stdout: ""
Jan 22 22:16:12.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:12.371: INFO: stderr: ""
Jan 22 22:16:12.371: INFO: stdout: ""
Jan 22 22:16:14.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:14.438: INFO: stderr: ""
Jan 22 22:16:14.438: INFO: stdout: ""
Jan 22 22:16:16.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:16.506: INFO: stderr: ""
Jan 22 22:16:16.506: INFO: stdout: ""
Jan 22 22:16:18.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:18.581: INFO: stderr: ""
Jan 22 22:16:18.581: INFO: stdout: ""
Jan 22 22:16:20.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:20.654: INFO: stderr: ""
Jan 22 22:16:20.654: INFO: stdout: ""
Jan 22 22:16:22.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:22.729: INFO: stderr: ""
Jan 22 22:16:22.729: INFO: stdout: ""
Jan 22 22:16:24.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:24.800: INFO: stderr: ""
Jan 22 22:16:24.800: INFO: stdout: ""
Jan 22 22:16:26.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:26.869: INFO: stderr: ""
Jan 22 22:16:26.869: INFO: stdout: ""
Jan 22 22:16:28.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:28.941: INFO: stderr: ""
Jan 22 22:16:28.941: INFO: stdout: ""
Jan 22 22:16:30.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:31.010: INFO: stderr: ""
Jan 22 22:16:31.010: INFO: stdout: ""
Jan 22 22:16:33.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:33.081: INFO: stderr: ""
Jan 22 22:16:33.081: INFO: stdout: ""
Jan 22 22:16:35.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:35.158: INFO: stderr: ""
Jan 22 22:16:35.158: INFO: stdout: ""
Jan 22 22:16:37.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:37.230: INFO: stderr: ""
Jan 22 22:16:37.230: INFO: stdout: ""
Jan 22 22:16:39.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:39.297: INFO: stderr: ""
Jan 22 22:16:39.297: INFO: stdout: ""
Jan 22 22:16:41.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:41.367: INFO: stderr: ""
Jan 22 22:16:41.367: INFO: stdout: ""
Jan 22 22:16:43.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:43.437: INFO: stderr: ""
Jan 22 22:16:43.437: INFO: stdout: ""
Jan 22 22:16:45.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:45.513: INFO: stderr: ""
Jan 22 22:16:45.513: INFO: stdout: ""
Jan 22 22:16:47.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:47.587: INFO: stderr: ""
Jan 22 22:16:47.588: INFO: stdout: ""
Jan 22 22:16:49.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:49.658: INFO: stderr: ""
Jan 22 22:16:49.658: INFO: stdout: ""
Jan 22 22:16:51.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:51.727: INFO: stderr: ""
Jan 22 22:16:51.727: INFO: stdout: ""
Jan 22 22:16:53.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:53.792: INFO: stderr: ""
Jan 22 22:16:53.792: INFO: stdout: ""
Jan 22 22:16:55.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:55.866: INFO: stderr: ""
Jan 22 22:16:55.866: INFO: stdout: ""
Jan 22 22:16:57.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:16:57.937: INFO: stderr: ""
Jan 22 22:16:57.937: INFO: stdout: ""
Jan 22 22:16:59.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:00.011: INFO: stderr: ""
Jan 22 22:17:00.011: INFO: stdout: ""
Jan 22 22:17:02.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:02.087: INFO: stderr: ""
Jan 22 22:17:02.087: INFO: stdout: ""
Jan 22 22:17:04.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:04.166: INFO: stderr: ""
Jan 22 22:17:04.166: INFO: stdout: ""
Jan 22 22:17:06.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:06.235: INFO: stderr: ""
Jan 22 22:17:06.235: INFO: stdout: ""
Jan 22 22:17:08.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:08.305: INFO: stderr: ""
Jan 22 22:17:08.305: INFO: stdout: ""
Jan 22 22:17:10.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:10.376: INFO: stderr: ""
Jan 22 22:17:10.376: INFO: stdout: ""
Jan 22 22:17:12.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:12.450: INFO: stderr: ""
Jan 22 22:17:12.450: INFO: stdout: ""
Jan 22 22:17:14.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:14.519: INFO: stderr: ""
Jan 22 22:17:14.519: INFO: stdout: ""
Jan 22 22:17:16.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:16.592: INFO: stderr: ""
Jan 22 22:17:16.592: INFO: stdout: ""
Jan 22 22:17:18.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:18.677: INFO: stderr: ""
Jan 22 22:17:18.677: INFO: stdout: ""
Jan 22 22:17:20.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:20.744: INFO: stderr: ""
Jan 22 22:17:20.744: INFO: stdout: ""
Jan 22 22:17:22.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:22.811: INFO: stderr: ""
Jan 22 22:17:22.811: INFO: stdout: ""
Jan 22 22:17:24.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:24.887: INFO: stderr: ""
Jan 22 22:17:24.887: INFO: stdout: ""
Jan 22 22:17:26.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:26.957: INFO: stderr: ""
Jan 22 22:17:26.957: INFO: stdout: ""
Jan 22 22:17:28.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:29.029: INFO: stderr: ""
Jan 22 22:17:29.029: INFO: stdout: ""
Jan 22 22:17:31.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:31.098: INFO: stderr: ""
Jan 22 22:17:31.098: INFO: stdout: ""
Jan 22 22:17:33.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:33.170: INFO: stderr: ""
Jan 22 22:17:33.170: INFO: stdout: ""
Jan 22 22:17:35.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:35.262: INFO: stderr: ""
Jan 22 22:17:35.262: INFO: stdout: ""
Jan 22 22:17:37.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:37.332: INFO: stderr: ""
Jan 22 22:17:37.332: INFO: stdout: ""
Jan 22 22:17:39.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:39.403: INFO: stderr: ""
Jan 22 22:17:39.403: INFO: stdout: ""
Jan 22 22:17:41.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:41.480: INFO: stderr: ""
Jan 22 22:17:41.480: INFO: stdout: ""
Jan 22 22:17:43.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:43.552: INFO: stderr: ""
Jan 22 22:17:43.552: INFO: stdout: ""
Jan 22 22:17:45.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:45.619: INFO: stderr: ""
Jan 22 22:17:45.619: INFO: stdout: ""
Jan 22 22:17:47.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:47.693: INFO: stderr: ""
Jan 22 22:17:47.694: INFO: stdout: ""
Jan 22 22:17:49.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:49.774: INFO: stderr: ""
Jan 22 22:17:49.774: INFO: stdout: ""
Jan 22 22:17:51.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:51.852: INFO: stderr: ""
Jan 22 22:17:51.852: INFO: stdout: ""
Jan 22 22:17:53.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:53.933: INFO: stderr: ""
Jan 22 22:17:53.933: INFO: stdout: ""
Jan 22 22:17:55.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:56.013: INFO: stderr: ""
Jan 22 22:17:56.013: INFO: stdout: ""
Jan 22 22:17:58.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:17:58.094: INFO: stderr: ""
Jan 22 22:17:58.094: INFO: stdout: ""
Jan 22 22:18:00.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:00.177: INFO: stderr: ""
Jan 22 22:18:00.177: INFO: stdout: ""
Jan 22 22:18:02.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:02.254: INFO: stderr: ""
Jan 22 22:18:02.254: INFO: stdout: ""
Jan 22 22:18:04.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:04.336: INFO: stderr: ""
Jan 22 22:18:04.336: INFO: stdout: ""
Jan 22 22:18:06.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:06.413: INFO: stderr: ""
Jan 22 22:18:06.413: INFO: stdout: ""
Jan 22 22:18:08.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:08.493: INFO: stderr: ""
Jan 22 22:18:08.493: INFO: stdout: ""
Jan 22 22:18:10.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:10.570: INFO: stderr: ""
Jan 22 22:18:10.570: INFO: stdout: ""
Jan 22 22:18:12.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:12.653: INFO: stderr: ""
Jan 22 22:18:12.653: INFO: stdout: ""
Jan 22 22:18:14.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:14.732: INFO: stderr: ""
Jan 22 22:18:14.732: INFO: stdout: ""
Jan 22 22:18:16.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:16.810: INFO: stderr: ""
Jan 22 22:18:16.810: INFO: stdout: ""
Jan 22 22:18:18.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:18.887: INFO: stderr: ""
Jan 22 22:18:18.887: INFO: stdout: ""
Jan 22 22:18:20.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:20.970: INFO: stderr: ""
Jan 22 22:18:20.970: INFO: stdout: ""
Jan 22 22:18:22.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:23.047: INFO: stderr: ""
Jan 22 22:18:23.047: INFO: stdout: ""
Jan 22 22:18:25.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:25.132: INFO: stderr: ""
Jan 22 22:18:25.132: INFO: stdout: ""
Jan 22 22:18:27.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:27.209: INFO: stderr: ""
Jan 22 22:18:27.209: INFO: stdout: ""
Jan 22 22:18:29.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:29.294: INFO: stderr: ""
Jan 22 22:18:29.294: INFO: stdout: ""
Jan 22 22:18:31.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:31.371: INFO: stderr: ""
Jan 22 22:18:31.371: INFO: stdout: ""
Jan 22 22:18:33.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:33.451: INFO: stderr: ""
Jan 22 22:18:33.451: INFO: stdout: ""
Jan 22 22:18:35.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:35.542: INFO: stderr: ""
Jan 22 22:18:35.542: INFO: stdout: ""
Jan 22 22:18:37.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:37.615: INFO: stderr: ""
Jan 22 22:18:37.615: INFO: stdout: ""
Jan 22 22:18:39.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:39.690: INFO: stderr: ""
Jan 22 22:18:39.690: INFO: stdout: ""
Jan 22 22:18:41.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:41.772: INFO: stderr: ""
Jan 22 22:18:41.772: INFO: stdout: ""
Jan 22 22:18:43.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:43.845: INFO: stderr: ""
Jan 22 22:18:43.845: INFO: stdout: ""
Jan 22 22:18:45.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:45.926: INFO: stderr: ""
Jan 22 22:18:45.926: INFO: stdout: ""
Jan 22 22:18:47.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:48.005: INFO: stderr: ""
Jan 22 22:18:48.005: INFO: stdout: ""
Jan 22 22:18:50.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:50.083: INFO: stderr: ""
Jan 22 22:18:50.083: INFO: stdout: ""
Jan 22 22:18:52.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:52.161: INFO: stderr: ""
Jan 22 22:18:52.161: INFO: stdout: ""
Jan 22 22:18:54.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:54.246: INFO: stderr: ""
Jan 22 22:18:54.246: INFO: stdout: ""
Jan 22 22:18:56.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:56.326: INFO: stderr: ""
Jan 22 22:18:56.326: INFO: stdout: ""
Jan 22 22:18:58.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:18:58.406: INFO: stderr: ""
Jan 22 22:18:58.406: INFO: stdout: ""
Jan 22 22:19:00.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:00.497: INFO: stderr: ""
Jan 22 22:19:00.497: INFO: stdout: ""
Jan 22 22:19:02.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:02.575: INFO: stderr: ""
Jan 22 22:19:02.575: INFO: stdout: ""
Jan 22 22:19:04.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:04.651: INFO: stderr: ""
Jan 22 22:19:04.651: INFO: stdout: ""
Jan 22 22:19:06.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:06.734: INFO: stderr: ""
Jan 22 22:19:06.734: INFO: stdout: ""
Jan 22 22:19:08.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:08.814: INFO: stderr: ""
Jan 22 22:19:08.814: INFO: stdout: ""
Jan 22 22:19:10.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:10.888: INFO: stderr: ""
Jan 22 22:19:10.888: INFO: stdout: ""
Jan 22 22:19:12.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:12.967: INFO: stderr: ""
Jan 22 22:19:12.967: INFO: stdout: ""
Jan 22 22:19:14.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:15.047: INFO: stderr: ""
Jan 22 22:19:15.047: INFO: stdout: ""
Jan 22 22:19:17.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:17.143: INFO: stderr: ""
Jan 22 22:19:17.143: INFO: stdout: ""
Jan 22 22:19:19.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:19.218: INFO: stderr: ""
Jan 22 22:19:19.218: INFO: stdout: ""
Jan 22 22:19:21.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:21.298: INFO: stderr: ""
Jan 22 22:19:21.298: INFO: stdout: ""
Jan 22 22:19:23.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:23.383: INFO: stderr: ""
Jan 22 22:19:23.383: INFO: stdout: ""
Jan 22 22:19:25.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:25.466: INFO: stderr: ""
Jan 22 22:19:25.466: INFO: stdout: ""
Jan 22 22:19:27.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:27.544: INFO: stderr: ""
Jan 22 22:19:27.544: INFO: stdout: ""
Jan 22 22:19:29.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:29.624: INFO: stderr: ""
Jan 22 22:19:29.624: INFO: stdout: ""
Jan 22 22:19:31.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:31.702: INFO: stderr: ""
Jan 22 22:19:31.702: INFO: stdout: ""
Jan 22 22:19:33.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:33.781: INFO: stderr: ""
Jan 22 22:19:33.781: INFO: stdout: ""
Jan 22 22:19:35.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:35.856: INFO: stderr: ""
Jan 22 22:19:35.856: INFO: stdout: ""
Jan 22 22:19:37.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:37.937: INFO: stderr: ""
Jan 22 22:19:37.937: INFO: stdout: ""
Jan 22 22:19:39.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:40.024: INFO: stderr: ""
Jan 22 22:19:40.024: INFO: stdout: ""
Jan 22 22:19:42.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:42.108: INFO: stderr: ""
Jan 22 22:19:42.108: INFO: stdout: ""
Jan 22 22:19:44.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:44.197: INFO: stderr: ""
Jan 22 22:19:44.197: INFO: stdout: ""
Jan 22 22:19:46.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:46.275: INFO: stderr: ""
Jan 22 22:19:46.275: INFO: stdout: ""
Jan 22 22:19:48.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:48.364: INFO: stderr: ""
Jan 22 22:19:48.364: INFO: stdout: ""
Jan 22 22:19:50.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:50.447: INFO: stderr: ""
Jan 22 22:19:50.447: INFO: stdout: ""
Jan 22 22:19:52.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:52.525: INFO: stderr: ""
Jan 22 22:19:52.525: INFO: stdout: ""
Jan 22 22:19:54.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:54.602: INFO: stderr: ""
Jan 22 22:19:54.602: INFO: stdout: ""
Jan 22 22:19:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:56.683: INFO: stderr: ""
Jan 22 22:19:56.683: INFO: stdout: ""
Jan 22 22:19:58.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:19:58.764: INFO: stderr: ""
Jan 22 22:19:58.764: INFO: stdout: ""
Jan 22 22:20:00.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:00.840: INFO: stderr: ""
Jan 22 22:20:00.840: INFO: stdout: ""
Jan 22 22:20:02.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:02.922: INFO: stderr: ""
Jan 22 22:20:02.922: INFO: stdout: ""
Jan 22 22:20:04.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:04.999: INFO: stderr: ""
Jan 22 22:20:04.999: INFO: stdout: ""
Jan 22 22:20:07.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:07.076: INFO: stderr: ""
Jan 22 22:20:07.076: INFO: stdout: ""
Jan 22 22:20:09.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:09.155: INFO: stderr: ""
Jan 22 22:20:09.155: INFO: stdout: ""
Jan 22 22:20:11.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:11.232: INFO: stderr: ""
Jan 22 22:20:11.232: INFO: stdout: ""
Jan 22 22:20:13.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:13.317: INFO: stderr: ""
Jan 22 22:20:13.317: INFO: stdout: ""
Jan 22 22:20:15.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:15.394: INFO: stderr: ""
Jan 22 22:20:15.394: INFO: stdout: ""
Jan 22 22:20:17.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:17.475: INFO: stderr: ""
Jan 22 22:20:17.475: INFO: stdout: ""
Jan 22 22:20:19.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:19.558: INFO: stderr: ""
Jan 22 22:20:19.558: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Jan 22 22:20:21.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:21.632: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 22:20:21.632: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jan 22 22:20:21.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-265w4'
Jan 22 22:20:21.728: INFO: stderr: "No resources found.\n"
Jan 22 22:20:21.728: INFO: stdout: ""
Jan 22 22:20:21.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -l name=nginx --namespace=e2e-tests-kubectl-265w4 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 22 22:20:21.798: INFO: stderr: ""
Jan 22 22:20:21.798: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-kubectl-265w4".
STEP: Found 6 events.
Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:18 +0000 UTC - event for redis-master: {replication-controller } SuccessfulCreate: Created pod: redis-master-crr8l
Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:18 +0000 UTC - event for redis-master-crr8l: {default-scheduler } Scheduled: Successfully assigned e2e-tests-kubectl-265w4/redis-master-crr8l to secconf-node
Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:19 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/redis:1.0" already present on machine
Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:19 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Created: Created container
Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:19 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Started: Started container
Jan 22 22:20:21.802: INFO: At 2019-01-22 22:20:21 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Killing: Killing container with id docker://redis-master:Need to kill Pod
Jan 22 22:20:21.810: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:20:21.810: INFO: redis-master-crr8l                                       secconf-node    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:18 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:20:21.810: INFO: 
Jan 22 22:20:21.814: INFO: 
Logging node info for node secconf-master
Jan 22 22:20:21.816: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:43803,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:20:14 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:20:14 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:20:14 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:20:14 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:20:21.816: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:20:21.820: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:20:21.827: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:20:21.827: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:20:21.827: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:20:21.827: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:20:21.827: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:20:21.827: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:20:21.827: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:20:21.827: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:20:21.827: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:20:21.827: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:20:21.827: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:20:21.827: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:20:21.827: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:20:21.827: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:20:21.827: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:20:21.827: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:20:21.847: INFO: 
Latency metrics for node secconf-master
Jan 22 22:20:21.847: INFO: 
Logging node info for node secconf-node
Jan 22 22:20:21.850: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:43811,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:20:21.850: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:20:21.853: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:20:21.861: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:20:21.861: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:20:21.861: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:20:21.861: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:20:21.861: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:20:21.861: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:20:21.861: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:20:21.861: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:20:21.861: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:20:21.861: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:20:21.861: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:20:21.861: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:20:21.861: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:20:21.861: INFO: redis-master-crr8l started at 2019-01-22 22:15:18 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:20:21.861: INFO: 	Container redis-master ready: true, restart count 0
Jan 22 22:20:21.884: INFO: 
Latency metrics for node secconf-node
Jan 22 22:20:21.884: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:20.360926s}
Jan 22 22:20:21.884: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:20.360926s}
Jan 22 22:20:21.884: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:12.879634s}
Jan 22 22:20:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-265w4" for this suite.
Jan 22 22:20:43.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:20:43.942: INFO: namespace: e2e-tests-kubectl-265w4, resource: bindings, ignored listing per whitelist
Jan 22 22:20:43.972: INFO: namespace e2e-tests-kubectl-265w4 deletion completed in 22.083792335s

• Failure [325.796 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance] [It]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

    Expected error:
        <*errors.errorString | 0xc002b82340>: {
            s: "Failed to find \"The server is now ready to accept connections\", last result: \"\"",
        }
        Failed to find "The server is now ready to accept connections", last result: ""
    not to have occurred

    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1167
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:20:43.972: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:20:44.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-448cj" to be "success or failure"
Jan 22 22:20:44.038: INFO: Pod "downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672876ms
Jan 22 22:20:46.044: INFO: Pod "downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008857479s
STEP: Saw pod success
Jan 22 22:20:46.044: INFO: Pod "downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:20:46.048: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:20:46.068: INFO: Waiting for pod downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:20:46.074: INFO: Pod downwardapi-volume-f58eaae1-1e93-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:20:46.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-448cj" for this suite.
Jan 22 22:20:52.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:20:52.102: INFO: namespace: e2e-tests-downward-api-448cj, resource: bindings, ignored listing per whitelist
Jan 22 22:20:52.162: INFO: namespace e2e-tests-downward-api-448cj deletion completed in 6.084571958s

• [SLOW TEST:8.190 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:20:52.162: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 22 22:20:52.223: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:20:55.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-ng72f" for this suite.
Jan 22 22:21:01.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:21:01.442: INFO: namespace: e2e-tests-init-container-ng72f, resource: bindings, ignored listing per whitelist
Jan 22 22:21:01.454: INFO: namespace e2e-tests-init-container-ng72f deletion completed in 6.087394024s

• [SLOW TEST:9.292 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:21:01.454: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 22 22:21:05.546: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 22 22:21:05.551: INFO: Pod pod-with-prestop-http-hook still exists
Jan 22 22:21:07.553: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 22 22:21:07.557: INFO: Pod pod-with-prestop-http-hook still exists
Jan 22 22:21:09.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 22 22:21:09.557: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:21:09.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-fms56" for this suite.
Jan 22 22:21:31.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:21:31.647: INFO: namespace: e2e-tests-container-lifecycle-hook-fms56, resource: bindings, ignored listing per whitelist
Jan 22 22:21:31.651: INFO: namespace e2e-tests-container-lifecycle-hook-fms56 deletion completed in 22.083928196s

• [SLOW TEST:30.197 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:21:31.652: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 22 22:21:33.742: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-11fbbd43-1e94-11e9-9284-e2fd7d97dccb,GenerateName:,Namespace:e2e-tests-events-h2kmf,SelfLink:/api/v1/namespaces/e2e-tests-events-h2kmf/pods/send-events-11fbbd43-1e94-11e9-9284-e2fd7d97dccb,UID:11fc72bf-1e94-11e9-945c-000c293afc9e,ResourceVersion:44043,Generation:0,CreationTimestamp:2019-01-22 22:21:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 721457886,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.100.1.117/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-scjs2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-scjs2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-scjs2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:secconf-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a96ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a96b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:21:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:21:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:21:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:21:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.43.101,PodIP:100.100.1.117,StartTime:2019-01-22 22:21:31 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-01-22 22:21:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://89b31f74656cb54b7a0d04fde6654027dbc330c2d66a1947e0143fc03930723f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jan 22 22:21:35.748: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 22 22:21:37.752: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:21:37.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-h2kmf" for this suite.
Jan 22 22:22:15.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:22:15.825: INFO: namespace: e2e-tests-events-h2kmf, resource: bindings, ignored listing per whitelist
Jan 22 22:22:15.848: INFO: namespace e2e-tests-events-h2kmf deletion completed in 38.08679561s

• [SLOW TEST:44.197 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:22:15.849: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 22 22:22:21.940: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:21.940: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.015: INFO: Exec stderr: ""
Jan 22 22:22:22.015: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.015: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.097: INFO: Exec stderr: ""
Jan 22 22:22:22.097: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.097: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.174: INFO: Exec stderr: ""
Jan 22 22:22:22.174: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.174: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.240: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 22 22:22:22.240: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.240: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.321: INFO: Exec stderr: ""
Jan 22 22:22:22.321: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.321: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.400: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 22 22:22:22.400: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.400: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.477: INFO: Exec stderr: ""
Jan 22 22:22:22.477: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.477: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.556: INFO: Exec stderr: ""
Jan 22 22:22:22.556: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.556: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.635: INFO: Exec stderr: ""
Jan 22 22:22:22.635: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-gpc9x PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:22:22.635: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:22:22.718: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:22:22.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-gpc9x" for this suite.
Jan 22 22:23:02.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:23:02.791: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-gpc9x, resource: bindings, ignored listing per whitelist
Jan 22 22:23:02.806: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-gpc9x deletion completed in 40.084349522s

• [SLOW TEST:46.957 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:23:02.806: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-484e07b9-1e94-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:23:02.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-5tg99" to be "success or failure"
Jan 22 22:23:02.870: INFO: Pod "pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.340105ms
Jan 22 22:23:04.874: INFO: Pod "pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006906105s
STEP: Saw pod success
Jan 22 22:23:04.874: INFO: Pod "pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:23:04.877: INFO: Trying to get logs from node secconf-node pod pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 22:23:04.894: INFO: Waiting for pod pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:23:04.897: INFO: Pod pod-configmaps-484e976c-1e94-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:23:04.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5tg99" for this suite.
Jan 22 22:23:10.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:23:10.976: INFO: namespace: e2e-tests-configmap-5tg99, resource: bindings, ignored listing per whitelist
Jan 22 22:23:10.982: INFO: namespace e2e-tests-configmap-5tg99 deletion completed in 6.081750712s

• [SLOW TEST:8.176 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:23:10.982: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 22 22:23:11.042: INFO: Waiting up to 5m0s for pod "downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-hfjjs" to be "success or failure"
Jan 22 22:23:11.045: INFO: Pod "downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380709ms
Jan 22 22:23:13.048: INFO: Pod "downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006009781s
STEP: Saw pod success
Jan 22 22:23:13.048: INFO: Pod "downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:23:13.050: INFO: Trying to get logs from node secconf-node pod downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 22:23:13.064: INFO: Waiting for pod downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:23:13.067: INFO: Pod downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:23:13.067: INFO: Unexpected error occurred: expected "POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb" in container output: Expected
    <string>: 
to match regular expression
    <string>: POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-downward-api-hfjjs".
STEP: Found 4 events.
Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-hfjjs/downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "docker.io/library/busybox:1.29" already present on machine
Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:23:13.084: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:23:13.084: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:23:13.084: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:23:13.084: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:23:13.085: INFO: 
Jan 22 22:23:13.088: INFO: 
Logging node info for node secconf-master
Jan 22 22:23:13.090: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:44251,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:23:04 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:23:04 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:23:04 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:23:04 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:23:13.090: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:23:13.093: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:23:13.098: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:23:13.098: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:23:13.098: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:23:13.098: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:23:13.098: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:23:13.098: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:23:13.098: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:23:13.098: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:23:13.098: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:23:13.098: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:23:13.098: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:23:13.098: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:23:13.098: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:23:13.098: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:23:13.098: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:23:13.098: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:23:13.113: INFO: 
Latency metrics for node secconf-master
Jan 22 22:23:13.113: INFO: 
Logging node info for node secconf-node
Jan 22 22:23:13.115: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:44281,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:23:13.116: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:23:13.118: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:23:13.123: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:23:13.123: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:23:13.123: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:23:13.123: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:23:13.123: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:23:13.123: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:23:13.123: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:23:13.123: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:23:13.123: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:23:13.123: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:23:13.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:23:13.123: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:23:13.123: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:23:13.143: INFO: 
Latency metrics for node secconf-node
Jan 22 22:23:13.143: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.133487s}
Jan 22 22:23:13.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hfjjs" for this suite.
Jan 22 22:23:19.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:23:19.172: INFO: namespace: e2e-tests-downward-api-hfjjs, resource: bindings, ignored listing per whitelist
Jan 22 22:23:19.227: INFO: namespace e2e-tests-downward-api-hfjjs deletion completed in 6.081350071s

• Failure [8.245 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc001864e00>: {
          s: "expected \"POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb\" in container output: Expected\n    <string>: \nto match regular expression\n    <string>: POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb",
      }
      expected "POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb" in container output: Expected
          <string>: 
      to match regular expression
          <string>: POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:23:19.227: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-dq2nd
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-dq2nd
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-dq2nd
Jan 22 22:23:19.299: INFO: Found 0 stateful pods, waiting for 1
Jan 22 22:23:29.303: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 22 22:23:29.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 22:23:29.454: INFO: stderr: ""
Jan 22 22:23:29.454: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 22:23:29.454: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 22:23:29.457: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 22 22:23:39.463: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 22:23:39.463: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 22:23:39.476: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:23:39.476: INFO: ss-0  secconf-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:19 +0000 UTC  }]
Jan 22 22:23:39.476: INFO: 
Jan 22 22:23:39.476: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 22 22:23:40.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99642025s
Jan 22 22:23:41.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992185988s
Jan 22 22:23:42.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986037693s
Jan 22 22:23:43.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981852485s
Jan 22 22:23:44.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977169386s
Jan 22 22:23:45.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972449645s
Jan 22 22:23:46.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968120755s
Jan 22 22:23:47.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963796198s
Jan 22 22:23:48.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.425938ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-dq2nd
Jan 22 22:23:49.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:23:49.663: INFO: stderr: ""
Jan 22 22:23:49.663: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 22:23:49.663: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 22:23:49.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:23:49.814: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 22 22:23:49.814: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 22:23:49.814: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 22:23:49.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:23:49.965: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 22 22:23:49.965: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 22:23:49.965: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 22:23:49.968: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 22 22:23:59.973: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:23:59.973: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:23:59.973: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 22 22:23:59.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 22:24:00.129: INFO: stderr: ""
Jan 22 22:24:00.129: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 22:24:00.129: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 22:24:00.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 22:24:00.276: INFO: stderr: ""
Jan 22 22:24:00.276: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 22:24:00.276: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 22:24:00.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 22:24:00.404: INFO: stderr: ""
Jan 22 22:24:00.404: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 22:24:00.404: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 22:24:00.405: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 22:24:00.407: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 22 22:24:10.414: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 22:24:10.414: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 22:24:10.414: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 22 22:24:10.425: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:10.425: INFO: ss-0  secconf-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:19 +0000 UTC  }]
Jan 22 22:24:10.425: INFO: ss-1  secconf-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:10.425: INFO: ss-2  secconf-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:10.425: INFO: 
Jan 22 22:24:10.425: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 22 22:24:11.430: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:11.430: INFO: ss-0  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:19 +0000 UTC  }]
Jan 22 22:24:11.430: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:11.430: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:11.430: INFO: 
Jan 22 22:24:11.430: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 22 22:24:12.434: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:12.434: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:12.434: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:12.434: INFO: 
Jan 22 22:24:12.434: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:13.442: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:13.442: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:13.442: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:13.442: INFO: 
Jan 22 22:24:13.442: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:14.447: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:14.447: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:14.447: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:14.447: INFO: 
Jan 22 22:24:14.447: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:15.452: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:15.452: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:15.452: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:15.452: INFO: 
Jan 22 22:24:15.452: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:16.457: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:16.458: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:16.458: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:16.458: INFO: 
Jan 22 22:24:16.458: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:17.462: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:17.462: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:17.462: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:17.462: INFO: 
Jan 22 22:24:17.462: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:18.466: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:18.466: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:18.466: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:18.466: INFO: 
Jan 22 22:24:18.466: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 22 22:24:19.471: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 22 22:24:19.471: INFO: ss-1  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:19.471: INFO: ss-2  secconf-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:24:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:23:39 +0000 UTC  }]
Jan 22 22:24:19.471: INFO: 
Jan 22 22:24:19.471: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-dq2nd
Jan 22 22:24:20.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:24:20.572: INFO: rc: 1
Jan 22 22:24:20.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00093fbc0 exit status 1 <nil> <nil> true [0xc00116e428 0xc00116e468 0xc00116e480] [0xc00116e428 0xc00116e468 0xc00116e480] [0xc00116e460 0xc00116e478] [0x92f8e0 0x92f8e0] 0xc001d6bc20 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jan 22 22:24:30.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:24:30.637: INFO: rc: 1
Jan 22 22:24:30.637: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001e69e30 exit status 1 <nil> <nil> true [0xc001e08250 0xc001e08268 0xc001e08280] [0xc001e08250 0xc001e08268 0xc001e08280] [0xc001e08260 0xc001e08278] [0x92f8e0 0x92f8e0] 0xc00255dda0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:24:40.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:24:40.700: INFO: rc: 1
Jan 22 22:24:40.700: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001596030 exit status 1 <nil> <nil> true [0xc00116e488 0xc00116e4a0 0xc00116e4e8] [0xc00116e488 0xc00116e4a0 0xc00116e4e8] [0xc00116e498 0xc00116e4c8] [0x92f8e0 0x92f8e0] 0xc002b7c000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:24:50.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:24:50.766: INFO: rc: 1
Jan 22 22:24:50.766: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0015963c0 exit status 1 <nil> <nil> true [0xc00116e4f0 0xc00116e540 0xc00116e580] [0xc00116e4f0 0xc00116e540 0xc00116e580] [0xc00116e518 0xc00116e568] [0x92f8e0 0x92f8e0] 0xc002b7c480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:25:00.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:25:00.831: INFO: rc: 1
Jan 22 22:25:00.831: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0017fd4a0 exit status 1 <nil> <nil> true [0xc001b24778 0xc001b24790 0xc001b247a8] [0xc001b24778 0xc001b24790 0xc001b247a8] [0xc001b24788 0xc001b247a0] [0x92f8e0 0x92f8e0] 0xc001cdfbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:25:10.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:25:10.899: INFO: rc: 1
Jan 22 22:25:10.899: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0015968d0 exit status 1 <nil> <nil> true [0xc00116e588 0xc00116e5e0 0xc00116e608] [0xc00116e588 0xc00116e5e0 0xc00116e608] [0xc00116e5c0 0xc00116e600] [0x92f8e0 0x92f8e0] 0xc002b7c840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:25:20.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:25:20.961: INFO: rc: 1
Jan 22 22:25:20.961: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0016ca390 exit status 1 <nil> <nil> true [0xc001616020 0xc001616068 0xc0016160a8] [0xc001616020 0xc001616068 0xc0016160a8] [0xc001616048 0xc001616098] [0x92f8e0 0x92f8e0] 0xc001250e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:25:30.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:25:31.029: INFO: rc: 1
Jan 22 22:25:31.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0016ca720 exit status 1 <nil> <nil> true [0xc0016160c0 0xc0016160f8 0xc001616150] [0xc0016160c0 0xc0016160f8 0xc001616150] [0xc0016160e8 0xc001616128] [0x92f8e0 0x92f8e0] 0xc001612180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:25:41.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:25:41.096: INFO: rc: 1
Jan 22 22:25:41.096: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe8390 exit status 1 <nil> <nil> true [0xc0020b8000 0xc0020b8030 0xc0020b8048] [0xc0020b8000 0xc0020b8030 0xc0020b8048] [0xc0020b8028 0xc0020b8040] [0x92f8e0 0x92f8e0] 0xc001a282a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:25:51.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:25:51.166: INFO: rc: 1
Jan 22 22:25:51.166: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe87b0 exit status 1 <nil> <nil> true [0xc0020b8050 0xc0020b8090 0xc0020b80b8] [0xc0020b8050 0xc0020b8090 0xc0020b80b8] [0xc0020b8088 0xc0020b80b0] [0x92f8e0 0x92f8e0] 0xc001a28660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:26:01.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:26:01.240: INFO: rc: 1
Jan 22 22:26:01.240: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe8bd0 exit status 1 <nil> <nil> true [0xc0020b80c0 0xc0020b80d8 0xc0020b80f0] [0xc0020b80c0 0xc0020b80d8 0xc0020b80f0] [0xc0020b80d0 0xc0020b80e8] [0x92f8e0 0x92f8e0] 0xc001a28a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:26:11.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:26:11.306: INFO: rc: 1
Jan 22 22:26:11.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022be480 exit status 1 <nil> <nil> true [0xc00000f250 0xc00000f2e8 0xc00000f3c8] [0xc00000f250 0xc00000f2e8 0xc00000f3c8] [0xc00000f2b8 0xc00000f380] [0x92f8e0 0x92f8e0] 0xc0020ec300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:26:21.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:26:21.370: INFO: rc: 1
Jan 22 22:26:21.370: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001468450 exit status 1 <nil> <nil> true [0xc000354258 0xc0003545d0 0xc000354d38] [0xc000354258 0xc0003545d0 0xc000354d38] [0xc0003545a8 0xc000354af0] [0x92f8e0 0x92f8e0] 0xc0022924e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:26:31.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:26:31.433: INFO: rc: 1
Jan 22 22:26:31.433: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe8fc0 exit status 1 <nil> <nil> true [0xc0020b80f8 0xc0020b8110 0xc0020b8128] [0xc0020b80f8 0xc0020b8110 0xc0020b8128] [0xc0020b8108 0xc0020b8120] [0x92f8e0 0x92f8e0] 0xc001a28fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:26:41.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:26:41.501: INFO: rc: 1
Jan 22 22:26:41.501: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001468810 exit status 1 <nil> <nil> true [0xc000354d58 0xc000354f28 0xc000355020] [0xc000354d58 0xc000354f28 0xc000355020] [0xc000354eb8 0xc000354fb0] [0x92f8e0 0x92f8e0] 0xc0022929c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:26:51.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:26:51.566: INFO: rc: 1
Jan 22 22:26:51.566: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022beb10 exit status 1 <nil> <nil> true [0xc00000f418 0xc00000f4b8 0xc00000f508] [0xc00000f418 0xc00000f4b8 0xc00000f508] [0xc00000f4a0 0xc00000f500] [0x92f8e0 0x92f8e0] 0xc0020ec6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:27:01.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:27:01.643: INFO: rc: 1
Jan 22 22:27:01.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe93b0 exit status 1 <nil> <nil> true [0xc0020b8130 0xc0020b8148 0xc0020b8160] [0xc0020b8130 0xc0020b8148 0xc0020b8160] [0xc0020b8140 0xc0020b8158] [0x92f8e0 0x92f8e0] 0xc001a29620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:27:11.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:27:11.731: INFO: rc: 1
Jan 22 22:27:11.731: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001468c00 exit status 1 <nil> <nil> true [0xc000355058 0xc000355140 0xc000355200] [0xc000355058 0xc000355140 0xc000355200] [0xc000355118 0xc0003551f8] [0x92f8e0 0x92f8e0] 0xc002292de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:27:21.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:27:21.798: INFO: rc: 1
Jan 22 22:27:21.798: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe83c0 exit status 1 <nil> <nil> true [0xc0020b8020 0xc0020b8038 0xc0020b8050] [0xc0020b8020 0xc0020b8038 0xc0020b8050] [0xc0020b8030 0xc0020b8048] [0x92f8e0 0x92f8e0] 0xc001250e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:27:31.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:27:31.862: INFO: rc: 1
Jan 22 22:27:31.862: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022be3f0 exit status 1 <nil> <nil> true [0xc00000f250 0xc00000f2e8 0xc00000f3c8] [0xc00000f250 0xc00000f2e8 0xc00000f3c8] [0xc00000f2b8 0xc00000f380] [0x92f8e0 0x92f8e0] 0xc001a282a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:27:41.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:27:41.929: INFO: rc: 1
Jan 22 22:27:41.929: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe8840 exit status 1 <nil> <nil> true [0xc0020b8068 0xc0020b80a8 0xc0020b80c0] [0xc0020b8068 0xc0020b80a8 0xc0020b80c0] [0xc0020b8090 0xc0020b80b8] [0x92f8e0 0x92f8e0] 0xc0020ec180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:27:51.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:27:51.994: INFO: rc: 1
Jan 22 22:27:51.994: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022beab0 exit status 1 <nil> <nil> true [0xc00000f418 0xc00000f4b8 0xc00000f508] [0xc00000f418 0xc00000f4b8 0xc00000f508] [0xc00000f4a0 0xc00000f500] [0x92f8e0 0x92f8e0] 0xc001a28660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:28:01.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:28:02.063: INFO: rc: 1
Jan 22 22:28:02.063: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001468420 exit status 1 <nil> <nil> true [0xc000354258 0xc0003545d0 0xc000354d38] [0xc000354258 0xc0003545d0 0xc000354d38] [0xc0003545a8 0xc000354af0] [0x92f8e0 0x92f8e0] 0xc0022924e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:28:12.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:28:12.135: INFO: rc: 1
Jan 22 22:28:12.135: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe8cc0 exit status 1 <nil> <nil> true [0xc0020b80c8 0xc0020b80e0 0xc0020b80f8] [0xc0020b80c8 0xc0020b80e0 0xc0020b80f8] [0xc0020b80d8 0xc0020b80f0] [0x92f8e0 0x92f8e0] 0xc0020ec540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:28:22.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:28:22.197: INFO: rc: 1
Jan 22 22:28:22.197: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022beea0 exit status 1 <nil> <nil> true [0xc00000f528 0xc00000f548 0xc00000f5d0] [0xc00000f528 0xc00000f548 0xc00000f5d0] [0xc00000f540 0xc00000f5b8] [0x92f8e0 0x92f8e0] 0xc001a28a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:28:32.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:28:32.269: INFO: rc: 1
Jan 22 22:28:32.269: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001468870 exit status 1 <nil> <nil> true [0xc000354d58 0xc000354f28 0xc000355020] [0xc000354d58 0xc000354f28 0xc000355020] [0xc000354eb8 0xc000354fb0] [0x92f8e0 0x92f8e0] 0xc0022929c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:28:42.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:28:42.331: INFO: rc: 1
Jan 22 22:28:42.331: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022bf290 exit status 1 <nil> <nil> true [0xc00000f5f0 0xc00000f640 0xc00000f688] [0xc00000f5f0 0xc00000f640 0xc00000f688] [0xc00000f620 0xc00000f670] [0x92f8e0 0x92f8e0] 0xc001a28fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:28:52.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:28:52.396: INFO: rc: 1
Jan 22 22:28:52.396: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001468c90 exit status 1 <nil> <nil> true [0xc000355058 0xc000355140 0xc000355200] [0xc000355058 0xc000355140 0xc000355200] [0xc000355118 0xc0003551f8] [0x92f8e0 0x92f8e0] 0xc002292de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:29:02.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:29:02.467: INFO: rc: 1
Jan 22 22:29:02.467: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0022bf650 exit status 1 <nil> <nil> true [0xc00000f6a8 0xc00000f6d8 0xc00000f790] [0xc00000f6a8 0xc00000f6d8 0xc00000f790] [0xc00000f6c8 0xc00000f778] [0x92f8e0 0x92f8e0] 0xc001a29620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:29:12.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:29:12.538: INFO: rc: 1
Jan 22 22:29:12.538: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014692c0 exit status 1 <nil> <nil> true [0xc000355210 0xc000355318 0xc0003553e8] [0xc000355210 0xc000355318 0xc0003553e8] [0xc0003552b0 0xc0003553a0] [0x92f8e0 0x92f8e0] 0xc0022930e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 22 22:29:22.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-dq2nd ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:29:22.609: INFO: rc: 1
Jan 22 22:29:22.609: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Jan 22 22:29:22.609: INFO: Scaling statefulset ss to 0
Jan 22 22:29:22.617: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 22 22:29:22.619: INFO: Deleting all statefulset in ns e2e-tests-statefulset-dq2nd
Jan 22 22:29:22.621: INFO: Scaling statefulset ss to 0
Jan 22 22:29:22.628: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 22:29:22.630: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:29:22.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-dq2nd" for this suite.
Jan 22 22:29:28.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:29:28.710: INFO: namespace: e2e-tests-statefulset-dq2nd, resource: bindings, ignored listing per whitelist
Jan 22 22:29:28.730: INFO: namespace e2e-tests-statefulset-dq2nd deletion completed in 6.086339145s

• [SLOW TEST:369.502 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:29:28.730: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:29:28.785: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:29:30.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5zr4q" for this suite.
Jan 22 22:30:14.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:30:14.939: INFO: namespace: e2e-tests-pods-5zr4q, resource: bindings, ignored listing per whitelist
Jan 22 22:30:14.979: INFO: namespace e2e-tests-pods-5zr4q deletion completed in 44.084089411s

• [SLOW TEST:46.249 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:30:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:30:17.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-b6t5h" for this suite.
Jan 22 22:31:07.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:31:07.090: INFO: namespace: e2e-tests-kubelet-test-b6t5h, resource: bindings, ignored listing per whitelist
Jan 22 22:31:07.159: INFO: namespace e2e-tests-kubelet-test-b6t5h deletion completed in 50.09001341s

• [SLOW TEST:52.180 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:31:07.159: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb
Jan 22 22:31:07.220: INFO: Pod name my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb: Found 0 pods out of 1
Jan 22 22:31:12.224: INFO: Pod name my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb: Found 1 pods out of 1
Jan 22 22:31:12.224: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb" are running
Jan 22 22:31:12.227: INFO: Pod "my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb-tm9zb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:31:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:31:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:31:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-22 22:31:07 +0000 UTC Reason: Message:}])
Jan 22 22:31:12.227: INFO: Trying to dial the pod
Jan 22 22:31:17.243: INFO: Controller my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb: Got expected result from replica 1 [my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb-tm9zb]: "my-hostname-basic-69010944-1e95-11e9-9284-e2fd7d97dccb-tm9zb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:31:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-w5fpg" for this suite.
Jan 22 22:31:23.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:31:23.291: INFO: namespace: e2e-tests-replication-controller-w5fpg, resource: bindings, ignored listing per whitelist
Jan 22 22:31:23.336: INFO: namespace e2e-tests-replication-controller-w5fpg deletion completed in 6.088533552s

• [SLOW TEST:16.176 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:31:23.336: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:31:23.401: INFO: (0) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.238671ms)
Jan 22 22:31:23.405: INFO: (1) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.910461ms)
Jan 22 22:31:23.409: INFO: (2) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.00701ms)
Jan 22 22:31:23.412: INFO: (3) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.446243ms)
Jan 22 22:31:23.416: INFO: (4) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.829834ms)
Jan 22 22:31:23.420: INFO: (5) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.382094ms)
Jan 22 22:31:23.424: INFO: (6) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.909726ms)
Jan 22 22:31:23.427: INFO: (7) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.425848ms)
Jan 22 22:31:23.430: INFO: (8) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.131599ms)
Jan 22 22:31:23.433: INFO: (9) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.974862ms)
Jan 22 22:31:23.437: INFO: (10) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.338821ms)
Jan 22 22:31:23.440: INFO: (11) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.195151ms)
Jan 22 22:31:23.443: INFO: (12) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.064103ms)
Jan 22 22:31:23.446: INFO: (13) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.075804ms)
Jan 22 22:31:23.449: INFO: (14) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.992787ms)
Jan 22 22:31:23.452: INFO: (15) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.932397ms)
Jan 22 22:31:23.455: INFO: (16) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.992642ms)
Jan 22 22:31:23.458: INFO: (17) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.939918ms)
Jan 22 22:31:23.461: INFO: (18) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.25254ms)
Jan 22 22:31:23.464: INFO: (19) /api/v1/nodes/secconf-node/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.755886ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:31:23.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-n2pcc" for this suite.
Jan 22 22:31:29.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:31:29.525: INFO: namespace: e2e-tests-proxy-n2pcc, resource: bindings, ignored listing per whitelist
Jan 22 22:31:29.558: INFO: namespace e2e-tests-proxy-n2pcc deletion completed in 6.091126684s

• [SLOW TEST:6.222 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:31:29.558: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-765aa60e-1e95-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:31:29.621: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-txthf" to be "success or failure"
Jan 22 22:31:29.631: INFO: Pod "pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.084877ms
Jan 22 22:31:31.637: INFO: Pod "pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015402381s
STEP: Saw pod success
Jan 22 22:31:31.637: INFO: Pod "pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:31:31.640: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 22:31:31.658: INFO: Waiting for pod pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:31:31.661: INFO: Pod pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:31:31.661: INFO: Unexpected error occurred: expected "content of file \"/etc/projected-configmap-volume/data-1\": value-1" in container output: Expected
    <string>: 
to contain substring
    <string>: content of file "/etc/projected-configmap-volume/data-1": value-1
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-txthf".
STEP: Found 4 events.
Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:29 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-txthf/pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:30 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:30 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:30 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:31:31.675: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:31:31.675: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:31:31.675: INFO: 
Jan 22 22:31:31.678: INFO: 
Logging node info for node secconf-master
Jan 22 22:31:31.680: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:45196,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:31:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:31:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:31:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:31:25 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:31:31.680: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:31:31.682: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:31:31.688: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:31:31.688: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:31:31.688: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:31:31.688: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:31:31.688: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:31:31.688: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:31:31.688: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:31:31.688: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:31:31.688: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:31:31.688: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:31:31.688: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:31:31.688: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:31:31.688: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:31:31.688: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:31:31.688: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:31:31.688: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:31:31.702: INFO: 
Latency metrics for node secconf-master
Jan 22 22:31:31.702: INFO: 
Logging node info for node secconf-node
Jan 22 22:31:31.705: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:45184,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:31:31.705: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:31:31.708: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:31:31.713: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:31:31.713: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:31:31.713: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:31:31.713: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:31:31.713: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:31:31.713: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:31:31.713: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:31:31.713: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:31:31.713: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:31:31.713: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:31:31.713: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:31:31.713: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:31:31.713: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:31:31.731: INFO: 
Latency metrics for node secconf-node
Jan 22 22:31:31.731: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.133487s}
Jan 22 22:31:31.731: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.9 Latency:30.071826s}
Jan 22 22:31:31.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-txthf" for this suite.
Jan 22 22:31:37.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:31:37.796: INFO: namespace: e2e-tests-projected-txthf, resource: bindings, ignored listing per whitelist
Jan 22 22:31:37.814: INFO: namespace e2e-tests-projected-txthf deletion completed in 6.081053389s

• Failure [8.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc001408930>: {
          s: "expected \"content of file \\\"/etc/projected-configmap-volume/data-1\\\": value-1\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: content of file \"/etc/projected-configmap-volume/data-1\": value-1",
      }
      expected "content of file \"/etc/projected-configmap-volume/data-1\": value-1" in container output: Expected
          <string>: 
      to contain substring
          <string>: content of file "/etc/projected-configmap-volume/data-1": value-1
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:31:37.815: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 22 22:31:37.879: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 22 22:31:37.885: INFO: Waiting for terminating namespaces to be deleted...
Jan 22 22:31:37.887: INFO: 
Logging pods the kubelet thinks is on node secconf-node before test
Jan 22 22:31:37.892: INFO: calico-node-6j2nx from kube-system started at 2019-01-21 09:37:56 +0000 UTC (2 container statuses recorded)
Jan 22 22:31:37.892: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:31:37.892: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:31:37.892: INFO: kube-proxy-6m8wc from kube-system started at 2019-01-21 09:37:56 +0000 UTC (1 container statuses recorded)
Jan 22 22:31:37.892: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:31:37.892: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn from heptio-sonobuoy started at 2019-01-22 21:07:14 +0000 UTC (2 container statuses recorded)
Jan 22 22:31:37.892: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:31:37.892: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:31:37.892: INFO: sonobuoy-e2e-job-98d427a1d3c0481f from heptio-sonobuoy started at 2019-01-22 21:07:14 +0000 UTC (2 container statuses recorded)
Jan 22 22:31:37.892: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:31:37.893: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:31:37.893: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (1 container statuses recorded)
Jan 22 22:31:37.893: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.157c4c548b5deae1], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:31:38.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-z7hh8" for this suite.
Jan 22 22:31:44.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:31:44.959: INFO: namespace: e2e-tests-sched-pred-z7hh8, resource: bindings, ignored listing per whitelist
Jan 22 22:31:44.996: INFO: namespace e2e-tests-sched-pred-z7hh8 deletion completed in 6.083151337s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.181 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:31:44.996: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-67ghp
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 22 22:31:45.051: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 22 22:32:03.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.100.1.129:8080/dial?request=hostName&protocol=udp&host=100.100.1.128&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-67ghp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:32:03.102: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:32:03.202: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:32:03.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-67ghp" for this suite.
Jan 22 22:32:25.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:32:25.228: INFO: namespace: e2e-tests-pod-network-test-67ghp, resource: bindings, ignored listing per whitelist
Jan 22 22:32:25.282: INFO: namespace e2e-tests-pod-network-test-67ghp deletion completed in 22.078003638s

• [SLOW TEST:40.287 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:32:25.283: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:32:25.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-q6k58" to be "success or failure"
Jan 22 22:32:25.350: INFO: Pod "downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539522ms
Jan 22 22:32:27.354: INFO: Pod "downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008972037s
STEP: Saw pod success
Jan 22 22:32:27.355: INFO: Pod "downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:32:27.358: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:32:27.376: INFO: Waiting for pod downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:32:27.380: INFO: Pod downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:32:27.380: INFO: Unexpected error occurred: expected "[1-9]" in container output: Expected
    <string>: 
to match regular expression
    <string>: [1-9]
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-downward-api-q6k58".
STEP: Found 4 events.
Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:25 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-q6k58/downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:25 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:26 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:26 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:32:27.407: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:32:27.407: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:32:27.407: INFO: 
Jan 22 22:32:27.410: INFO: 
Logging node info for node secconf-master
Jan 22 22:32:27.413: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:45376,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:32:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:32:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:32:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:32:25 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:32:27.413: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:32:27.415: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:32:27.420: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:32:27.420: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:32:27.420: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:32:27.420: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:32:27.420: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:32:27.420: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:32:27.420: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:32:27.420: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:32:27.420: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:32:27.420: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:32:27.420: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:32:27.420: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:32:27.420: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:32:27.420: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:32:27.420: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:32:27.420: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:32:27.440: INFO: 
Latency metrics for node secconf-master
Jan 22 22:32:27.440: INFO: 
Logging node info for node secconf-node
Jan 22 22:32:27.443: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:45370,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:32:27.443: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:32:27.445: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:32:27.450: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:32:27.450: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:32:27.450: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:32:27.450: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:32:27.450: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:32:27.450: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:32:27.450: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:32:27.450: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:32:27.450: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:32:27.450: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:32:27.450: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:32:27.450: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:32:27.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:32:27.473: INFO: 
Latency metrics for node secconf-node
Jan 22 22:32:27.473: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.071826s}
Jan 22 22:32:27.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-q6k58" for this suite.
Jan 22 22:32:33.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:32:33.548: INFO: namespace: e2e-tests-downward-api-q6k58, resource: bindings, ignored listing per whitelist
Jan 22 22:32:33.562: INFO: namespace e2e-tests-downward-api-q6k58 deletion completed in 6.085722762s

• Failure [8.280 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc002de6a10>: {
          s: "expected \"[1-9]\" in container output: Expected\n    <string>: \nto match regular expression\n    <string>: [1-9]",
      }
      expected "[1-9]" in container output: Expected
          <string>: 
      to match regular expression
          <string>: [1-9]
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:32:33.563: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Jan 22 22:32:33.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 api-versions'
Jan 22 22:32:33.688: INFO: stderr: ""
Jan 22 22:32:33.688: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:32:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n7fhx" for this suite.
Jan 22 22:32:39.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:32:39.750: INFO: namespace: e2e-tests-kubectl-n7fhx, resource: bindings, ignored listing per whitelist
Jan 22 22:32:39.772: INFO: namespace e2e-tests-kubectl-n7fhx deletion completed in 6.080801033s

• [SLOW TEST:6.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:32:39.772: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-a035adc2-1e95-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:32:39.841: INFO: Waiting up to 5m0s for pod "pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-2xh7h" to be "success or failure"
Jan 22 22:32:39.844: INFO: Pod "pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.575863ms
Jan 22 22:32:41.847: INFO: Pod "pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005943733s
STEP: Saw pod success
Jan 22 22:32:41.848: INFO: Pod "pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:32:41.851: INFO: Trying to get logs from node secconf-node pod pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb container secret-volume-test: <nil>
STEP: delete the pod
Jan 22 22:32:41.870: INFO: Waiting for pod pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:32:41.875: INFO: Pod pod-secrets-a03618fa-1e95-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:32:41.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2xh7h" for this suite.
Jan 22 22:32:47.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:32:47.931: INFO: namespace: e2e-tests-secrets-2xh7h, resource: bindings, ignored listing per whitelist
Jan 22 22:32:47.967: INFO: namespace e2e-tests-secrets-2xh7h deletion completed in 6.087659451s

• [SLOW TEST:8.195 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:32:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5xv8x
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Jan 22 22:32:48.044: INFO: Found 0 stateful pods, waiting for 3
Jan 22 22:32:58.049: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:32:58.050: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:32:58.050: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 22 22:32:58.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-5xv8x ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 22:32:58.206: INFO: stderr: ""
Jan 22 22:32:58.206: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 22:32:58.206: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 22 22:33:08.235: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 22 22:33:18.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-5xv8x ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:33:18.451: INFO: stderr: ""
Jan 22 22:33:18.451: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 22:33:18.451: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 22:33:28.469: INFO: Waiting for StatefulSet e2e-tests-statefulset-5xv8x/ss2 to complete update
Jan 22 22:33:28.469: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 22 22:33:28.469: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 22 22:33:28.469: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 22 22:33:38.476: INFO: Waiting for StatefulSet e2e-tests-statefulset-5xv8x/ss2 to complete update
Jan 22 22:33:38.476: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 22 22:33:38.476: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 22 22:33:48.475: INFO: Waiting for StatefulSet e2e-tests-statefulset-5xv8x/ss2 to complete update
Jan 22 22:33:48.475: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jan 22 22:33:58.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-5xv8x ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 22 22:33:58.623: INFO: stderr: ""
Jan 22 22:33:58.623: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 22 22:33:58.623: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 22 22:34:08.653: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 22 22:34:18.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 exec --namespace=e2e-tests-statefulset-5xv8x ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 22 22:34:18.820: INFO: stderr: ""
Jan 22 22:34:18.821: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 22 22:34:18.821: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 22 22:34:28.839: INFO: Waiting for StatefulSet e2e-tests-statefulset-5xv8x/ss2 to complete update
Jan 22 22:34:28.839: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jan 22 22:34:28.839: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Jan 22 22:34:28.839: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Jan 22 22:34:38.848: INFO: Waiting for StatefulSet e2e-tests-statefulset-5xv8x/ss2 to complete update
Jan 22 22:34:38.848: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jan 22 22:34:38.848: INFO: Waiting for Pod e2e-tests-statefulset-5xv8x/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 22 22:34:48.848: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5xv8x
Jan 22 22:34:48.852: INFO: Scaling statefulset ss2 to 0
Jan 22 22:34:58.875: INFO: Waiting for statefulset status.replicas updated to 0
Jan 22 22:34:58.878: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:34:58.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5xv8x" for this suite.
Jan 22 22:35:04.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:35:04.976: INFO: namespace: e2e-tests-statefulset-5xv8x, resource: bindings, ignored listing per whitelist
Jan 22 22:35:04.978: INFO: namespace e2e-tests-statefulset-5xv8x deletion completed in 6.084673456s

• [SLOW TEST:137.011 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:35:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f6c11db7-1e95-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:35:05.040: INFO: Waiting up to 5m0s for pod "pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-secrets-7qb9z" to be "success or failure"
Jan 22 22:35:05.043: INFO: Pod "pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30126ms
Jan 22 22:35:07.048: INFO: Pod "pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007531394s
STEP: Saw pod success
Jan 22 22:35:07.048: INFO: Pod "pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:35:07.052: INFO: Trying to get logs from node secconf-node pod pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb container secret-env-test: <nil>
STEP: delete the pod
Jan 22 22:35:07.077: INFO: Waiting for pod pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:35:07.080: INFO: Pod pod-secrets-f6c1a69a-1e95-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:35:07.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7qb9z" for this suite.
Jan 22 22:35:13.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:35:13.122: INFO: namespace: e2e-tests-secrets-7qb9z, resource: bindings, ignored listing per whitelist
Jan 22 22:35:13.166: INFO: namespace e2e-tests-secrets-7qb9z deletion completed in 6.081794189s

• [SLOW TEST:8.188 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:35:13.166: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:35:13.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 version --client'
Jan 22 22:35:13.289: INFO: stderr: ""
Jan 22 22:35:13.289: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jan 22 22:35:13.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-cjf94'
Jan 22 22:35:13.490: INFO: stderr: ""
Jan 22 22:35:13.490: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan 22 22:35:13.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-cjf94'
Jan 22 22:35:13.633: INFO: stderr: ""
Jan 22 22:35:13.633: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 22 22:35:14.637: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 22:35:14.637: INFO: Found 0 / 1
Jan 22 22:35:15.641: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 22:35:15.641: INFO: Found 1 / 1
Jan 22 22:35:15.641: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 22 22:35:15.644: INFO: Selector matched 1 pods for map[app:redis]
Jan 22 22:35:15.644: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 22 22:35:15.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 describe pod redis-master-w46cx --namespace=e2e-tests-kubectl-cjf94'
Jan 22 22:35:15.726: INFO: stderr: ""
Jan 22 22:35:15.726: INFO: stdout: "Name:               redis-master-w46cx\nNamespace:          e2e-tests-kubectl-cjf94\nPriority:           0\nPriorityClassName:  <none>\nNode:               secconf-node/192.168.43.101\nStart Time:         Tue, 22 Jan 2019 22:35:13 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.100.1.142/32\nStatus:             Running\nIP:                 100.100.1.142\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://53bc0613ba563a35e8539d34bab9a91ec79e1a0f4c5729463dcbbae0e7b83b12\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 22 Jan 2019 22:35:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8b4bp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8b4bp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8b4bp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                   Message\n  ----    ------     ----  ----                   -------\n  Normal  Scheduled  2s    default-scheduler      Successfully assigned e2e-tests-kubectl-cjf94/redis-master-w46cx to secconf-node\n  Normal  Pulled     1s    kubelet, secconf-node  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, secconf-node  Created container\n  Normal  Started    1s    kubelet, secconf-node  Started container\n"
Jan 22 22:35:15.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 describe rc redis-master --namespace=e2e-tests-kubectl-cjf94'
Jan 22 22:35:15.803: INFO: stderr: ""
Jan 22 22:35:15.804: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-cjf94\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-w46cx\n"
Jan 22 22:35:15.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 describe service redis-master --namespace=e2e-tests-kubectl-cjf94'
Jan 22 22:35:15.877: INFO: stderr: ""
Jan 22 22:35:15.878: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-cjf94\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.98.247.174\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.100.1.142:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 22 22:35:15.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 describe node secconf-master'
Jan 22 22:35:15.967: INFO: stderr: ""
Jan 22 22:35:15.967: INFO: stdout: "Name:               secconf-master\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=secconf-master\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.43.100/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 21 Jan 2019 09:31:48 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 22 Jan 2019 22:35:15 +0000   Mon, 21 Jan 2019 09:31:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 22 Jan 2019 22:35:15 +0000   Mon, 21 Jan 2019 09:31:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 22 Jan 2019 22:35:15 +0000   Mon, 21 Jan 2019 09:31:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 22 Jan 2019 22:35:15 +0000   Tue, 22 Jan 2019 16:13:28 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.43.100\n  Hostname:    secconf-master\nCapacity:\n cpu:                4\n ephemeral-storage:  17194Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1862636Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  16226294143\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1760236Ki\n pods:               110\nSystem Info:\n Machine ID:                 7861286c482a4015bfff3886763c7c6f\n System UUID:                C72F4D56-7176-4CF6-7186-C2689E3AFC9E\n Boot ID:                    834082ce-213e-42ff-ad2a-98f7c960b895\n Kernel Version:             3.10.0-957.1.3.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.13.2\n Kube-Proxy Version:         v1.13.2\nPodCIDR:                     100.100.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  kube-system                calico-node-cbdfd                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         36h\n  kube-system                coredns-86c58d9df4-9895s                                   100m (2%)     0 (0%)      70Mi (4%)        170Mi (9%)     37h\n  kube-system                coredns-86c58d9df4-kd6j9                                   100m (2%)     0 (0%)      70Mi (4%)        170Mi (9%)     37h\n  kube-system                etcd-secconf-master                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         37h\n  kube-system                kube-apiserver-secconf-master                              250m (6%)     0 (0%)      0 (0%)           0 (0%)         6h26m\n  kube-system                kube-controller-manager-secconf-master                     200m (5%)     0 (0%)      0 (0%)           0 (0%)         37h\n  kube-system                kube-proxy-mc5pl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         37h\n  kube-system                kube-scheduler-secconf-master                              100m (2%)     0 (0%)      0 (0%)           0 (0%)         37h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (25%)     0 (0%)\n  memory             140Mi (8%)  340Mi (19%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan 22 22:35:15.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 describe namespace e2e-tests-kubectl-cjf94'
Jan 22 22:35:16.047: INFO: stderr: ""
Jan 22 22:35:16.047: INFO: stdout: "Name:         e2e-tests-kubectl-cjf94\nLabels:       e2e-framework=kubectl\n              e2e-run=b2776d0b-1e89-11e9-9284-e2fd7d97dccb\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:35:16.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cjf94" for this suite.
Jan 22 22:35:38.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:35:38.128: INFO: namespace: e2e-tests-kubectl-cjf94, resource: bindings, ignored listing per whitelist
Jan 22 22:35:38.134: INFO: namespace e2e-tests-kubectl-cjf94 deletion completed in 22.083756279s

• [SLOW TEST:24.968 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:35:38.134: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 22 22:35:38.209: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Jan 22 22:35:38.214: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-c852m/daemonsets","resourceVersion":"46114"},"items":null}

Jan 22 22:35:38.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-c852m/pods","resourceVersion":"46114"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:35:38.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-c852m" for this suite.
Jan 22 22:35:44.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:35:44.301: INFO: namespace: e2e-tests-daemonsets-c852m, resource: bindings, ignored listing per whitelist
Jan 22 22:35:44.304: INFO: namespace e2e-tests-daemonsets-c852m deletion completed in 6.080920791s

S [SKIPPING] [6.170 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Jan 22 22:35:38.209: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:35:44.305: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Jan 22 22:35:44.370: INFO: Waiting up to 5m0s for pod "client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-containers-vg8t7" to be "success or failure"
Jan 22 22:35:44.376: INFO: Pod "client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.911341ms
Jan 22 22:35:46.380: INFO: Pod "client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009433252s
STEP: Saw pod success
Jan 22 22:35:46.380: INFO: Pod "client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:35:46.383: INFO: Trying to get logs from node secconf-node pod client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:35:46.403: INFO: Waiting for pod client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:35:46.408: INFO: Pod client-containers-0e32f141-1e96-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:35:46.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vg8t7" for this suite.
Jan 22 22:35:52.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:35:52.482: INFO: namespace: e2e-tests-containers-vg8t7, resource: bindings, ignored listing per whitelist
Jan 22 22:35:52.493: INFO: namespace e2e-tests-containers-vg8t7 deletion completed in 6.08134762s

• [SLOW TEST:8.189 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:35:52.493: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Jan 22 22:35:54.562: INFO: Pod pod-hostip-13128413-1e96-11e9-9284-e2fd7d97dccb has hostIP: 192.168.43.101
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:35:54.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-4rljn" for this suite.
Jan 22 22:36:16.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:36:16.620: INFO: namespace: e2e-tests-pods-4rljn, resource: bindings, ignored listing per whitelist
Jan 22 22:36:16.649: INFO: namespace e2e-tests-pods-4rljn deletion completed in 22.082760887s

• [SLOW TEST:24.156 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:36:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Jan 22 22:36:16.706: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-259822818 proxy --unix-socket=/tmp/kubectl-proxy-unix692665561/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:36:16.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zvrmm" for this suite.
Jan 22 22:36:22.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:36:22.796: INFO: namespace: e2e-tests-kubectl-zvrmm, resource: bindings, ignored listing per whitelist
Jan 22 22:36:22.832: INFO: namespace e2e-tests-kubectl-zvrmm deletion completed in 6.072593496s

• [SLOW TEST:6.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:36:22.832: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-cbw7
STEP: Creating a pod to test atomic-volume-subpath
Jan 22 22:36:22.899: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cbw7" in namespace "e2e-tests-subpath-vvf8j" to be "success or failure"
Jan 22 22:36:22.903: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201003ms
Jan 22 22:36:24.908: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008820171s
Jan 22 22:36:26.914: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 4.014568969s
Jan 22 22:36:28.918: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 6.01875124s
Jan 22 22:36:30.922: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 8.022943406s
Jan 22 22:36:32.928: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 10.028938329s
Jan 22 22:36:34.933: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 12.033465007s
Jan 22 22:36:36.937: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 14.038175044s
Jan 22 22:36:38.943: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 16.044019159s
Jan 22 22:36:40.947: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 18.047892305s
Jan 22 22:36:42.952: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 20.053037674s
Jan 22 22:36:44.958: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Running", Reason="", readiness=false. Elapsed: 22.05912014s
Jan 22 22:36:46.964: INFO: Pod "pod-subpath-test-secret-cbw7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064474173s
STEP: Saw pod success
Jan 22 22:36:46.964: INFO: Pod "pod-subpath-test-secret-cbw7" satisfied condition "success or failure"
Jan 22 22:36:46.967: INFO: Trying to get logs from node secconf-node pod pod-subpath-test-secret-cbw7 container test-container-subpath-secret-cbw7: <nil>
STEP: delete the pod
Jan 22 22:36:46.985: INFO: Waiting for pod pod-subpath-test-secret-cbw7 to disappear
Jan 22 22:36:46.988: INFO: Pod pod-subpath-test-secret-cbw7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-cbw7
Jan 22 22:36:46.988: INFO: Deleting pod "pod-subpath-test-secret-cbw7" in namespace "e2e-tests-subpath-vvf8j"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:36:46.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-vvf8j" for this suite.
Jan 22 22:36:53.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:36:53.032: INFO: namespace: e2e-tests-subpath-vvf8j, resource: bindings, ignored listing per whitelist
Jan 22 22:36:53.078: INFO: namespace e2e-tests-subpath-vvf8j deletion completed in 6.084133955s

• [SLOW TEST:30.246 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:36:53.079: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Jan 22 22:36:53.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 --namespace=e2e-tests-kubectl-cn69c run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 22 22:36:54.888: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 22 22:36:54.889: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:36:56.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cn69c" for this suite.
Jan 22 22:37:02.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:37:02.975: INFO: namespace: e2e-tests-kubectl-cn69c, resource: bindings, ignored listing per whitelist
Jan 22 22:37:02.982: INFO: namespace e2e-tests-kubectl-cn69c deletion completed in 6.080572549s

• [SLOW TEST:9.903 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:37:02.982: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-j64gg
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 22 22:37:03.080: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 22 22:37:21.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.100.1.147:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-j64gg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 22 22:37:21.132: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
Jan 22 22:37:21.227: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:37:21.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-j64gg" for this suite.
Jan 22 22:37:43.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:37:43.287: INFO: namespace: e2e-tests-pod-network-test-j64gg, resource: bindings, ignored listing per whitelist
Jan 22 22:37:43.313: INFO: namespace e2e-tests-pod-network-test-j64gg deletion completed in 22.082455595s

• [SLOW TEST:40.331 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:37:43.313: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 22 22:37:43.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:43.513: INFO: stderr: ""
Jan 22 22:37:43.513: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 22 22:37:43.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:43.607: INFO: stderr: ""
Jan 22 22:37:43.607: INFO: stdout: "update-demo-nautilus-4dwqm update-demo-nautilus-ddfdg "
Jan 22 22:37:43.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-4dwqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:43.672: INFO: stderr: ""
Jan 22 22:37:43.672: INFO: stdout: ""
Jan 22 22:37:43.672: INFO: update-demo-nautilus-4dwqm is created but not running
Jan 22 22:37:48.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:48.755: INFO: stderr: ""
Jan 22 22:37:48.755: INFO: stdout: "update-demo-nautilus-4dwqm update-demo-nautilus-ddfdg "
Jan 22 22:37:48.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-4dwqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:48.829: INFO: stderr: ""
Jan 22 22:37:48.829: INFO: stdout: "true"
Jan 22 22:37:48.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-4dwqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:48.893: INFO: stderr: ""
Jan 22 22:37:48.893: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 22:37:48.893: INFO: validating pod update-demo-nautilus-4dwqm
Jan 22 22:37:48.897: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 22:37:48.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 22:37:48.897: INFO: update-demo-nautilus-4dwqm is verified up and running
Jan 22 22:37:48.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-ddfdg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:48.960: INFO: stderr: ""
Jan 22 22:37:48.960: INFO: stdout: "true"
Jan 22 22:37:48.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods update-demo-nautilus-ddfdg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:49.030: INFO: stderr: ""
Jan 22 22:37:49.030: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 22 22:37:49.030: INFO: validating pod update-demo-nautilus-ddfdg
Jan 22 22:37:49.034: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 22 22:37:49.034: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 22 22:37:49.034: INFO: update-demo-nautilus-ddfdg is verified up and running
STEP: using delete to clean up resources
Jan 22 22:37:49.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:49.120: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 22 22:37:49.120: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 22 22:37:49.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-4pj5r'
Jan 22 22:37:49.212: INFO: stderr: "No resources found.\n"
Jan 22 22:37:49.213: INFO: stdout: ""
Jan 22 22:37:49.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -l name=update-demo --namespace=e2e-tests-kubectl-4pj5r -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 22 22:37:49.292: INFO: stderr: ""
Jan 22 22:37:49.292: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:37:49.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4pj5r" for this suite.
Jan 22 22:38:11.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:38:11.327: INFO: namespace: e2e-tests-kubectl-4pj5r, resource: bindings, ignored listing per whitelist
Jan 22 22:38:11.382: INFO: namespace e2e-tests-kubectl-4pj5r deletion completed in 22.087103601s

• [SLOW TEST:28.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:38:11.383: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 22:38:11.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-zsff5'
Jan 22 22:38:11.518: INFO: stderr: ""
Jan 22 22:38:11.518: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Jan 22 22:38:11.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-zsff5'
Jan 22 22:38:21.959: INFO: stderr: ""
Jan 22 22:38:21.959: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:38:21.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zsff5" for this suite.
Jan 22 22:38:27.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:38:28.024: INFO: namespace: e2e-tests-kubectl-zsff5, resource: bindings, ignored listing per whitelist
Jan 22 22:38:28.043: INFO: namespace e2e-tests-kubectl-zsff5 deletion completed in 6.079896925s

• [SLOW TEST:16.660 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:38:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:38:28.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-t5r9x" to be "success or failure"
Jan 22 22:38:28.111: INFO: Pod "downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.309068ms
Jan 22 22:38:30.116: INFO: Pod "downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014475719s
STEP: Saw pod success
Jan 22 22:38:30.117: INFO: Pod "downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:38:30.119: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:38:30.146: INFO: Waiting for pod downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:38:30.149: INFO: Pod downwardapi-volume-6fca3451-1e96-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:38:30.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t5r9x" for this suite.
Jan 22 22:38:36.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:38:36.191: INFO: namespace: e2e-tests-projected-t5r9x, resource: bindings, ignored listing per whitelist
Jan 22 22:38:36.237: INFO: namespace e2e-tests-projected-t5r9x deletion completed in 6.08345932s

• [SLOW TEST:8.194 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:38:36.237: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-74ac71d3-1e96-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:38:36.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-configmap-5h8w9" to be "success or failure"
Jan 22 22:38:36.301: INFO: Pod "pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621662ms
Jan 22 22:38:38.305: INFO: Pod "pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008416097s
STEP: Saw pod success
Jan 22 22:38:38.306: INFO: Pod "pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:38:38.309: INFO: Trying to get logs from node secconf-node pod pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 22:38:38.324: INFO: Waiting for pod pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:38:38.327: INFO: Pod pod-configmaps-74ace5de-1e96-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:38:38.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5h8w9" for this suite.
Jan 22 22:38:44.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:38:44.406: INFO: namespace: e2e-tests-configmap-5h8w9, resource: bindings, ignored listing per whitelist
Jan 22 22:38:44.418: INFO: namespace e2e-tests-configmap-5h8w9 deletion completed in 6.087654399s

• [SLOW TEST:8.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:38:44.418: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 22 22:38:44.479: INFO: Waiting up to 5m0s for pod "pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-emptydir-d4d47" to be "success or failure"
Jan 22 22:38:44.482: INFO: Pod "pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.518179ms
Jan 22 22:38:46.486: INFO: Pod "pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007395269s
STEP: Saw pod success
Jan 22 22:38:46.486: INFO: Pod "pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:38:46.489: INFO: Trying to get logs from node secconf-node pod pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb container test-container: <nil>
STEP: delete the pod
Jan 22 22:38:46.510: INFO: Waiting for pod pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:38:46.513: INFO: Pod pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:38:46.513: INFO: Unexpected error occurred: expected "perms of file \"/test-volume/test-file\": -rwxrwxrwx" in container output: Expected
    <string>: 
to contain substring
    <string>: perms of file "/test-volume/test-file": -rwxrwxrwx
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-emptydir-d4d47".
STEP: Found 4 events.
Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:44 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-emptydir-d4d47/pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:45 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0" already present on machine
Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:45 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:45 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:38:46.526: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:38:46.526: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:38:46.526: INFO: 
Jan 22 22:38:46.529: INFO: 
Logging node info for node secconf-master
Jan 22 22:38:46.531: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:46693,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:38:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:38:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:38:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:38:45 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:38:46.531: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:38:46.533: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:38:46.539: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:38:46.539: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:38:46.539: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:38:46.539: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:38:46.539: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:38:46.539: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:38:46.539: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:38:46.539: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:38:46.539: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:38:46.539: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:38:46.539: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:38:46.539: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:38:46.539: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:38:46.539: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:38:46.539: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:38:46.539: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:38:46.557: INFO: 
Latency metrics for node secconf-master
Jan 22 22:38:46.557: INFO: 
Logging node info for node secconf-node
Jan 22 22:38:46.560: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:46666,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:38:46.560: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:38:46.562: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:38:46.567: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:38:46.567: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:38:46.567: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:38:46.567: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:38:46.567: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:38:46.567: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:38:46.567: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:38:46.567: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:38:46.567: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:38:46.567: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:38:46.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:38:46.567: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:38:46.567: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:38:46.590: INFO: 
Latency metrics for node secconf-node
Jan 22 22:38:46.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-d4d47" for this suite.
Jan 22 22:38:52.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:38:52.658: INFO: namespace: e2e-tests-emptydir-d4d47, resource: bindings, ignored listing per whitelist
Jan 22 22:38:52.682: INFO: namespace e2e-tests-emptydir-d4d47 deletion completed in 6.08924746s

• Failure [8.263 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc001bddb70>: {
          s: "expected \"perms of file \\\"/test-volume/test-file\\\": -rwxrwxrwx\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: perms of file \"/test-volume/test-file\": -rwxrwxrwx",
      }
      expected "perms of file \"/test-volume/test-file\": -rwxrwxrwx" in container output: Expected
          <string>: 
      to contain substring
          <string>: perms of file "/test-volume/test-file": -rwxrwxrwx
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:38:52.682: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:38:56.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-2dhr9" for this suite.
Jan 22 22:39:02.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:39:02.810: INFO: namespace: e2e-tests-kubelet-test-2dhr9, resource: bindings, ignored listing per whitelist
Jan 22 22:39:02.837: INFO: namespace e2e-tests-kubelet-test-2dhr9 deletion completed in 6.079353732s

• [SLOW TEST:10.155 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:39:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 22 22:39:02.896: INFO: Waiting up to 5m0s for pod "downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-q4w78" to be "success or failure"
Jan 22 22:39:02.900: INFO: Pod "downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798274ms
Jan 22 22:39:04.906: INFO: Pod "downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009459746s
STEP: Saw pod success
Jan 22 22:39:04.906: INFO: Pod "downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:39:04.909: INFO: Trying to get logs from node secconf-node pod downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb container dapi-container: <nil>
STEP: delete the pod
Jan 22 22:39:04.929: INFO: Waiting for pod downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:39:04.932: INFO: Pod downward-api-8487ad93-1e96-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:39:04.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-q4w78" for this suite.
Jan 22 22:39:10.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:39:10.991: INFO: namespace: e2e-tests-downward-api-q4w78, resource: bindings, ignored listing per whitelist
Jan 22 22:39:11.019: INFO: namespace e2e-tests-downward-api-q4w78 deletion completed in 6.083761585s

• [SLOW TEST:8.183 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:39:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 22 22:39:11.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-r7vjd'
Jan 22 22:39:11.153: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 22 22:39:11.153: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 22 22:39:11.162: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-dvqq5]
Jan 22 22:39:11.163: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-dvqq5" in namespace "e2e-tests-kubectl-r7vjd" to be "running and ready"
Jan 22 22:39:11.166: INFO: Pod "e2e-test-nginx-rc-dvqq5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818693ms
Jan 22 22:39:13.170: INFO: Pod "e2e-test-nginx-rc-dvqq5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007036997s
Jan 22 22:39:13.170: INFO: Pod "e2e-test-nginx-rc-dvqq5" satisfied condition "running and ready"
Jan 22 22:39:13.170: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-dvqq5]
Jan 22 22:39:13.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-r7vjd'
Jan 22 22:39:13.257: INFO: stderr: ""
Jan 22 22:39:13.257: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Jan 22 22:39:13.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-r7vjd'
Jan 22 22:39:13.331: INFO: stderr: ""
Jan 22 22:39:13.331: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:39:13.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r7vjd" for this suite.
Jan 22 22:39:19.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:39:19.405: INFO: namespace: e2e-tests-kubectl-r7vjd, resource: bindings, ignored listing per whitelist
Jan 22 22:39:19.423: INFO: namespace e2e-tests-kubectl-r7vjd deletion completed in 6.08070762s

• [SLOW TEST:8.404 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:39:19.423: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-8e69d1c5-1e96-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume secrets
Jan 22 22:39:19.481: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-6rjsd" to be "success or failure"
Jan 22 22:39:19.486: INFO: Pod "pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243698ms
Jan 22 22:39:21.490: INFO: Pod "pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008470973s
STEP: Saw pod success
Jan 22 22:39:21.490: INFO: Pod "pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:39:21.493: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 22 22:39:21.523: INFO: Waiting for pod pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:39:21.525: INFO: Pod pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:39:21.525: INFO: Unexpected error occurred: expected "content of file \"/etc/projected-secret-volume/new-path-data-1\": value-1" in container output: Expected
    <string>: 
to contain substring
    <string>: content of file "/etc/projected-secret-volume/new-path-data-1": value-1
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-projected-6rjsd".
STEP: Found 4 events.
Jan 22 22:39:21.533: INFO: At 2019-01-22 22:39:19 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-6rjsd/pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:39:21.534: INFO: At 2019-01-22 22:39:20 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:39:21.534: INFO: At 2019-01-22 22:39:20 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:39:21.534: INFO: At 2019-01-22 22:39:20 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:39:21.541: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:39:21.541: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:39:21.541: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:39:21.542: INFO: 
Jan 22 22:39:21.544: INFO: 
Logging node info for node secconf-master
Jan 22 22:39:21.547: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:46814,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:39:15 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:39:15 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:39:15 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:39:15 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:39:21.547: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:39:21.551: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:39:21.557: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:39:21.557: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:39:21.557: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:39:21.557: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:39:21.557: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:39:21.557: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:39:21.557: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:39:21.557: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:39:21.557: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:39:21.557: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:39:21.557: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:39:21.557: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:39:21.557: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:39:21.557: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:39:21.557: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:39:21.557: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:39:21.571: INFO: 
Latency metrics for node secconf-master
Jan 22 22:39:21.571: INFO: 
Logging node info for node secconf-node
Jan 22 22:39:21.573: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:46806,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:39:21.574: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:39:21.576: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:39:21.581: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:39:21.581: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:39:21.581: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:39:21.581: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:39:21.581: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:39:21.581: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:39:21.581: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:39:21.581: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:39:21.581: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:39:21.581: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:39:21.581: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:39:21.581: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:39:21.581: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:39:21.596: INFO: 
Latency metrics for node secconf-node
Jan 22 22:39:21.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6rjsd" for this suite.
Jan 22 22:39:27.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:39:27.639: INFO: namespace: e2e-tests-projected-6rjsd, resource: bindings, ignored listing per whitelist
Jan 22 22:39:27.683: INFO: namespace e2e-tests-projected-6rjsd deletion completed in 6.083913801s

• Failure [8.260 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc00219b3a0>: {
          s: "expected \"content of file \\\"/etc/projected-secret-volume/new-path-data-1\\\": value-1\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: content of file \"/etc/projected-secret-volume/new-path-data-1\": value-1",
      }
      expected "content of file \"/etc/projected-secret-volume/new-path-data-1\": value-1" in container output: Expected
          <string>: 
      to contain substring
          <string>: content of file "/etc/projected-secret-volume/new-path-data-1": value-1
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:39:27.684: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-mkkzx
Jan 22 22:39:29.765: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-mkkzx
STEP: checking the pod's current state and verifying that restartCount is present
Jan 22 22:39:29.768: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:43:30.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mkkzx" for this suite.
Jan 22 22:43:36.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:43:36.372: INFO: namespace: e2e-tests-container-probe-mkkzx, resource: bindings, ignored listing per whitelist
Jan 22 22:43:36.425: INFO: namespace e2e-tests-container-probe-mkkzx deletion completed in 6.08579341s

• [SLOW TEST:248.742 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:43:36.425: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-279a2aaf-1e97-11e9-9284-e2fd7d97dccb
STEP: Creating a pod to test consume configMaps
Jan 22 22:43:36.490: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-projected-r269s" to be "success or failure"
Jan 22 22:43:36.493: INFO: Pod "pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646887ms
Jan 22 22:43:38.496: INFO: Pod "pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006630131s
STEP: Saw pod success
Jan 22 22:43:38.497: INFO: Pod "pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:43:38.499: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 22 22:43:38.517: INFO: Waiting for pod pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:43:38.527: INFO: Pod pod-projected-configmaps-279aa984-1e97-11e9-9284-e2fd7d97dccb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 22 22:43:38.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-r269s" for this suite.
Jan 22 22:43:44.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:43:44.575: INFO: namespace: e2e-tests-projected-r269s, resource: bindings, ignored listing per whitelist
Jan 22 22:43:44.606: INFO: namespace e2e-tests-projected-r269s deletion completed in 6.076541333s

• [SLOW TEST:8.181 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 22 22:43:44.607: INFO: >>> kubeConfig: /tmp/kubeconfig-259822818
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 22 22:43:44.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb" in namespace "e2e-tests-downward-api-cdlzq" to be "success or failure"
Jan 22 22:43:44.670: INFO: Pod "downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261109ms
Jan 22 22:43:46.673: INFO: Pod "downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005089966s
STEP: Saw pod success
Jan 22 22:43:46.673: INFO: Pod "downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb" satisfied condition "success or failure"
Jan 22 22:43:46.675: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb container client-container: <nil>
STEP: delete the pod
Jan 22 22:43:46.696: INFO: Waiting for pod downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb to disappear
Jan 22 22:43:46.700: INFO: Pod downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb no longer exists
Jan 22 22:43:46.700: INFO: Unexpected error occurred: expected "2\n" in container output: Expected
    <string>: 
to contain substring
    <string>: 2
    
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
STEP: Collecting events from namespace "e2e-tests-downward-api-cdlzq".
STEP: Found 4 events.
Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:44 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-cdlzq/downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb to secconf-node
Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:45 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image "gcr.io/kubernetes-e2e-test-images/mounttest:1.0" already present on machine
Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:45 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container
Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:45 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container
Jan 22 22:43:46.712: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS
Jan 22 22:43:46.713: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]
Jan 22 22:43:46.713: INFO: 
Jan 22 22:43:46.716: INFO: 
Logging node info for node secconf-master
Jan 22 22:43:46.718: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:47267,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule <nil>}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:43:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:43:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:43:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:43:45 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:43:46.718: INFO: 
Logging kubelet events for node secconf-master
Jan 22 22:43:46.721: INFO: 
Logging pods the kubelet thinks is on node secconf-master
Jan 22 22:43:46.726: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:43:46.726: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:43:46.726: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:43:46.726: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:43:46.726: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:43:46.726: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:43:46.726: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:43:46.726: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:43:46.726: INFO: 	Container coredns ready: true, restart count 2
Jan 22 22:43:46.726: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:43:46.726: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:43:46.726: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:43:46.726: INFO: etcd-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:43:46.726: INFO: kube-apiserver-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:43:46.726: INFO: kube-controller-manager-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:43:46.726: INFO: kube-scheduler-secconf-master started at <nil> (0+0 container statuses recorded)
Jan 22 22:43:46.745: INFO: 
Latency metrics for node secconf-master
Jan 22 22:43:46.745: INFO: 
Logging node info for node secconf-node
Jan 22 22:43:46.748: INFO: Node Info: &Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:47241,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {<nil>} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1907339264 0} {<nil>} 1862636Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {<nil>} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{1802481664 0} {<nil>} 1760236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}
Jan 22 22:43:46.748: INFO: 
Logging kubelet events for node secconf-node
Jan 22 22:43:46.750: INFO: 
Logging pods the kubelet thinks is on node secconf-node
Jan 22 22:43:46.755: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:43:46.755: INFO: 	Container e2e ready: true, restart count 0
Jan 22 22:43:46.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 22 22:43:46.755: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:43:46.755: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 22 22:43:46.755: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:43:46.755: INFO: 	Container calico-node ready: true, restart count 2
Jan 22 22:43:46.755: INFO: 	Container install-cni ready: true, restart count 2
Jan 22 22:43:46.755: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)
Jan 22 22:43:46.755: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 22 22:43:46.755: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)
Jan 22 22:43:46.755: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 22 22:43:46.755: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 22 22:43:46.775: INFO: 
Latency metrics for node secconf-node
Jan 22 22:43:46.775: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:2m3.027673s}
Jan 22 22:43:46.775: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:2m3.027673s}
Jan 22 22:43:46.775: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:2m3.027673s}
Jan 22 22:43:46.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cdlzq" for this suite.
Jan 22 22:43:52.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 22 22:43:52.850: INFO: namespace: e2e-tests-downward-api-cdlzq, resource: bindings, ignored listing per whitelist
Jan 22 22:43:52.864: INFO: namespace e2e-tests-downward-api-cdlzq deletion completed in 6.085444926s

• Failure [8.257 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Expected error:
      <*errors.errorString | 0xc0015ecfb0>: {
          s: "expected \"2\\n\" in container output: Expected\n    <string>: \nto contain substring\n    <string>: 2\n    ",
      }
      expected "2\n" in container output: Expected
          <string>: 
      to contain substring
          <string>: 2
          
  not to have occurred

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395
------------------------------
SSSSSSJan 22 22:43:52.864: INFO: Running AfterSuite actions on all nodes
Jan 22 22:43:52.864: INFO: Running AfterSuite actions on node 1
Jan 22 22:43:52.864: INFO: Skipping dumping logs from cluster


Summarizing 19 Failures:

[Fail] [sig-storage] Projected secret [It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] EmptyDir volumes [It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected secret [It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [k8s.io] Variable Expansion [It] should allow composing env vars into new env vars [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected configMap [It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-node] ConfigMap [It] should be consumable via environment variable [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected downwardAPI [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-node] Downward API [It] should provide host IP as an env var [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Downward API volume [It] should set mode on item file [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] EmptyDir volumes [It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected downwardAPI [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected downwardAPI [It] should set DefaultMode on files [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-cli] Kubectl client [k8s.io] Kubectl logs [It] should be able to retrieve and filter logs  [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1167

[Fail] [sig-node] Downward API [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected configMap [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Downward API volume [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] EmptyDir volumes [It] should support (non-root,0777,default) [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Projected secret [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

[Fail] [sig-storage] Downward API volume [It] should provide container's cpu limit [NodeConformance] [Conformance] 
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395

Ran 200 of 1946 Specs in 5795.671 seconds
FAIL! -- 181 Passed | 19 Failed | 0 Pending | 1746 Skipped --- FAIL: TestE2E (5795.82s)
FAIL

Ginkgo ran 1 suite in 1h36m36.448778986s
Test Suite Failed
