<?xml version="1.0" encoding="UTF-8"?>
  <testsuite tests="200" failures="19" time="5795.671066706">
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]" classname="Kubernetes e2e suite" time="50.846278885"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Initializers [Feature:Initializers] will be set to nil if a patch removes the last pending initializer" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 4 PVs and 2 PVCs: test write access [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] shouldn&#39;t scale down with underutilized nodes due to host port conflicts [Feature:ClusterAutoscalerScalability5]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a secret." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] host cleanup with volume mounts [sig-storage][HostCleanup][Flaky] Host cleanup after disrupting NFS volume [NFS] after stopping the nfs-server and deleting the (sleeping) client pod, the NFS mount and the pod&#39;s UID directory should be removed." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to switch between IG and NEG modes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted flexvolume volume expand [Slow] [Feature:ExpandInUsePersistentVolumes] should be resizable when mounted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.284287754"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Detaching volumes should not work when mount is in progress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.274061252"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota without scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s memory limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.232001829"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should support orphan deletion of custom resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with the same volume source on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.236915996"></testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota with scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Upgrade [Feature:Upgrade] node upgrade should maintain a functioning cluster [Feature:NodeUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:Performance] should allow starting 30 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] RuntimeClass [Feature:RuntimeClass] should reject a Pod requesting a non-existent RuntimeClass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="82.172442643"></testcase>
      <testcase name="[k8s.io] Downward API [Serial] [Disruptive] [NodeFeature:EphemeralStorage] Downward API tests for local ephemeral storage should provide default limits.ephemeral-storage from node allocatable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by switching off the network interface and ensure they function upon switch on" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a pod." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.227027847"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should create endpoints for unready pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should not detach and unmount PV when associated pvc with delete as reclaimPolicy is deleted when it is in use by the pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should be able to deny custom resource creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with pre-shared certificate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from API server." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message if TerminationMessagePath is set [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.236713394"></testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values and a VSAN datastore is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should resign the bootstrap tokens when the clusterInfo ConfigMap updated [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="44.242931452"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [StatefulSet] should come back up if node goes down [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should check kube-proxy urls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the token secret when the secret expired" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should prevent NodePort collisions" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume that cannot be mounted [Slow] should fail due to non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]" classname="Kubernetes e2e suite" time="84.16570437"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape metrics from annotated pods." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for services  [Conformance]" classname="Kubernetes e2e suite" time="40.538699907"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.239970753"></testcase>
      <testcase name="[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="28.290070536"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]" classname="Kubernetes e2e suite" time="77.763617736"></testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when node is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Restart [Disruptive] should restart all nodes and ensure all nodes and pods recover" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Random with 0 secrets, 0 configmaps and 0 daemons with quotas" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:Performance] should be able to handle 30 pods per node ReplicationController with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for LoadBalancer service with ESIPP on [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on PodSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Advanced Audit should audit API calls [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape metrics from annotated services." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl patch should add annotations for pods in rc  [Conformance]" classname="Kubernetes e2e suite" time="24.511261713"></testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod&#39;s predecessor fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceConversionWebhook [Feature:CustomResourceWebhookConversion] Should be able to convert a non homogeneous list of CRs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Flexvolumes should be mountable when non-attachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes Default StorageClass pods that use multiple volumes should be reschedulable [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should exceed active deadline" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.192914386"></testcase>
      <testcase name="[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv4]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target value from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] multicluster ingress should get instance group annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="100.610957484"></testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Scheduler should continue assigning pods to nodes across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that a vspehre volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [Feature:RunAsGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 4 containers and 1 PDs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging using Elasticsearch [Feature:Elasticsearch] should check that logs from containers are ingested into Elasticsearch" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]" classname="Kubernetes e2e suite" time="9.268581157"></testcase>
      <testcase name="[k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Secret should create a pod that reads a secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Object from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner External should let an external dynamic provisioner create and delete persistent volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should handle updates to ExternalTrafficPolicy field" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap Should fail non-optional pod creation due to configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (cpu, memory quota set) against a pod with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.215976509"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull non-existing image from gcr.io [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should replace jobs when ReplaceConcurrent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] regular resource usage tracking resource tracking for 100 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through kubectl proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.19809268"></testcase>
      <testcase name="[sig-storage] CSI Volumes CSI attach test using HostPath driver [Feature:CSIDriverRegistry] attachable volume needs VolumeAttachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Nodes [Disruptive] Resize [Slow] should be able to add nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 0 secrets, 0 configmaps, 2 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should work after restarting kube-proxy [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.273495927999999"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a volume subpath [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two External metrics from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]" classname="Kubernetes e2e suite" time="7.303590358"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that deleting a PVC before the pod does not cause pod deletion to fail on vsphere volume detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed and one node is broken [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] RuntimeClass [Feature:RuntimeClass] should recover when the RuntimeClass CRD is deleted [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets Should fail non-optional pod creation due to the key in the secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support inline execution and attach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should support multiple TLS certs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node ReplicationController with 0 secrets, 0 configmaps and 0 daemons with quotas" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that deleting the Namespace of a PVC and Pod causes the successful detach of vsphere volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should only target nodes with endpoints" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.304989635"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Stress with local volume provisioner [Serial] should use be able to process many pods and reuse local volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [Feature:ClusterSizeAutoscalingScaleUp] [Slow] Autoscaling [sig-autoscaling] Autoscaling a service from 1 pod and 3 nodes to 8 pods and &gt;=4 nodes takes less than 15 minutes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for git_repo [Serial] [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and there is another node pool that is not autoscaled [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]" classname="Kubernetes e2e suite" time="18.43498889"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="6.223319455"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Node Poweroff [Feature:vsphere] [Slow] [Disruptive] verify volume status after node power off" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Deployment.extensions with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support volume SELinux relabeling" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ClusterIP to ExternalName" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap Should fail non-optional pod creation due to the key in the configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="13.268373593"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with different parameters" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="29.350838138"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [StatefulSet] should not reschedule stateful pods if there is a network partition [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s cpu request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.217995768"></testcase>
      <testcase name="[sig-storage] Projected secret Should fail non-optional pod creation due to the key in the secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] HA-master [Feature:HAMaster] survive addition/removal replicas multizone workers [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]" classname="Kubernetes e2e suite" time="30.271025888"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should have cluster metrics [Feature:StackdriverMonitoring]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.227561202"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.189839841"></testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] Deployment Should scale from 1 pod to 3 pods and from 3 to 5" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]" classname="Kubernetes e2e suite" time="66.191130428"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="48.256285436"></testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver with Prometheus [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning on Datastore [Feature:vsphere] verify dynamically provisioned pv using storageclass fails on an invalid datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should delete old replica sets [Conformance]" classname="Kubernetes e2e suite" time="11.227112351"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should be able to switch between HTTPS and HTTP2 modes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Upgrade [Feature:Upgrade] master upgrade should maintain a functioning cluster [Feature:MasterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should be able to deny attaching pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]" classname="Kubernetes e2e suite" time="8.757334534"></testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="46.20255774"></testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv6][Experimental]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Etcd failure [Disruptive] should recover from SIGKILL" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a replication controller." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run CronJob should create a CronJob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vcp at scale [Feature:vsphere]  vsphere scale tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against 2 pods with different priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest events [Feature:StackdriverLogging]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an existing and compatible SPBM policy is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default arguments (docker cmd) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.188914614"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [Job] should create new pods when node is partitioned" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node ReplicationController with 0 secrets, 0 configmaps and 2 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.195607175"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume provisioner [Serial] should create and recreate local persistent volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should not be able to mutate or prevent deletion of webhook configuration objects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons with quotas" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should support configurable pod resolv.conf" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="28.728355432"></testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for new resource model [Feature:StackdriverCustomMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]" classname="Kubernetes e2e suite" time="22.728277485"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks should be able to delete a non-existent PD without error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 3 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a configMap." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working zookeeper cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should be able to deny pod and configmap creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by removing the default annotation [Serial] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s memory request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.296705375"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a replica set." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group up from 0[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]" classname="Kubernetes e2e suite" time="11.257164303"></testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the pod [Feature:Seccomp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] [Feature:GPUDevicePlugin] run Nvidia GPU Device Plugin tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner delayed binding with allowedTopologies [Slow] should create persistent volumes in the same zone as specified in allowedTopologies after a pod mounting the claims is started" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should support cascading deletion of custom resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] SSH should SSH to all nodes and run commands" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for pod-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 1 pod to 2 pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support pod.Spec.SecurityContext.SupplementalGroups" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for API chunking should support continue listing from the last key if the original version has been compacted away, though the list is inconsistent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Deployment.extensions with 2 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pod with node different from PV&#39;s NodeAffinity should fail scheduling due to different NodeSelector" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]" classname="Kubernetes e2e suite" time="22.877537715"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update nodePort: http [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]" classname="Kubernetes e2e suite" time="14.853559654"></testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support pod.Spec.SecurityContext.RunAsUser" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PV Protection Verify &#34;immediate&#34; deletion of a PV that is not bound to a PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] etcd Upgrade [Feature:EtcdUpgrade] etcd upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceDefinition Watch CustomResourceDefinition Watch watch on custom resource definition objects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:Initializers] should create a ResourceQuota and capture the life of an uninitialized pod." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with backend HTTPS" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]" classname="Kubernetes e2e suite" time="8.187766568"></testcase>
      <testcase name="[sig-network] NoSNAT [Feature:NoSNAT] [Slow] Should be able to send traffic between Pods without SNAT" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [Feature:Networking-IPv6] Forward external name lookup should forward externalname lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]" classname="Kubernetes e2e suite" time="12.253381032"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working CockroachDB cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should reject invalid sysctls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.207690193"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="26.201213626"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.184193498"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Deployment.extensions with 0 secrets, 2 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should deny crd creation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate custom resource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes spread across nodes when pod has anti-affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Initializers [Feature:Initializers] don&#39;t cause replicaset controller creating extra pods if the initializer is not handled [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]" classname="Kubernetes e2e suite" time="6.289572293"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl alpha client [k8s.io] Kubectl run CronJob should create a CronJob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support volume SELinux relabeling when using hostIPC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should be able to create a ClusterIP service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s multiple priority class scope (quota set to pod count: 2) against 2 pods with same priority classes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes CSI attach test using HostPath driver [Feature:CSIDriverRegistry] non-attachable volume does not need VolumeAttachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale up when non expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by dropping all outbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - eagerzeroedthick is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="40.194427825"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.185919542"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Logging soak [Performance] [Slow] [Disruptive] should survive logging 1KB every 1s seconds, for a duration of 2m0s" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]" classname="Kubernetes e2e suite" time="28.236686662"></testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should support allow-all policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment reaping should cascade to its replica sets and pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]" classname="Kubernetes e2e suite" time="8.278908118"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes spread across nodes when pod management is parallel and pod has anti-affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [Feature:VolumeSubpathEnvExpansion][NodeAlphaFeature:VolumeSubpathEnvExpansion][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with spbm policy on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify static provisioning on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] Deployment Should scale from 5 pods to 3 pods and from 3 to 1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0666,default) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.252843097"></testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.260998351"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates MaxPods limit number of pods that are allowed to run [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Kibana Logging Instances Is Alive [Feature:Elasticsearch] should check that the Kibana logging instance is alive" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.21279469"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: enough pods, absolute =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to pod anti-affinity [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against 2 pods with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl version should check is all data is printed  [Conformance]" classname="Kubernetes e2e suite" time="6.214376399"></testcase>
      <testcase name="[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="26.315701814"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] CustomResourceConversionWebhook [Feature:CustomResourceWebhookConversion] Should be able to convert from CR v1 to CR v2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Etcd failure [Disruptive] should recover from network partition with master" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]" classname="Kubernetes e2e suite" time="6.24422944"></testcase>
      <testcase name="[sig-storage] vsphere cloud provider stress [Feature:vsphere] vsphere stress tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [IPv4] Forward external name lookup should forward externalname lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ClusterDns [Feature:Example] should create pod that uses dns" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0644,default) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.220775842"></testcase>
      <testcase name="[sig-storage] Secrets Should fail non-optional pod creation due to secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale down underutilized nodes [Feature:ClusterAutoscalerScalability4]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when pod is evicted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for LoadBalancer service with ESIPP off [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with given static-ip" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale up at all [Feature:ClusterAutoscalerScalability1]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should provision storage with non-default reclaim policy Retain" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Stackdriver Metadata Agent [Feature:StackdriverMetadataAgent]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should use same NodePort with same port but different protocols" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.769656881"></testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with backend HTTPS" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes GlusterFS should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should serve a basic endpoint from pods  [Conformance]" classname="Kubernetes e2e suite" time="29.308517246"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpNotIn)." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]" classname="Kubernetes e2e suite" time="28.192292717"></testcase>
      <testcase name="[k8s.io] GKE local SSD [Feature:GKELocalSSD] should write and read from node local SSD [Feature:GKELocalSSD]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="156.488500746"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.231884007"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for service with type clusterIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should have session affinity work for LoadBalancer service with ESIPP off [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] CA ignores unschedulable pods while scheduling schedulable pods [Feature:ClusterAutoscalerScalability6]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Kubelet should not restart containers across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should fail to create secret in volume due to empty secret key" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]" classname="Kubernetes e2e suite" time="16.183533502"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]" classname="Kubernetes e2e suite" time="6.174803244"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="248.685746368"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ExternalName to NodePort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]" classname="Kubernetes e2e suite" time="38.300475832"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 0 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]" classname="Kubernetes e2e suite" time="46.270271324"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify an if a SPBM policy and VSAN capabilities cannot be honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Mount propagation should propagate mounts to the host" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl label should update the label on a resource  [Conformance]" classname="Kubernetes e2e suite" time="8.998100891"></testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner delayed binding [Slow] should create persistent volumes in the same zone as node after a pod mounting the claims is started" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s cpu request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.20295673"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]" classname="Kubernetes e2e suite" time="22.339789475"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should serve multiport endpoints from pods  [Conformance]" classname="Kubernetes e2e suite" time="13.288188966"></testcase>
      <testcase name="[k8s.io] Downward API [Serial] [Disruptive] [NodeFeature:EphemeralStorage] Downward API tests for local ephemeral storage should provide container&#39;s limits.ephemeral-storage and requests.ephemeral-storage as env vars" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver [Feature:StackdriverLogging] [Soak] should ingest logs from applications running for a prolonged amount of time" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: too few pods, absolute =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.206192891"></testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]" classname="Kubernetes e2e suite" time="25.893052598"></testcase>
      <testcase name="[sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that deleting the PV before the pod does not cause pod deletion to fail on vspehre volume detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] NFSPersistentVolumes[Disruptive][Flaky] when kube-controller-manager restarts should delete a bound PVC from a clientPod, restart the kube-control-manager, and ensure the kube-controller-manager does not crash" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should release no longer matching pods [Conformance]" classname="Kubernetes e2e suite" time="12.23060306"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should have accelerator metrics [Feature:StackdriverAcceleratorMonitoring]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="30.232772179"></testcase>
      <testcase name="[sig-storage] CSI Volumes CSI attach test using HostPath driver [Feature:CSIDriverRegistry] volume with no CSI driver needs VolumeAttachment" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run deployment should create a deployment from an image  [Conformance]" classname="Kubernetes e2e suite" time="26.387201474"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="42.170836986"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl create quota should reject quota with invalid scopes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should unconditionally reject operations on fail closed webhook" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work for type=LoadBalancer" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a container&#39;s args [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.206582041"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DaemonRestart [Disruptive] Controller Manager should not create/delete replicas across restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for external metrics [Feature:StackdriverExternalMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates GeneralPredicates is properly invalidated when a pod is scheduled [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] A node shouldn&#39;t be able to delete another node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with multiple volumes from different datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap Should fail non-optional pod creation due to configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for the cluster  [Conformance]" classname="Kubernetes e2e suite" time="10.239772788"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [Feature:RunAsGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should scale a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="25.884540597"></testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should eagerly create replacement pod during network partition when termination grace is non-zero" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.236476253"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale up correct target pool [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small and one node is broken [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should be disabled by changing the default annotation [Serial] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Job.batch with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should receive events on concurrent watches in same order" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support rollback" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with terminating scopes through scope selectors." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for endpoint-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should return command exit codes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a service." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.223216971"></testcase>
      <testcase name="[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down when rescheduling a pod is required and pdb allows for it[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.266581902"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] deletion should be idempotent" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:NodeLease][NodeAlphaFeature:NodeLease] when the NodeLease feature is enabled the kubelet should create and update a lease in the kube-node-lease namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting EmptyDir volume is pending [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return pod details" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Service endpoints latency should not be very high  [Conformance]" classname="Kubernetes e2e suite" time="32.87839543"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default command and arguments [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.217274643"></testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Guestbook application should create and stop a working application  [Conformance]" classname="Kubernetes e2e suite" time="107.024700076"></testcase>
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with default parameter on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should set mode on item file [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.185657489"></testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints for both Ingress-referenced NEG and standalone NEG" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost should support forwarding over websockets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume limits should verify that all nodes have volume limits" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should not reconcile manually modified health check for ingress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] experimental resource usage tracking [Feature:ExperimentalResourceUsageTracking] resource tracking for 100 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:HighDensityPerformance] should allow starting 95 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should add node to the particular mig [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]" classname="Kubernetes e2e suite" time="36.690477168"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should check NodePort out-of-range" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="46.192611006"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] vsphere statefulset vsphere statefulset testing" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support rollover [Conformance]" classname="Kubernetes e2e suite" time="27.247672496"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume expand [Slow] Verify if editing PVC allows resize" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run default should create an rc or deployment from an image  [Conformance]" classname="Kubernetes e2e suite" time="22.3209108"></testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl expose should create services for rc  [Conformance]" classname="Kubernetes e2e suite" time="27.56540141"></testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support container.SecurityContext.RunAsUser" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.209451829"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should allow ingress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pod garbage collector [Feature:PodGarbageCollector] [Slow] should handle the creation of 1000 pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow substituting values in a container&#39;s command [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.178887443"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="12.688434087"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] Pods should return to running and ready state after network partition is healed All pods on the unreachable node should be marked as NotReady upon the node turn NotReady AND all pods should be mark back to Ready when the node get back to Ready before pod eviction timeout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create volume metrics in Volume Manager" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] [Feature:PerformanceDNS][Serial] Should answer DNS query for maximum number of services per cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Flexvolumes should be mountable when attachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Attach Verify [Feature:vsphere][Serial][Disruptive] verify volume remains attached after master kubelet restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]" classname="Kubernetes e2e suite" time="13.196647591"></testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]" classname="Kubernetes e2e suite" time="30.240578541"></testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should create NEGs for all ports with the Ingress annotation, and NEGs for the standalone annotation otherwise" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should work after restarting apiserver [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should do a rolling update of a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="50.535522182"></testcase>
      <testcase name="[sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="28.699546528"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]" classname="Kubernetes e2e suite" time="65.064307661"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl replace should update a single-container pod&#39;s image  [Conformance]" classname="Kubernetes e2e suite" time="19.521298551"></testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.184029014"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.189362178"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Initializers [Feature:Initializers] should dynamically register and apply initializers to pods [Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s memory request [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.194967559"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.185623764"></testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s memory limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.179123306"></testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Kubelet [Serial] [Slow] [k8s.io] [sig-node] regular resource usage tracking resource tracking for 0 pods per node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.326195906">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc000b7e290&gt;: {&#xA;        s: &#34;expected \&#34;content of file \\\&#34;/etc/projected-secret-volume/new-path-data-1\\\&#34;: value-1\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: content of file \&#34;/etc/projected-secret-volume/new-path-data-1\&#34;: value-1&#34;,&#xA;    }&#xA;    expected &#34;content of file \&#34;/etc/projected-secret-volume/new-path-data-1\&#34;: value-1&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: content of file &#34;/etc/projected-secret-volume/new-path-data-1&#34;: value-1&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected secret&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 21:53:38.982: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating projection with secret that has name projected-secret-test-map-2cfbae7b-1e90-11e9-9284-e2fd7d97dccb&#xA;STEP: Creating a pod to test consume secrets&#xA;Jan 22 21:53:39.045: INFO: Waiting up to 5m0s for pod &#34;pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-5zhg7&#34; to be &#34;success or failure&#34;&#xA;Jan 22 21:53:39.049: INFO: Pod &#34;pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.178096ms&#xA;Jan 22 21:53:41.054: INFO: Pod &#34;pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.009222768s&#xA;STEP: Saw pod success&#xA;Jan 22 21:53:41.054: INFO: Pod &#34;pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 21:53:41.057: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 21:53:41.075: INFO: Waiting for pod pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 21:53:41.078: INFO: Pod pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 21:53:41.078: INFO: Unexpected error occurred: expected &#34;content of file \&#34;/etc/projected-secret-volume/new-path-data-1\&#34;: value-1&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: content of file &#34;/etc/projected-secret-volume/new-path-data-1&#34;: value-1&#xA;[AfterEach] [sig-storage] Projected secret&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-5zhg7&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-5zhg7/pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 21:53:41.083: INFO: At 2019-01-22 21:53:39 +0000 UTC - event for pod-projected-secrets-2cfce359-1e90-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 21:53:41.094: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 21:53:41.094: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 21:53:41.094: INFO: &#xA;Jan 22 21:53:41.098: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 21:53:41.100: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:38343,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 21:53:32 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 21:53:32 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 21:53:32 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 21:53:32 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 21:53:41.101: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 21:53:41.104: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 21:53:41.154: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 21:53:41.154: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 21:53:41.154: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 21:53:41.154: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 21:53:41.154: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 21:53:41.154: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 21:53:41.188: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 21:53:41.188: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 21:53:41.191: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:38372,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 21:53:39 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 21:53:41.191: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 21:53:41.193: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 21:53:41.198: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 21:53:41.198: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 21:53:41.199: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 21:53:41.199: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 21:53:41.199: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 21:53:41.199: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 21:53:41.199: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 21:53:41.225: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 21:53:41.225: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.07586s}&#xA;Jan 22 21:53:41.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-5zhg7&#34; for this suite.&#xA;Jan 22 21:53:47.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 21:53:47.291: INFO: namespace: e2e-tests-projected-5zhg7, resource: bindings, ignored listing per whitelist&#xA;Jan 22 21:53:47.308: INFO: namespace e2e-tests-projected-5zhg7 deletion completed in 6.079748227s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Downgrade [Feature:Downgrade] cluster downgrade should maintain a functioning cluster [Feature:ClusterDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update endpoints: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with the same volume source attach/detach to different worker nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the signed bootstrap tokens from clusterInfo ConfigMap when bootstrap token is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]" classname="Kubernetes e2e suite" time="30.25775667"></testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]" classname="Kubernetes e2e suite" time="6.233283555"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with multiple volumes from same datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: no PDB =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="26.221040869"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid hostFailuresToTolerate and cacheReservation values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 should support forwarding over websockets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should allow egress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should schedule multiple jobs concurrently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up when expendable pod is preempted [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicationController Should scale from 5 pods to 3 pods and from 3 to 1 and verify decision stability" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for endpoint-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update endpoints: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not have port 10255 open on its all public IP addresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]" classname="Kubernetes e2e suite" time="16.188115003"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] crictl should be able to run crictl on the node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap Should fail non-optional pod creation due to the key in the configMap object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing secret should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by dropping all inbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to up and down services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking IPerf IPv4 [Experimental] [Feature:Networking-IPv4] [Slow] [Feature:Networking-Performance] should transfer ~ 1GB onto the service endpoint 1 servers (maximum of 1 clients)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to cadvisor port 4194 using proxy subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale down empty nodes [Feature:ClusterAutoscalerScalability3]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a SPBM policy is not honored on a non-compatible datastore for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="46.208800432"></testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] A node shouldn&#39;t be able to create another node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should not provision a volume in an unmanaged GCE zone." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage =&gt; should allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify if a non-existing SPBM policy is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Size [Feature:vsphere] verify dynamically provisioned pv using storageclass with an invalid disk size fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should correctly scale down after a node is not needed when there is non autoscaled pool[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should create a PodDisruptionBudget" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.185198617"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed  [Flaky]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a exec &#34;cat /tmp/health&#34; liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="62.306171438"></testcase>
      <testcase name="[sig-storage] vcp-performance [Feature:vsphere] vcp performance tests" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] gpu Upgrade [Feature:GPUUpgrade] cluster downgrade should be able to run gpu pod after downgrade [Feature:GPUClusterDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="26.229865623"></testcase>
      <testcase name="[sig-network] Firewall rule [Slow] [Serial] should create valid firewall rules for LoadBalancer type service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should verify ResourceQuota with terminating scopes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]" classname="Kubernetes e2e suite" time="12.232748754"></testcase>
      <testcase name="[sig-network] Loadbalancing: L7 [Slow] Nginx should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should not delete the token secret when the secret is not expired" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should sign the new added bootstrap tokens" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="106.624691593"></testcase>
      <testcase name="[sig-storage] Mounted flexvolume expand[Slow] Should verify mounted flex volumes can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 Scalability GCE [Slow] [Serial] [Feature:IngressScale] Creating and updating ingresses should happen promptly with small/medium/large amount of ingresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should contain correct container CPU metric." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]" classname="Kubernetes e2e suite" time="18.640758055"></testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working redis cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment iterative rollouts should eventually progress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for ExternalName services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type and ports of a service [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]" classname="Kubernetes e2e suite" time="7.179679753"></testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]" classname="Kubernetes e2e suite" time="54.280780584"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] ServiceAccounts should ensure a single API token exists" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]" classname="Kubernetes e2e suite" time="89.996633916"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:NodeLease][NodeAlphaFeature:NodeLease][Disruptive] NodeLease deletion node lease should be deleted when corresponding node is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] rolling update backend pods should not cause service disruption" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - zeroedthick is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pending pods are small [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-service-catalog] [Feature:PodPreset] PodPreset should not modify the pod on conflict" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicaSet Should scale from 1 pod to 3 pods and from 3 to 5" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 2 pods to 1 pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message from log output if TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should give a volume the correct mode [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.180911367"></testcase>
      <testcase name="[k8s.io] Pods should cap back-off at MaxContainerBackOff [Slow][NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-service-catalog] [Feature:PodPreset] PodPreset should create a pod preset" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.252336244">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc001192200&gt;: {&#xA;        s: &#34;expected \&#34;perms of file \\\&#34;/test-volume/test-file\\\&#34;: -rw-r--r--\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: perms of file \&#34;/test-volume/test-file\&#34;: -rw-r--r--&#34;,&#xA;    }&#xA;    expected &#34;perms of file \&#34;/test-volume/test-file\&#34;: -rw-r--r--&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: perms of file &#34;/test-volume/test-file&#34;: -rw-r--r--&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] EmptyDir volumes&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:02:26.276: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename emptydir&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test emptydir 0644 on tmpfs&#xA;Jan 22 22:02:26.328: INFO: Waiting up to 5m0s for pod &#34;pod-6745e982-1e91-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-emptydir-jz52l&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:02:26.330: INFO: Pod &#34;pod-6745e982-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.87295ms&#xA;Jan 22 22:02:28.334: INFO: Pod &#34;pod-6745e982-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.006805185s&#xA;STEP: Saw pod success&#xA;Jan 22 22:02:28.334: INFO: Pod &#34;pod-6745e982-1e91-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:02:28.337: INFO: Trying to get logs from node secconf-node pod pod-6745e982-1e91-11e9-9284-e2fd7d97dccb container test-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:02:28.354: INFO: Waiting for pod pod-6745e982-1e91-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:02:28.357: INFO: Pod pod-6745e982-1e91-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:02:28.357: INFO: Unexpected error occurred: expected &#34;perms of file \&#34;/test-volume/test-file\&#34;: -rw-r--r--&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: perms of file &#34;/test-volume/test-file&#34;: -rw-r--r--&#xA;[AfterEach] [sig-storage] EmptyDir volumes&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-emptydir-jz52l&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:26 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-emptydir-jz52l/pod-6745e982-1e91-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:26 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&#34; already present on machine&#xA;Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:26 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:02:28.362: INFO: At 2019-01-22 22:02:27 +0000 UTC - event for pod-6745e982-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:02:28.370: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:02:28.370: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:02:28.370: INFO: &#xA;Jan 22 22:02:28.373: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:02:28.375: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:39667,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:02:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:02:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:02:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:02:23 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:02:28.375: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:02:28.378: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:02:28.387: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:02:28.387: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:02:28.387: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:02:28.387: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:02:28.387: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:02:28.387: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:02:28.387: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:02:28.387: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:02:28.408: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:02:28.408: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:02:28.412: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:39658,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:02:19 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:02:28.412: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:02:28.415: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:02:28.420: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:02:28.420: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:02:28.420: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:02:28.420: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:02:28.420: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:02:28.420: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:02:28.441: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:02:28.441: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.066745s}&#xA;Jan 22 22:02:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-emptydir-jz52l&#34; for this suite.&#xA;Jan 22 22:02:34.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:02:34.476: INFO: namespace: e2e-tests-emptydir-jz52l, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:02:34.529: INFO: namespace e2e-tests-emptydir-jz52l deletion completed in 6.084446906s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Downgrade kube-proxy from a DaemonSet to static pods should maintain a functioning cluster [Feature:KubeProxyDaemonSetDowngrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should verify ResourceQuota with best effort scope." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [Feature:Networking-IPv6] Forward PTR lookup should forward PTR records lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should retain persistent volume when reclaimPolicy set to retain when associated claim is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Downward API should create a pod that prints his name and namespace" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:LabelSelector] [sig-storage] Selector-Label Volume Binding:vsphere should bind volume with claim for given label" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="39.350384787"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="38.270607091"></testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement test back to back pod creation and deletion with different volume sources on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule a pod w/ RW PD(s) mounted to 1 or more containers, write to PD, verify content, delete pod, and repeat in rapid succession [Slow] using 1 containers and 2 PDs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.17858652"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create a functioning NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should remove from active list jobs that have been deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should remove clusters as expected" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.258909233">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc0023fdbc0&gt;: {&#xA;        s: &#34;expected \&#34;content of file \\\&#34;/etc/projected-secret-volume/data-1\\\&#34;: value-1\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: content of file \&#34;/etc/projected-secret-volume/data-1\&#34;: value-1&#34;,&#xA;    }&#xA;    expected &#34;content of file \&#34;/etc/projected-secret-volume/data-1\&#34;: value-1&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: content of file &#34;/etc/projected-secret-volume/data-1&#34;: value-1&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected secret&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:04:00.329: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating projection with secret that has name projected-secret-test-9f5654cf-1e91-11e9-9284-e2fd7d97dccb&#xA;STEP: Creating a pod to test consume secrets&#xA;Jan 22 22:04:00.391: INFO: Waiting up to 5m0s for pod &#34;pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-dtljd&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:04:00.393: INFO: Pod &#34;pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.039844ms&#xA;Jan 22 22:04:02.396: INFO: Pod &#34;pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.005162027s&#xA;STEP: Saw pod success&#xA;Jan 22 22:04:02.396: INFO: Pod &#34;pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:04:02.399: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:04:02.415: INFO: Waiting for pod pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:04:02.419: INFO: Pod pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:04:02.419: INFO: Unexpected error occurred: expected &#34;content of file \&#34;/etc/projected-secret-volume/data-1\&#34;: value-1&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: content of file &#34;/etc/projected-secret-volume/data-1&#34;: value-1&#xA;[AfterEach] [sig-storage] Projected secret&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-dtljd&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:00 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-dtljd/pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:01 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:01 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:04:02.423: INFO: At 2019-01-22 22:04:01 +0000 UTC - event for pod-projected-secrets-9f56cc3c-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:04:02.433: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:04:02.433: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:02.433: INFO: &#xA;Jan 22 22:04:02.436: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:04:02.438: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:39943,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:03:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:03:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:03:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:03:53 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:04:02.438: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:04:02.441: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:04:02.447: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:04:02.447: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:04:02.447: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:04:02.447: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:04:02.447: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:04:02.447: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:02.472: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:04:02.472: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:04:02.475: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:39964,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:00 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:04:02.475: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:04:02.477: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:04:02.483: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:02.483: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:04:02.483: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:04:02.483: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:04:02.483: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:04:02.483: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:02.508: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:04:02.508: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}&#xA;Jan 22 22:04:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-dtljd&#34; for this suite.&#xA;Jan 22 22:04:08.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:04:08.572: INFO: namespace: e2e-tests-projected-dtljd, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:04:08.588: INFO: namespace e2e-tests-projected-dtljd deletion completed in 6.076923955s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should scrape container metrics from all nodes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.244946525">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc00230e450&gt;: {&#xA;        s: &#34;expected \&#34;FOO=foo-value\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: FOO=foo-value&#34;,&#xA;    }&#xA;    expected &#34;FOO=foo-value&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: FOO=foo-value&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [k8s.io] Variable Expansion&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:04:08.588: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename var-expansion&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test env composition&#xA;Jan 22 22:04:08.648: INFO: Waiting up to 5m0s for pod &#34;var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-var-expansion-7vn4k&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:04:08.653: INFO: Pod &#34;var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.449141ms&#xA;Jan 22 22:04:10.656: INFO: Pod &#34;var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.007908989s&#xA;STEP: Saw pod success&#xA;Jan 22 22:04:10.656: INFO: Pod &#34;var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:04:10.658: INFO: Trying to get logs from node secconf-node pod var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb container dapi-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:04:10.673: INFO: Waiting for pod var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:04:10.677: INFO: Pod var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:04:10.677: INFO: Unexpected error occurred: expected &#34;FOO=foo-value&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: FOO=foo-value&#xA;[AfterEach] [k8s.io] Variable Expansion&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-var-expansion-7vn4k&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:08 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-var-expansion-7vn4k/var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:09 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;docker.io/library/busybox:1.29&#34; already present on machine&#xA;Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:09 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:04:10.681: INFO: At 2019-01-22 22:04:09 +0000 UTC - event for var-expansion-a4428fe3-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:04:10.689: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:04:10.689: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:10.689: INFO: &#xA;Jan 22 22:04:10.692: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:04:10.694: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:39986,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:03 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:03 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:03 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:03 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:04:10.694: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:04:10.696: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:04:10.701: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:04:10.701: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:10.701: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:04:10.701: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:04:10.701: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:04:10.701: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:04:10.719: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:04:10.719: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:04:10.722: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:40016,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:04:10.722: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:04:10.724: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:04:10.728: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:04:10.728: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:10.728: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:10.728: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:04:10.728: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:10.728: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:04:10.747: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:04:10.747: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}&#xA;Jan 22 22:04:10.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-var-expansion-7vn4k&#34; for this suite.&#xA;Jan 22 22:04:16.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:04:16.821: INFO: namespace: e2e-tests-var-expansion-7vn4k, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:04:16.833: INFO: namespace e2e-tests-var-expansion-7vn4k deletion completed in 6.082491672s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.268903693">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc002a299a0&gt;: {&#xA;        s: &#34;expected \&#34;content of file \\\&#34;/etc/projected-configmap-volume/data-1\\\&#34;: value-1\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: content of file \&#34;/etc/projected-configmap-volume/data-1\&#34;: value-1&#34;,&#xA;    }&#xA;    expected &#34;content of file \&#34;/etc/projected-configmap-volume/data-1\&#34;: value-1&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: content of file &#34;/etc/projected-configmap-volume/data-1&#34;: value-1&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected configMap&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:04:16.833: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating configMap with name projected-configmap-test-volume-a92c678b-1e91-11e9-9284-e2fd7d97dccb&#xA;STEP: Creating a pod to test consume configMaps&#xA;Jan 22 22:04:16.892: INFO: Waiting up to 5m0s for pod &#34;pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-9ljbq&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:04:16.895: INFO: Pod &#34;pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.728191ms&#xA;Jan 22 22:04:18.901: INFO: Pod &#34;pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.008386278s&#xA;STEP: Saw pod success&#xA;Jan 22 22:04:18.901: INFO: Pod &#34;pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:04:18.905: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:04:18.938: INFO: Waiting for pod pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:04:18.941: INFO: Pod pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:04:18.941: INFO: Unexpected error occurred: expected &#34;content of file \&#34;/etc/projected-configmap-volume/data-1\&#34;: value-1&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: content of file &#34;/etc/projected-configmap-volume/data-1&#34;: value-1&#xA;[AfterEach] [sig-storage] Projected configMap&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-9ljbq&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:16 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-9ljbq/pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:17 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:17 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:04:18.948: INFO: At 2019-01-22 22:04:17 +0000 UTC - event for pod-projected-configmaps-a92cd2bd-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:04:18.956: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:04:18.956: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:18.956: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:18.957: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:18.957: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:18.957: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:04:18.957: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:04:18.957: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:04:18.957: INFO: &#xA;Jan 22 22:04:18.959: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:04:18.962: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:40025,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:13 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:04:18.962: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:04:18.965: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:04:18.973: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:04:18.973: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:18.973: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:04:18.973: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:04:18.973: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:04:18.973: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:04:18.987: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:04:18.987: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:04:18.989: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:40016,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:04:10 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:04:18.990: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:04:18.992: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:04:18.997: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:18.997: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:04:18.997: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:04:18.997: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:04:18.997: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:04:18.997: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:04:19.017: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:04:19.017: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}&#xA;Jan 22 22:04:19.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-9ljbq&#34; for this suite.&#xA;Jan 22 22:04:25.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:04:25.051: INFO: namespace: e2e-tests-projected-9ljbq, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:04:25.102: INFO: namespace e2e-tests-projected-9ljbq deletion completed in 6.081769831s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid capability name objectSpaceReserve is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update should support rolling-update to same image  [Conformance]" classname="Kubernetes e2e suite" time="37.507682813"></testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale down when non expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support volume SELinux relabeling when using hostPID" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce multiple, stacked policies with overlapping podSelectors [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node Random with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="16.190444251"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid diskStripes value is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the container [Feature:Seccomp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.318298973">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc002b203d0&gt;: {&#xA;        s: &#34;expected \&#34;CONFIG_DATA_1=value-1\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: CONFIG_DATA_1=value-1&#34;,&#xA;    }&#xA;    expected &#34;CONFIG_DATA_1=value-1&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: CONFIG_DATA_1=value-1&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-node] ConfigMap&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:05:18.800: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename configmap&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable via environment variable [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating configMap e2e-tests-configmap-x64qm/configmap-test-ce1e2589-1e91-11e9-9284-e2fd7d97dccb&#xA;STEP: Creating a pod to test consume configMaps&#xA;Jan 22 22:05:18.876: INFO: Waiting up to 5m0s for pod &#34;pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-configmap-x64qm&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:05:18.881: INFO: Pod &#34;pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 5.348834ms&#xA;Jan 22 22:05:20.885: INFO: Pod &#34;pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.009422434s&#xA;STEP: Saw pod success&#xA;Jan 22 22:05:20.885: INFO: Pod &#34;pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:05:20.888: INFO: Trying to get logs from node secconf-node pod pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb container env-test: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:05:20.947: INFO: Waiting for pod pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:05:20.952: INFO: Pod pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:05:20.952: INFO: Unexpected error occurred: expected &#34;CONFIG_DATA_1=value-1&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: CONFIG_DATA_1=value-1&#xA;[AfterEach] [sig-node] ConfigMap&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-configmap-x64qm&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:18 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-configmap-x64qm/pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:19 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;docker.io/library/busybox:1.29&#34; already present on machine&#xA;Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:19 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:05:20.959: INFO: At 2019-01-22 22:05:19 +0000 UTC - event for pod-configmaps-ce1e9267-1e91-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:05:20.967: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:05:20.967: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:05:20.967: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:05:20.968: INFO: &#xA;Jan 22 22:05:20.971: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:05:20.973: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:40242,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:05:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:05:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:05:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:05:13 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:05:20.974: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:05:20.976: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:05:20.981: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:05:20.981: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:05:20.981: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:05:20.981: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:05:20.981: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:05:20.981: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:05:20.999: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:05:20.999: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:05:21.002: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:40277,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:05:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:05:21.002: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:05:21.005: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:05:21.011: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:05:21.011: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:05:21.011: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:05:21.011: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:05:21.011: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 0&#xA;Jan 22 22:05:21.011: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:05:21.035: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:05:21.035: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.06661s}&#xA;Jan 22 22:05:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-configmap-x64qm&#34; for this suite.&#xA;Jan 22 22:05:27.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:05:27.071: INFO: namespace: e2e-tests-configmap-x64qm, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:05:27.118: INFO: namespace e2e-tests-configmap-x64qm deletion completed in 6.079896198s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Deploy clustered applications [Feature:StatefulSet] [Slow] should creating a working mysql cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale down GPU pool from 1 [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]" classname="Kubernetes e2e suite" time="171.564297261"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should release NodePorts on delete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support port-forward" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:NEG] should sync endpoints to NEG" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.271232377">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc001d88340&gt;: {&#xA;        s: &#34;expected \&#34;[1-9]\&#34; in container output: Expected\n    &lt;string&gt;: \nto match regular expression\n    &lt;string&gt;: [1-9]&#34;,&#xA;    }&#xA;    expected &#34;[1-9]&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to match regular expression&#xA;        &lt;string&gt;: [1-9]&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:08:18.683: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39&#xA;[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward API volume plugin&#xA;Jan 22 22:08:18.756: INFO: Waiting up to 5m0s for pod &#34;downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-gq7qj&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:08:18.760: INFO: Pod &#34;downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.159017ms&#xA;Jan 22 22:08:20.766: INFO: Pod &#34;downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.009860565s&#xA;STEP: Saw pod success&#xA;Jan 22 22:08:20.766: INFO: Pod &#34;downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:08:20.768: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb container client-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:08:20.786: INFO: Waiting for pod downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:08:20.789: INFO: Pod downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:08:20.789: INFO: Unexpected error occurred: expected &#34;[1-9]&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to match regular expression&#xA;    &lt;string&gt;: [1-9]&#xA;[AfterEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-gq7qj&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:18 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-gq7qj/downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:19 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:19 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:08:20.793: INFO: At 2019-01-22 22:08:19 +0000 UTC - event for downwardapi-volume-39561214-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:08:20.803: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:08:20.803: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:08:20.803: INFO: &#xA;Jan 22 22:08:20.806: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:08:20.811: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41217,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:08:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:08:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:08:13 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:08:13 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:08:20.811: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:08:20.813: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:08:20.819: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:08:20.819: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:08:20.819: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:08:20.819: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:08:20.819: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:08:20.819: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:08:20.840: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:08:20.840: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:08:20.842: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41422,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:08:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:08:20.843: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:08:20.845: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:08:20.852: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:08:20.852: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:08:20.852: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:08:20.852: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:08:20.852: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:08:20.852: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:08:20.876: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:08:20.876: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.140318s}&#xA;Jan 22 22:08:20.876: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}&#xA;Jan 22 22:08:20.876: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.996776s}&#xA;Jan 22 22:08:20.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-gq7qj&#34; for this suite.&#xA;Jan 22 22:08:26.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:08:26.934: INFO: namespace: e2e-tests-projected-gq7qj, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:08:26.955: INFO: namespace e2e-tests-projected-gq7qj deletion completed in 6.075832604s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] gpu Upgrade [Feature:GPUUpgrade] cluster upgrade should be able to run gpu pod after upgrade [Feature:GPUClusterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]" classname="Kubernetes e2e suite" time="26.171714183"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0644,default) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.181268493"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] gpu Upgrade [Feature:GPUUpgrade] master upgrade should NOT disrupt gpu pod [Feature:GPUMasterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be submitted and removed  [Conformance]" classname="Kubernetes e2e suite" time="22.202433487"></testcase>
      <testcase name="[sig-network] Services should have session affinity work for NodePort service" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 100 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest system logs from all nodes [Feature:StackdriverLogging]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up when expendable pod is created [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Disk Format [Feature:vsphere] verify disk format type - thin is honored for dynamically provisioned pv using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.236899611">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc001a96c30&gt;: {&#xA;        s: &#34;expected \&#34;HOST_IP=(?:\\\\d+)\\\\.(?:\\\\d+)\\\\.(?:\\\\d+)\\\\.(?:\\\\d+)\&#34; in container output: Expected\n    &lt;string&gt;: \nto match regular expression\n    &lt;string&gt;: HOST_IP=(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)&#34;,&#xA;    }&#xA;    expected &#34;HOST_IP=(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to match regular expression&#xA;        &lt;string&gt;: HOST_IP=(?:\d+)\.(?:\d+)\.(?:\d+)\.(?:\d+)&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-node] Downward API&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:09:23.510: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename downward-api&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should provide host IP as an env var [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward api env vars&#xA;Jan 22 22:09:23.566: INFO: Waiting up to 5m0s for pod &#34;downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-downward-api-8h44h&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:09:23.569: INFO: Pod &#34;downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.904484ms&#xA;Jan 22 22:09:25.571: INFO: Pod &#34;downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.005398817s&#xA;STEP: Saw pod success&#xA;Jan 22 22:09:25.572: INFO: Pod &#34;downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:09:25.574: INFO: Trying to get logs from node secconf-node pod downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb container dapi-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:09:25.589: INFO: Waiting for pod downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:09:25.591: INFO: Pod downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:09:25.592: INFO: Unexpected error occurred: expected &#34;HOST_IP=(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)\\.(?:\\d+)&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to match regular expression&#xA;    &lt;string&gt;: HOST_IP=(?:\d+)\.(?:\d+)\.(?:\d+)\.(?:\d+)&#xA;[AfterEach] [sig-node] Downward API&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-downward-api-8h44h&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:23 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-8h44h/downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:24 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;docker.io/library/busybox:1.29&#34; already present on machine&#xA;Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:24 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:09:25.595: INFO: At 2019-01-22 22:09:24 +0000 UTC - event for downward-api-5ff77559-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:09:25.604: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:09:25.604: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:25.604: INFO: &#xA;Jan 22 22:09:25.607: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:09:25.609: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41604,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:23 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:25.609: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:09:25.611: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:09:25.616: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:25.616: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:25.616: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:25.616: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:25.616: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:25.616: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:25.616: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:25.616: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:25.636: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:09:25.636: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:09:25.639: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41589,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:20 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:25.639: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:09:25.641: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:09:25.646: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:09:25.646: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:25.646: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:25.646: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:09:25.646: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:25.646: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:25.664: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:09:25.664: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.140318s}&#xA;Jan 22 22:09:25.664: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}&#xA;Jan 22 22:09:25.664: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.996776s}&#xA;Jan 22 22:09:25.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-downward-api-8h44h&#34; for this suite.&#xA;Jan 22 22:09:31.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:09:31.728: INFO: namespace: e2e-tests-downward-api-8h44h, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:09:31.747: INFO: namespace e2e-tests-downward-api-8h44h deletion completed in 6.080147403s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0666,default) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.190819668"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should set mode on item file [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.236104189">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc0008df280&gt;: {&#xA;        s: &#34;expected \&#34;mode of file \\\&#34;/etc/podinfo/podname\\\&#34;: -r--------\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: mode of file \&#34;/etc/podinfo/podname\&#34;: -r--------&#34;,&#xA;    }&#xA;    expected &#34;mode of file \&#34;/etc/podinfo/podname\&#34;: -r--------&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: mode of file &#34;/etc/podinfo/podname&#34;: -r--------&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:09:39.938: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename downward-api&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39&#xA;[It] should set mode on item file [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward API volume plugin&#xA;Jan 22 22:09:39.999: INFO: Waiting up to 5m0s for pod &#34;downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-downward-api-ksmjc&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:09:40.003: INFO: Pod &#34;downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.122948ms&#xA;Jan 22 22:09:42.007: INFO: Pod &#34;downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.007790777s&#xA;STEP: Saw pod success&#xA;Jan 22 22:09:42.007: INFO: Pod &#34;downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:09:42.010: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb container client-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:09:42.024: INFO: Waiting for pod downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:09:42.027: INFO: Pod downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:09:42.027: INFO: Unexpected error occurred: expected &#34;mode of file \&#34;/etc/podinfo/podname\&#34;: -r--------&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: mode of file &#34;/etc/podinfo/podname&#34;: -r--------&#xA;[AfterEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-downward-api-ksmjc&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-ksmjc/downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:09:42.030: INFO: At 2019-01-22 22:09:40 +0000 UTC - event for downwardapi-volume-69c2fe3b-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:09:42.039: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:09:42.039: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:42.039: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:42.040: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:42.040: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:42.040: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:42.040: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:42.040: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:42.040: INFO: &#xA;Jan 22 22:09:42.043: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:09:42.045: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41647,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:33 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:33 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:33 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:33 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:42.046: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:09:42.048: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:09:42.052: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:42.052: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:42.052: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:42.052: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:42.052: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:42.052: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:42.066: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:09:42.066: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:09:42.068: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41674,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:42.068: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:09:42.071: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:09:42.076: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:42.076: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:42.076: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:09:42.076: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:42.076: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:09:42.076: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:09:42.094: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:09:42.094: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}&#xA;Jan 22 22:09:42.094: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}&#xA;Jan 22 22:09:42.094: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.996776s}&#xA;Jan 22 22:09:42.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-downward-api-ksmjc&#34; for this suite.&#xA;Jan 22 22:09:48.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:09:48.159: INFO: namespace: e2e-tests-downward-api-ksmjc, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:09:48.174: INFO: namespace e2e-tests-downward-api-ksmjc deletion completed in 6.077394683s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.242288709">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc002b83a40&gt;: {&#xA;        s: &#34;expected \&#34;perms of file \\\&#34;/test-volume/test-file\\\&#34;: -rw-rw-rw-\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: perms of file \&#34;/test-volume/test-file\&#34;: -rw-rw-rw-&#34;,&#xA;    }&#xA;    expected &#34;perms of file \&#34;/test-volume/test-file\&#34;: -rw-rw-rw-&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: perms of file &#34;/test-volume/test-file&#34;: -rw-rw-rw-&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] EmptyDir volumes&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:09:48.175: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename emptydir&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test emptydir 0666 on tmpfs&#xA;Jan 22 22:09:48.236: INFO: Waiting up to 5m0s for pod &#34;pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-emptydir-prhcl&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:09:48.239: INFO: Pod &#34;pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.610966ms&#xA;Jan 22 22:09:50.244: INFO: Pod &#34;pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.007333575s&#xA;STEP: Saw pod success&#xA;Jan 22 22:09:50.244: INFO: Pod &#34;pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:09:50.246: INFO: Trying to get logs from node secconf-node pod pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb container test-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:09:50.264: INFO: Waiting for pod pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:09:50.266: INFO: Pod pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:09:50.266: INFO: Unexpected error occurred: expected &#34;perms of file \&#34;/test-volume/test-file\&#34;: -rw-rw-rw-&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: perms of file &#34;/test-volume/test-file&#34;: -rw-rw-rw-&#xA;[AfterEach] [sig-storage] EmptyDir volumes&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-emptydir-prhcl&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-emptydir-prhcl/pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&#34; already present on machine&#xA;Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:09:50.269: INFO: At 2019-01-22 22:09:48 +0000 UTC - event for pod-6eabd2da-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:09:50.277: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:09:50.277: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:50.278: INFO: &#xA;Jan 22 22:09:50.280: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:09:50.283: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41687,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:43 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:43 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:43 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:43 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:50.283: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:09:50.285: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:09:50.290: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:50.290: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:50.290: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:50.290: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:50.290: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:50.290: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:50.303: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:09:50.303: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:09:50.306: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41674,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:40 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:50.306: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:09:50.308: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:09:50.313: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:09:50.313: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:50.313: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:50.313: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:09:50.313: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:50.313: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:50.334: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:09:50.334: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}&#xA;Jan 22 22:09:50.334: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}&#xA;Jan 22 22:09:50.334: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.959052s}&#xA;Jan 22 22:09:50.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-emptydir-prhcl&#34; for this suite.&#xA;Jan 22 22:09:56.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:09:56.378: INFO: namespace: e2e-tests-emptydir-prhcl, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:09:56.417: INFO: namespace e2e-tests-emptydir-prhcl deletion completed in 6.080276937s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should support a &#39;default-deny&#39; policy [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking IPerf IPv6 [Experimental] [Feature:Networking-IPv6] [Slow] [Feature:Networking-Performance] should transfer ~ 1GB onto the service endpoint 1 servers (maximum of 1 clients)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage with delayed binding [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate pod and apply defaults after mutation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should reconcile LB health check interval [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not schedule jobs when suspended [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should support r/w [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.239211068">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc0003c7d40&gt;: {&#xA;        s: &#34;expected \&#34;[1-9]\&#34; in container output: Expected\n    &lt;string&gt;: \nto match regular expression\n    &lt;string&gt;: [1-9]&#34;,&#xA;    }&#xA;    expected &#34;[1-9]&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to match regular expression&#xA;        &lt;string&gt;: [1-9]&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:09:56.417: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39&#xA;[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward API volume plugin&#xA;Jan 22 22:09:56.478: INFO: Waiting up to 5m0s for pod &#34;downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-kstc9&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:09:56.481: INFO: Pod &#34;downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3.719634ms&#xA;Jan 22 22:09:58.484: INFO: Pod &#34;downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.006654943s&#xA;STEP: Saw pod success&#xA;Jan 22 22:09:58.484: INFO: Pod &#34;downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:09:58.486: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb container client-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:09:58.501: INFO: Waiting for pod downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:09:58.503: INFO: Pod downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:09:58.503: INFO: Unexpected error occurred: expected &#34;[1-9]&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to match regular expression&#xA;    &lt;string&gt;: [1-9]&#xA;[AfterEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-kstc9&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:56 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-kstc9/downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:57 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:57 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:09:58.506: INFO: At 2019-01-22 22:09:57 +0000 UTC - event for downwardapi-volume-739557ab-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:09:58.517: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:09:58.517: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:09:58.517: INFO: &#xA;Jan 22 22:09:58.520: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:09:58.522: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41725,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:53 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:53 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:58.522: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:09:58.525: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:09:58.530: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:58.530: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:58.530: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:58.530: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:09:58.530: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:58.530: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:58.545: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:09:58.545: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:09:58.547: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:41720,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:09:50 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:09:58.548: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:09:58.550: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:09:58.555: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:09:58.555: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:09:58.555: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:09:58.555: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:09:58.555: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:09:58.555: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:09:58.572: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:09:58.572: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}&#xA;Jan 22 22:09:58.572: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}&#xA;Jan 22 22:09:58.572: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.959052s}&#xA;Jan 22 22:09:58.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-kstc9&#34; for this suite.&#xA;Jan 22 22:10:04.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:10:04.625: INFO: namespace: e2e-tests-projected-kstc9, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:10:04.656: INFO: namespace e2e-tests-projected-kstc9 deletion completed in 6.081337408s&#xA;</system-out>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp default which is unconfined [Feature:Seccomp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should handle in-cluster config" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should failover to a different zone when all nodes in one zone become unreachable [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpExists)." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.195168157"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that&#39;s waiting for dependents to be deleted [Conformance]" classname="Kubernetes e2e suite" time="16.233083448"></testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 3 pods per node ReplicationController with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on NamespaceSelector [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should set DefaultMode on files [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.254899365">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc002a79700&gt;: {&#xA;        s: &#34;expected \&#34;mode of file \\\&#34;/etc/podinfo/podname\\\&#34;: -r--------\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: mode of file \&#34;/etc/podinfo/podname\&#34;: -r--------&#34;,&#xA;    }&#xA;    expected &#34;mode of file \&#34;/etc/podinfo/podname\&#34;: -r--------&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: mode of file &#34;/etc/podinfo/podname&#34;: -r--------&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:10:29.085: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39&#xA;[It] should set DefaultMode on files [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward API volume plugin&#xA;Jan 22 22:10:29.146: INFO: Waiting up to 5m0s for pod &#34;downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-krkq5&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:10:29.149: INFO: Pod &#34;downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.490355ms&#xA;Jan 22 22:10:31.152: INFO: Pod &#34;downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.006315817s&#xA;STEP: Saw pod success&#xA;Jan 22 22:10:31.152: INFO: Pod &#34;downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:10:31.155: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb container client-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:10:31.173: INFO: Waiting for pod downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:10:31.176: INFO: Pod downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:10:31.176: INFO: Unexpected error occurred: expected &#34;mode of file \&#34;/etc/podinfo/podname\&#34;: -r--------&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: mode of file &#34;/etc/podinfo/podname&#34;: -r--------&#xA;[AfterEach] [sig-storage] Projected downwardAPI&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-krkq5&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:10:31.179: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-krkq5/downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:10:31.180: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:10:31.180: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:10:31.180: INFO: At 2019-01-22 22:10:29 +0000 UTC - event for downwardapi-volume-870e280f-1e92-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:10:31.187: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:10:31.187: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:10:31.187: INFO: &#xA;Jan 22 22:10:31.190: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:10:31.192: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:41978,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:10:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:10:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:10:23 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:10:23 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:10:31.193: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:10:31.195: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:10:31.200: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:10:31.200: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:10:31.200: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:10:31.200: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:10:31.200: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:10:31.200: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:10:31.218: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:10:31.218: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:10:31.221: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:42071,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:10:30 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:10:31.221: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:10:31.223: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:10:31.229: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:10:31.229: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:10:31.229: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:10:31.229: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:10:31.229: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:10:31.229: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:10:31.250: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:10:31.250: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.130277s}&#xA;Jan 22 22:10:31.250: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.398049s}&#xA;Jan 22 22:10:31.250: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.9 Latency:12.954125s}&#xA;Jan 22 22:10:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-krkq5&#34; for this suite.&#xA;Jan 22 22:10:37.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:10:37.317: INFO: namespace: e2e-tests-projected-krkq5, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:10:37.340: INFO: namespace e2e-tests-projected-krkq5 deletion completed in 6.086546629s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group down to 0[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should only allow access from service loadbalancer source ranges [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should enforce the restricted policy.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Deployment deployment should support proportional scaling [Conformance]" classname="Kubernetes e2e suite" time="16.478511632"></testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for LoadBalancer service with ESIPP on [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="34.810811158"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should provide secure master service  [Conformance]" classname="Kubernetes e2e suite" time="6.147371112"></testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] single and multi-cluster ingresses should be able to exist together" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] DNS horizontal autoscaling [DisabledForLargeClusters] kube-dns-autoscaler should scale kube-dns pods in both nonfaulty and faulty scenarios" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [IPv4] Change stubDomain should be able to change stubDomain configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Node Unregister [Feature:vsphere] [Slow] [Disruptive] node unregister" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volumes ConfigMap should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should disable node pool autoscaling [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl apply apply set/view last-applied" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.179108946"></testcase>
      <testcase name="[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to switch session affinity for service with type clusterIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should delete a job" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don&#39;t match the PodAntiAffinity terms" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] Pods should be evicted from unready Node [Feature:TaintEviction] All pods on the unreachable node should be marked as NotReady upon the node turn NotReady AND all pods should be evicted after eviction timeout passes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates local ephemeral storage resource limits of pods that are allowed to run [Feature:LocalStorageCapacityIsolation]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t increase cluster size if pending pod is too large [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cadvisor should be healthy on every node." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="44.797476502"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] kube-proxy migration [Feature:KubeProxyDaemonSetMigration] Upgrade kube-proxy from static pods to a DaemonSet should maintain a functioning cluster [Feature:KubeProxyDaemonSetUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t be able to scale down when rescheduling a pod is required, but pdb doesn&#39;t allow drain[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should create ingress with pre-shared certificate" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should have their auto-restart back-off timer reset on image update [Slow][NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by ordering unclean reboot and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner [Slow] should test that deleting a claim before the volume is provisioned deletes the volume." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Proxy server should support proxy with --port 0  [Conformance]" classname="Kubernetes e2e suite" time="6.230023637"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Verify Volume Attach Through vpxd Restart [Feature:vsphere][Serial][Disruptive] verify volume remains attached through vpxd restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.277418376"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]" classname="Kubernetes e2e suite" time="86.360644738"></testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]" classname="Kubernetes e2e suite" time="47.186150072"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should allow pods under the privileged policy.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:NodeLease][NodeAlphaFeature:NodeLease] when the NodeLease feature is enabled the kubelet should report node status infrequently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner allowedTopologies should create persistent volume in the zone specified in allowedTopologies of storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume that cannot be mounted [Slow] should fail due to wrong node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] ConfigMap should fail to create configMap in volume due to empty configmap key" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha runtime/default annotation [Feature:Seccomp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with non-vsan datastore is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes:vsphere should test that a file written to the vspehre volume mount before kubelet restart can be read after restart [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (root,0777,default) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.188504772"></testcase>
      <testcase name="[sig-storage] Downward API volume should provide podname as non-root with fsgroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify invalid fstype" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Upgrade [Feature:Upgrade] cluster upgrade should maintain a functioning cluster [Feature:ClusterUpgrade]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner Default should create and delete default persistent volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Nodes [Disruptive] Resize [Slow] should be able to delete nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes on one node when pod management is parallel and pod has affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.17841921"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 2 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by triggering kernel panic and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Mounted volume expand[Slow] Should verify mounted devices can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl logs should be able to retrieve and filter logs  [Conformance]" classname="Kubernetes e2e suite" time="325.795595142">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc002b82340&gt;: {&#xA;        s: &#34;Failed to find \&#34;The server is now ready to accept connections\&#34;, last result: \&#34;\&#34;&#34;,&#xA;    }&#xA;    Failed to find &#34;The server is now ready to accept connections&#34;, last result: &#34;&#34;&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1167</failure>
          <system-out>[BeforeEach] [sig-cli] Kubectl client&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:15:18.176: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename kubectl&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-cli] Kubectl client&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243&#xA;[BeforeEach] [k8s.io] Kubectl logs&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134&#xA;STEP: creating an rc&#xA;Jan 22 22:15:18.230: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 create -f - --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:18.465: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:18.465: INFO: stdout: &#34;replicationcontroller/redis-master created\n&#34;&#xA;[It] should be able to retrieve and filter logs  [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Waiting for Redis master to start.&#xA;Jan 22 22:15:19.468: INFO: Selector matched 1 pods for map[app:redis]&#xA;Jan 22 22:15:19.468: INFO: Found 0 / 1&#xA;Jan 22 22:15:20.469: INFO: Selector matched 1 pods for map[app:redis]&#xA;Jan 22 22:15:20.469: INFO: Found 1 / 1&#xA;Jan 22 22:15:20.469: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1&#xA;Jan 22 22:15:20.473: INFO: Selector matched 1 pods for map[app:redis]&#xA;Jan 22 22:15:20.473: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.&#xA;STEP: checking for a matching strings&#xA;Jan 22 22:15:20.473: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:20.540: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:20.540: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:22.541: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:22.609: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:22.609: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:24.611: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:24.689: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:24.689: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:26.689: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:26.758: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:26.758: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:28.758: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:28.827: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:28.827: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:30.828: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:30.898: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:30.898: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:32.900: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:32.973: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:32.973: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:34.973: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:35.043: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:35.043: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:37.044: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:37.113: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:37.113: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:39.114: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:39.189: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:39.189: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:41.190: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:41.260: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:41.260: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:43.264: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:43.334: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:43.334: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:45.335: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:45.404: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:45.404: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:47.404: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:47.495: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:47.495: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:49.495: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:49.568: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:49.568: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:51.569: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:51.641: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:51.641: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:53.642: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:53.712: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:53.712: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:55.715: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:55.788: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:55.788: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:57.789: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:57.865: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:57.865: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:15:59.866: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:15:59.936: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:15:59.936: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:01.936: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:02.008: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:02.008: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:04.009: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:04.088: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:04.088: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:06.089: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:06.159: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:06.159: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:08.159: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:08.226: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:08.226: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:10.226: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:10.297: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:10.297: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:12.299: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:12.371: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:12.371: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:14.371: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:14.438: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:14.438: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:16.439: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:16.506: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:16.506: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:18.507: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:18.581: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:18.581: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:20.583: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:20.654: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:20.654: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:22.654: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:22.729: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:22.729: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:24.730: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:24.800: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:24.800: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:26.802: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:26.869: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:26.869: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:28.869: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:28.941: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:28.941: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:30.942: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:31.010: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:31.010: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:33.010: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:33.081: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:33.081: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:35.082: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:35.158: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:35.158: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:37.159: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:37.230: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:37.230: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:39.230: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:39.297: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:39.297: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:41.298: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:41.367: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:41.367: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:43.368: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:43.437: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:43.437: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:45.437: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:45.513: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:45.513: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:47.515: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:47.587: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:47.588: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:49.591: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:49.658: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:49.658: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:51.658: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:51.727: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:51.727: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:53.728: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:53.792: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:53.792: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:55.795: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:55.866: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:55.866: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:57.866: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:16:57.937: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:16:57.937: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:16:59.937: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:00.011: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:00.011: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:02.013: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:02.087: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:02.087: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:04.088: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:04.166: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:04.166: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:06.167: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:06.235: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:06.235: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:08.236: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:08.305: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:08.305: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:10.306: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:10.376: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:10.376: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:12.376: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:12.450: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:12.450: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:14.450: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:14.519: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:14.519: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:16.519: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:16.592: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:16.592: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:18.593: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:18.677: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:18.677: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:20.678: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:20.744: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:20.744: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:22.745: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:22.811: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:22.811: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:24.814: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:24.887: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:24.887: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:26.887: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:26.957: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:26.957: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:28.960: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:29.029: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:29.029: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:31.030: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:31.098: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:31.098: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:33.099: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:33.170: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:33.170: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:35.172: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:35.262: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:35.262: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:37.264: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:37.332: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:37.332: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:39.333: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:39.403: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:39.403: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:41.406: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:41.480: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:41.480: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:43.482: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:43.552: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:43.552: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:45.552: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:45.619: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:45.619: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:47.619: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:47.693: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:47.694: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:49.696: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:49.774: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:49.774: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:51.776: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:51.852: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:51.852: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:53.853: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:53.933: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:53.933: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:55.934: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:56.013: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:56.013: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:17:58.014: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:17:58.094: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:17:58.094: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:00.097: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:00.177: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:00.177: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:02.178: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:02.254: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:02.254: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:04.255: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:04.336: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:04.336: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:06.338: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:06.413: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:06.413: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:08.414: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:08.493: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:08.493: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:10.494: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:10.570: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:10.570: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:12.570: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:12.653: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:12.653: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:14.653: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:14.732: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:14.732: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:16.733: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:16.810: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:16.810: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:18.811: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:18.887: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:18.887: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:20.887: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:20.970: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:20.970: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:22.972: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:23.047: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:23.047: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:25.047: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:25.132: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:25.132: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:27.133: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:27.209: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:27.209: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:29.211: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:29.294: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:29.294: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:31.294: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:31.371: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:31.371: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:33.372: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:33.451: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:33.451: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:35.451: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:35.542: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:35.542: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:37.543: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:37.615: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:37.615: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:39.615: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:39.690: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:39.690: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:41.692: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:41.772: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:41.772: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:43.773: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:43.845: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:43.845: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:45.845: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:45.926: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:45.926: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:47.926: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:48.005: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:48.005: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:50.006: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:50.083: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:50.083: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:52.084: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:52.161: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:52.161: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:54.163: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:54.246: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:54.246: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:56.247: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:56.326: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:56.326: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:18:58.326: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:18:58.406: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:18:58.406: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:00.406: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:00.497: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:00.497: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:02.498: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:02.575: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:02.575: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:04.577: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:04.651: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:04.651: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:06.653: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:06.734: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:06.734: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:08.736: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:08.814: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:08.814: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:10.814: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:10.888: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:10.888: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:12.889: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:12.967: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:12.967: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:14.969: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:15.047: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:15.047: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:17.048: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:17.143: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:17.143: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:19.143: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:19.218: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:19.218: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:21.219: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:21.298: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:21.298: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:23.298: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:23.383: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:23.383: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:25.384: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:25.466: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:25.466: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:27.467: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:27.544: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:27.544: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:29.546: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:29.624: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:29.624: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:31.625: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:31.702: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:31.702: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:33.703: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:33.781: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:33.781: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:35.782: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:35.856: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:35.856: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:37.856: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:37.937: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:37.937: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:39.938: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:40.024: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:40.024: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:42.025: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:42.108: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:42.108: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:44.110: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:44.197: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:44.197: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:46.197: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:46.275: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:46.275: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:48.275: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:48.364: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:48.364: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:50.365: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:50.447: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:50.447: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:52.449: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:52.525: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:52.525: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:54.526: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:54.602: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:54.602: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:56.604: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:56.683: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:56.683: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:19:58.686: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:19:58.764: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:19:58.764: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:00.764: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:00.840: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:00.840: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:02.841: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:02.922: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:02.922: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:04.923: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:04.999: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:04.999: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:07.000: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:07.076: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:07.076: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:09.077: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:09.155: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:09.155: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:11.156: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:11.232: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:11.232: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:13.233: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:13.317: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:13.317: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:15.318: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:15.394: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:15.394: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:17.397: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:17.475: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:17.475: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:19.476: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 logs redis-master-crr8l redis-master --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:19.558: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:19.558: INFO: stdout: &#34;&#34;&#xA;[AfterEach] [k8s.io] Kubectl logs&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140&#xA;STEP: using delete to clean up resources&#xA;Jan 22 22:20:21.559: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:21.632: INFO: stderr: &#34;warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n&#34;&#xA;Jan 22 22:20:21.632: INFO: stdout: &#34;replicationcontroller \&#34;redis-master\&#34; force deleted\n&#34;&#xA;Jan 22 22:20:21.632: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-265w4&#39;&#xA;Jan 22 22:20:21.728: INFO: stderr: &#34;No resources found.\n&#34;&#xA;Jan 22 22:20:21.728: INFO: stdout: &#34;&#34;&#xA;Jan 22 22:20:21.728: INFO: Running &#39;/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-259822818 get pods -l name=nginx --namespace=e2e-tests-kubectl-265w4 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ &#34;\n&#34; }}{{ end }}{{ end }}&#39;&#xA;Jan 22 22:20:21.798: INFO: stderr: &#34;&#34;&#xA;Jan 22 22:20:21.798: INFO: stdout: &#34;&#34;&#xA;[AfterEach] [sig-cli] Kubectl client&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-kubectl-265w4&#34;.&#xA;STEP: Found 6 events.&#xA;Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:18 +0000 UTC - event for redis-master: {replication-controller } SuccessfulCreate: Created pod: redis-master-crr8l&#xA;Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:18 +0000 UTC - event for redis-master-crr8l: {default-scheduler } Scheduled: Successfully assigned e2e-tests-kubectl-265w4/redis-master-crr8l to secconf-node&#xA;Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:19 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/redis:1.0&#34; already present on machine&#xA;Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:19 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:20:21.802: INFO: At 2019-01-22 22:15:19 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:20:21.802: INFO: At 2019-01-22 22:20:21 +0000 UTC - event for redis-master-crr8l: {kubelet secconf-node} Killing: Killing container with id docker://redis-master:Need to kill Pod&#xA;Jan 22 22:20:21.810: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:20:21.810: INFO: redis-master-crr8l                                       secconf-node    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:15:18 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:20:21.810: INFO: &#xA;Jan 22 22:20:21.814: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:20:21.816: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:43803,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:20:14 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:20:14 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:20:14 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:20:14 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:20:21.816: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:20:21.820: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:20:21.827: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:20:21.827: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:20:21.827: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:20:21.827: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:20:21.827: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:20:21.827: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:20:21.827: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:20:21.847: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:20:21.847: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:20:21.850: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:43811,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:20:21 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:20:21.850: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:20:21.853: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:20:21.861: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:20:21.861: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:20:21.861: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:20:21.861: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:20:21.861: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:20:21.861: INFO: redis-master-crr8l started at 2019-01-22 22:15:18 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:20:21.861: INFO: &#x9;Container redis-master ready: true, restart count 0&#xA;Jan 22 22:20:21.884: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:20:21.884: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:20.360926s}&#xA;Jan 22 22:20:21.884: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:20.360926s}&#xA;Jan 22 22:20:21.884: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:12.879634s}&#xA;Jan 22 22:20:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-kubectl-265w4&#34; for this suite.&#xA;Jan 22 22:20:43.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:20:43.942: INFO: namespace: e2e-tests-kubectl-265w4, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:20:43.972: INFO: namespace e2e-tests-kubectl-265w4 deletion completed in 22.083792335s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through an HTTP proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Inline-volume (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should set DefaultMode on files [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.189727803"></testcase>
      <testcase name="[sig-apps] Job should run a job to completion when tasks succeed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS should provide DNS for pods for Hostname and Subdomain" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Sysctls [NodeFeature:Sysctls] should support sysctls" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]" classname="Kubernetes e2e suite" time="9.292251509"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Network should set TCP CLOSE_WAIT timeout" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Operations Storm [Feature:vsphere] should create pod with many volumes and verify no attach call fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should preserve source pod IP for traffic thru service cluster IP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthenticator] The kubelet&#39;s main port 10250 should reject requests with no credentials" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 2 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify fstype - ext3 formatted volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid objectSpaceReservation and iopsLimit values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Firewall rule should have correct firewall rules for e2e cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="30.197138168"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Pod with node different from PV&#39;s NodeAffinity should fail scheduling due to different NodeAffinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing configmap should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]" classname="Kubernetes e2e suite" time="44.196803858"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two metrics of type Pod from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to update NodePorts with two same port numbers but different protocols" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] HA-master [Feature:HAMaster] survive addition/removal replicas same zone [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] ingress Downgrade [Feature:IngressDowngrade] ingress downgrade should maintain a functioning ingress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Shouldn&#39;t perform scale up operation and should list unhealthy status if most of the cluster is broken[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with best effort scope using scope-selectors." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="46.957030186"></testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicaSet Should scale from 5 pods to 3 pods and from 3 to 1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage in the allowedTopologies with delayed binding [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer =&gt; should not allow an eviction" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn&#39;t evict pod with tolerations from tainted nodes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 50 pods per node using ReplicationController with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging implemented by Stackdriver should ingest logs [Feature:StackdriverLogging]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should exceed backoffLimit" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Stackdriver Monitoring should run Custom Metrics - Stackdriver Adapter for old resource model [Feature:StackdriverCustomMetrics]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] TTLAfterFinished Job should be deleted once it finishes after TTL seconds [Feature:TTLAfterFinished]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes [Feature:ReclaimPolicy] [sig-storage] persistentvolumereclaim:vsphere should delete persistent volume when reclaimPolicy set to delete and associated claim is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Job should adopt matching orphans and release non-matching pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] HostPath should support subPath [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.176059108"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not have port 4194 open on its all public IP addresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.245397387">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc001864e00&gt;: {&#xA;        s: &#34;expected \&#34;POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb\&#34; in container output: Expected\n    &lt;string&gt;: \nto match regular expression\n    &lt;string&gt;: POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34;,&#xA;    }&#xA;    expected &#34;POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to match regular expression&#xA;        &lt;string&gt;: POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-node] Downward API&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:23:10.982: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename downward-api&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward api env vars&#xA;Jan 22 22:23:11.042: INFO: Waiting up to 5m0s for pod &#34;downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-downward-api-hfjjs&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:23:11.045: INFO: Pod &#34;downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.380709ms&#xA;Jan 22 22:23:13.048: INFO: Pod &#34;downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.006009781s&#xA;STEP: Saw pod success&#xA;Jan 22 22:23:13.048: INFO: Pod &#34;downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:23:13.050: INFO: Trying to get logs from node secconf-node pod downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb container dapi-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:23:13.064: INFO: Waiting for pod downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:23:13.067: INFO: Pod downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:23:13.067: INFO: Unexpected error occurred: expected &#34;POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to match regular expression&#xA;    &lt;string&gt;: POD_NAME=downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb&#xA;[AfterEach] [sig-node] Downward API&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-downward-api-hfjjs&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-hfjjs/downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;docker.io/library/busybox:1.29&#34; already present on machine&#xA;Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:23:13.076: INFO: At 2019-01-22 22:23:11 +0000 UTC - event for downward-api-4d2e2e46-1e94-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:23:13.084: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:23:13.084: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:23:13.084: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:23:13.084: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:23:13.085: INFO: &#xA;Jan 22 22:23:13.088: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:23:13.090: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:44251,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:23:04 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:23:04 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:23:04 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:23:04 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:23:13.090: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:23:13.093: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:23:13.098: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:23:13.098: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:23:13.098: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:23:13.098: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:23:13.098: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:23:13.098: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:23:13.113: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:23:13.113: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:23:13.115: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:44281,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:23:11 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:23:13.116: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:23:13.118: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:23:13.123: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:23:13.123: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:23:13.123: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:23:13.123: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:23:13.123: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:23:13.123: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:23:13.143: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:23:13.143: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.133487s}&#xA;Jan 22 22:23:13.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-downward-api-hfjjs&#34; for this suite.&#xA;Jan 22 22:23:19.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:23:19.172: INFO: namespace: e2e-tests-downward-api-hfjjs, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:23:19.227: INFO: namespace e2e-tests-downward-api-hfjjs deletion completed in 6.081350071s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify &#34;immediate&#34; deletion of a PVC that is not in active use by a pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not emit unexpected warnings" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Conformance]" classname="Kubernetes e2e suite" time="369.502227655"></testcase>
      <testcase name="[sig-storage] PersistentVolumes GCEPD should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking should provide unchanging, static URL paths for kubernetes api services" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] RuntimeClass [Feature:RuntimeClass] should run a Pod requesting a RuntimeClass with an empty handler" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] stateful Upgrade [Feature:StatefulUpgrade] [k8s.io] stateful upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] RuntimeClass [Feature:RuntimeClass] should reject a Pod requesting a deleted RuntimeClass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="46.249390468"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for client IP based session affinity: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should delete successful finished jobs with limit of one successful job" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] DisruptionController should update PodDisruptionBudget status" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="52.179754908"></testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by ordering clean reboot and ensure they function upon restart" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  StatefulSet with pod affinity [Slow] should use volumes on one node when pod has affinity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to the readonly kubelet port 10255 using proxy subresource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create an internal type load balancer [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="16.176359405"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]" classname="Kubernetes e2e suite" time="6.221851997"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume FStype [Feature:vsphere] verify fstype - default value should be ext4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for pod-Service: http" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from ExternalName to ClusterIP" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to host port conflict [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.256833443">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc001408930&gt;: {&#xA;        s: &#34;expected \&#34;content of file \\\&#34;/etc/projected-configmap-volume/data-1\\\&#34;: value-1\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: content of file \&#34;/etc/projected-configmap-volume/data-1\&#34;: value-1&#34;,&#xA;    }&#xA;    expected &#34;content of file \&#34;/etc/projected-configmap-volume/data-1\&#34;: value-1&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: content of file &#34;/etc/projected-configmap-volume/data-1&#34;: value-1&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected configMap&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:31:29.558: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating configMap with name projected-configmap-test-volume-765aa60e-1e95-11e9-9284-e2fd7d97dccb&#xA;STEP: Creating a pod to test consume configMaps&#xA;Jan 22 22:31:29.621: INFO: Waiting up to 5m0s for pod &#34;pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-txthf&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:31:29.631: INFO: Pod &#34;pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 9.084877ms&#xA;Jan 22 22:31:31.637: INFO: Pod &#34;pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.015402381s&#xA;STEP: Saw pod success&#xA;Jan 22 22:31:31.637: INFO: Pod &#34;pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:31:31.640: INFO: Trying to get logs from node secconf-node pod pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb container projected-configmap-volume-test: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:31:31.658: INFO: Waiting for pod pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:31:31.661: INFO: Pod pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:31:31.661: INFO: Unexpected error occurred: expected &#34;content of file \&#34;/etc/projected-configmap-volume/data-1\&#34;: value-1&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: content of file &#34;/etc/projected-configmap-volume/data-1&#34;: value-1&#xA;[AfterEach] [sig-storage] Projected configMap&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-txthf&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:29 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-txthf/pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:30 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:30 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:31:31.667: INFO: At 2019-01-22 22:31:30 +0000 UTC - event for pod-projected-configmaps-765b4c90-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:31:31.675: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:31:31.675: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:31:31.675: INFO: &#xA;Jan 22 22:31:31.678: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:31:31.680: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:45196,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:31:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:31:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:31:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:31:25 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:31:31.680: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:31:31.682: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:31:31.688: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:31:31.688: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:31:31.688: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:31:31.688: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:31:31.688: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:31:31.688: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:31:31.702: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:31:31.702: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:31:31.705: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:45184,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:31:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:31:31.705: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:31:31.708: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:31:31.713: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:31:31.713: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:31:31.713: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:31:31.713: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:31:31.713: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:31:31.713: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:31:31.731: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:31:31.731: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.133487s}&#xA;Jan 22 22:31:31.731: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.9 Latency:30.071826s}&#xA;Jan 22 22:31:31.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-txthf&#34; for this suite.&#xA;Jan 22 22:31:37.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:31:37.796: INFO: namespace: e2e-tests-projected-txthf, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:31:37.814: INFO: namespace e2e-tests-projected-txthf deletion completed in 6.081053389s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]" classname="Kubernetes e2e suite" time="7.18122839"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="40.286567512"></testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv3 should be mountable for NFSv3" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.279926582">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc002de6a10&gt;: {&#xA;        s: &#34;expected \&#34;[1-9]\&#34; in container output: Expected\n    &lt;string&gt;: \nto match regular expression\n    &lt;string&gt;: [1-9]&#34;,&#xA;    }&#xA;    expected &#34;[1-9]&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to match regular expression&#xA;        &lt;string&gt;: [1-9]&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:32:25.283: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename downward-api&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39&#xA;[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward API volume plugin&#xA;Jan 22 22:32:25.345: INFO: Waiting up to 5m0s for pod &#34;downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-downward-api-q6k58&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:32:25.350: INFO: Pod &#34;downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.539522ms&#xA;Jan 22 22:32:27.354: INFO: Pod &#34;downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.008972037s&#xA;STEP: Saw pod success&#xA;Jan 22 22:32:27.355: INFO: Pod &#34;downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:32:27.358: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb container client-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:32:27.376: INFO: Waiting for pod downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:32:27.380: INFO: Pod downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:32:27.380: INFO: Unexpected error occurred: expected &#34;[1-9]&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to match regular expression&#xA;    &lt;string&gt;: [1-9]&#xA;[AfterEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-downward-api-q6k58&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:25 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-q6k58/downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:25 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:26 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:32:27.395: INFO: At 2019-01-22 22:32:26 +0000 UTC - event for downwardapi-volume-9791f45a-1e95-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:32:27.407: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:32:27.407: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:32:27.407: INFO: &#xA;Jan 22 22:32:27.410: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:32:27.413: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:45376,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:32:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:32:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:32:25 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:32:25 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:32:27.413: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:32:27.415: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:32:27.420: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:32:27.420: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:32:27.420: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:32:27.420: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:32:27.420: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:32:27.420: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:32:27.420: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:32:27.440: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:32:27.440: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:32:27.443: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:45370,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:32:22 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:32:27.443: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:32:27.445: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:32:27.450: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:32:27.450: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:32:27.450: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:32:27.450: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:32:27.450: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:32:27.450: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:32:27.473: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:32:27.473: INFO: {Operation:stop_container Method:docker_operations_latency_microseconds Quantile:0.99 Latency:30.071826s}&#xA;Jan 22 22:32:27.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-downward-api-q6k58&#34; for this suite.&#xA;Jan 22 22:32:33.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:32:33.548: INFO: namespace: e2e-tests-downward-api-q6k58, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:32:33.562: INFO: namespace e2e-tests-downward-api-q6k58 deletion completed in 6.085722762s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthenticator] The kubelet can delegate ServiceAccount tokens to the API server" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a secret for a workload the node has access to should succeed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl api-versions should check if v1 is in available api versions  [Conformance]" classname="Kubernetes e2e suite" time="6.209702779"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.194777738"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-ui] Kubernetes Dashboard should check that the kubernetes-dashboard instance is alive" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with valid diskStripes and objectSpaceReservation values is honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] GKE node pools [Feature:GKENodePool] should create a cluster with multiple node pools [Feature:GKENodePool]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] DNS horizontal autoscaling [Serial] [Slow] kube-dns-autoscaler should scale kube-dns pods when cluster size changed" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]" classname="Kubernetes e2e suite" time="137.010613371"></testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.188266531"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]" classname="Kubernetes e2e suite" time="24.967624427"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining system pods with pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver [Feature:Networking-IPv6] Change stubDomain should be able to change stubDomain configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]" classname="Kubernetes e2e suite" time="6.170399838">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid hostFailuresToTolerate value is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Docker Containers should be able to override the image&#39;s default command (docker entrypoint) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.18868945"></testcase>
      <testcase name="[k8s.io] Pods should get a host IP [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="24.155512133"></testcase>
      <testcase name="[sig-network] DNS configMap nameserver [IPv4] Forward PTR lookup should forward PTR records lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Liveness liveness pods should be automatically restarted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should support https-only annotation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for client IP based session affinity: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]" classname="Kubernetes e2e suite" time="6.182956829"></testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work from pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] [Serial] [Slow] ReplicationController Should scale from 1 pod to 3 pods and from 3 to 5 and verify decision stability" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks detach in a disrupted environment [Slow] [Disruptive] when node&#39;s API object is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]" classname="Kubernetes e2e suite" time="30.246438947"></testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] AppArmor load AppArmor profiles should enforce an AppArmor profile" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl apply should apply a new configuration to an existing RC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should not scale GPU pool up if pod does not require GPUs [GpuType:] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] ingress Upgrade [Feature:IngressUpgrade] ingress upgrade should maintain a functioning ingress" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] AppArmor load AppArmor profiles can disable an AppArmor profile, using unconfined" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret Should fail non-optional pod creation due to secret object does not exist [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify clean up of stale dummy VM for dynamically provisioned pvc using SPBM policy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (ext3)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Cluster size autoscaler scalability [Slow] should scale up twice [Feature:ClusterAutoscalerScalability2]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: iscsi][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]" classname="Kubernetes e2e suite" time="9.902961115"></testcase>
      <testcase name="[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="40.331225216"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] RuntimeClass [Feature:RuntimeClass] should reject a Pod requesting a RuntimeClass with an unconfigured handler" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should recreate pods scheduled on the unreachable node AND allow scheduling of pods on a node after it rejoins the cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should create and stop a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="28.069436531"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should update nodePort: udp [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Certificates API should support building a client with a CSR" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run pod should create a pod from an image when restart is Never  [Conformance]" classname="Kubernetes e2e suite" time="16.66019936"></testcase>
      <testcase name="[sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Deployment.extensions with 0 secrets, 2 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected downwardAPI should provide container&#39;s cpu limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.193852994"></testcase>
      <testcase name="[sig-cluster-lifecycle] HA-master [Feature:HAMaster] survive addition/removal replicas different zones [Serial][Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.181439281"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,default) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.263272474">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc001bddb70&gt;: {&#xA;        s: &#34;expected \&#34;perms of file \\\&#34;/test-volume/test-file\\\&#34;: -rwxrwxrwx\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: perms of file \&#34;/test-volume/test-file\&#34;: -rwxrwxrwx&#34;,&#xA;    }&#xA;    expected &#34;perms of file \&#34;/test-volume/test-file\&#34;: -rwxrwxrwx&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: perms of file &#34;/test-volume/test-file&#34;: -rwxrwxrwx&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] EmptyDir volumes&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:38:44.418: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename emptydir&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should support (non-root,0777,default) [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test emptydir 0777 on node default medium&#xA;Jan 22 22:38:44.479: INFO: Waiting up to 5m0s for pod &#34;pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-emptydir-d4d47&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:38:44.482: INFO: Pod &#34;pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 3.518179ms&#xA;Jan 22 22:38:46.486: INFO: Pod &#34;pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.007395269s&#xA;STEP: Saw pod success&#xA;Jan 22 22:38:46.486: INFO: Pod &#34;pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:38:46.489: INFO: Trying to get logs from node secconf-node pod pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb container test-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:38:46.510: INFO: Waiting for pod pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:38:46.513: INFO: Pod pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:38:46.513: INFO: Unexpected error occurred: expected &#34;perms of file \&#34;/test-volume/test-file\&#34;: -rwxrwxrwx&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: perms of file &#34;/test-volume/test-file&#34;: -rwxrwxrwx&#xA;[AfterEach] [sig-storage] EmptyDir volumes&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-emptydir-d4d47&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:44 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-emptydir-d4d47/pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:45 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&#34; already present on machine&#xA;Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:45 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:38:46.518: INFO: At 2019-01-22 22:38:45 +0000 UTC - event for pod-798d60e2-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:38:46.526: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:38:46.526: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:38:46.526: INFO: &#xA;Jan 22 22:38:46.529: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:38:46.531: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:46693,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:38:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:38:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:38:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:38:45 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:38:46.531: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:38:46.533: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:38:46.539: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:38:46.539: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:38:46.539: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:38:46.539: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:38:46.539: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:38:46.539: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:38:46.557: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:38:46.557: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:38:46.560: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:46666,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:38:42 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:38:46.560: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:38:46.562: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:38:46.567: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:38:46.567: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:38:46.567: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:38:46.567: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:38:46.567: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:38:46.567: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:38:46.590: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:38:46.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-emptydir-d4d47&#34; for this suite.&#xA;Jan 22 22:38:52.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:38:52.658: INFO: namespace: e2e-tests-emptydir-d4d47, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:38:52.682: INFO: namespace e2e-tests-emptydir-d4d47 deletion completed in 6.08924746s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should scale down when expendable pod is running [Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Addon update should propagate add-on file changes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should conform to Ingress spec" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="10.154981892"></testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] host cleanup with volume mounts [sig-storage][HostCleanup][Flaky] Host cleanup after disrupting NFS volume [NFS] after stopping the nfs-server and deleting the (active) client pod, the NFS mount and the pod&#39;s UID directory should be removed." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Initializers [Feature:Initializers] should be invisible to controllers by default" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-node] Downward API should provide container&#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.182551486"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work for type=NodePort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run rc should create an rc from an image  [Conformance]" classname="Kubernetes e2e suite" time="8.403564135"></testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota&#39;s priority class scope (quota set to pod count: 1) against a pod with same priority class." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on Ports [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.25994083">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc00219b3a0&gt;: {&#xA;        s: &#34;expected \&#34;content of file \\\&#34;/etc/projected-secret-volume/new-path-data-1\\\&#34;: value-1\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: content of file \&#34;/etc/projected-secret-volume/new-path-data-1\&#34;: value-1&#34;,&#xA;    }&#xA;    expected &#34;content of file \&#34;/etc/projected-secret-volume/new-path-data-1\&#34;: value-1&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: content of file &#34;/etc/projected-secret-volume/new-path-data-1&#34;: value-1&#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Projected secret&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:39:19.423: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename projected&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating projection with secret that has name projected-secret-test-map-8e69d1c5-1e96-11e9-9284-e2fd7d97dccb&#xA;STEP: Creating a pod to test consume secrets&#xA;Jan 22 22:39:19.481: INFO: Waiting up to 5m0s for pod &#34;pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-projected-6rjsd&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:39:19.486: INFO: Pod &#34;pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 4.243698ms&#xA;Jan 22 22:39:21.490: INFO: Pod &#34;pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.008470973s&#xA;STEP: Saw pod success&#xA;Jan 22 22:39:21.490: INFO: Pod &#34;pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:39:21.493: INFO: Trying to get logs from node secconf-node pod pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb container projected-secret-volume-test: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:39:21.523: INFO: Waiting for pod pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:39:21.525: INFO: Pod pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:39:21.525: INFO: Unexpected error occurred: expected &#34;content of file \&#34;/etc/projected-secret-volume/new-path-data-1\&#34;: value-1&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: content of file &#34;/etc/projected-secret-volume/new-path-data-1&#34;: value-1&#xA;[AfterEach] [sig-storage] Projected secret&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-projected-6rjsd&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:39:21.533: INFO: At 2019-01-22 22:39:19 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-projected-6rjsd/pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:39:21.534: INFO: At 2019-01-22 22:39:20 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:39:21.534: INFO: At 2019-01-22 22:39:20 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:39:21.534: INFO: At 2019-01-22 22:39:20 +0000 UTC - event for pod-projected-secrets-8e6a3dd3-1e96-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:39:21.541: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:39:21.541: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:39:21.541: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:39:21.542: INFO: &#xA;Jan 22 22:39:21.544: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:39:21.547: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:46814,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:39:15 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:39:15 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:39:15 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:39:15 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:39:21.547: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:39:21.551: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:39:21.557: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:39:21.557: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:39:21.557: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:39:21.557: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:39:21.557: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:39:21.557: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:39:21.557: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:39:21.557: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:39:21.571: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:39:21.571: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:39:21.573: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:46806,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:39:12 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:39:21.574: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:39:21.576: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:39:21.581: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:39:21.581: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:39:21.581: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:39:21.581: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:39:21.581: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:39:21.581: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:39:21.596: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:39:21.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-projected-6rjsd&#34; for this suite.&#xA;Jan 22 22:39:27.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:39:27.639: INFO: namespace: e2e-tests-projected-6rjsd, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:39:27.683: INFO: namespace e2e-tests-projected-6rjsd deletion completed in 6.083913801s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info dump should check if cluster-info dump succeeds" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t trigger additional scale-ups during processing scale-up [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using Job.batch with 0 secrets, 0 configmaps, 0 token projections, and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] Metadata Concealment should run a check-metadata-concealment job to completion" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl copy should copy a file from a running Pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Dynamic PV (default fs)] subPath should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Dynamic PV (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="248.741538533"></testcase>
      <testcase name="[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should fail to create pod by failing to mount volume" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.18105862"></testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl apply should reuse port when apply to an existing SVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: azure] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should successfully scrape all targets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services [Feature:GCEAlphaFeature][Slow] should be able to create and tear down a standard-tier load balancer [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to change the type from NodePort to ExternalName" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with External Metric with target average value from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] should update ingress while sync failures occur on other ingresses" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap federations [Feature:Federation] should be able to change federation configuration [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should fail if subpath with backstepping is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Pre-provisioned PV (default fs)] subPath should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using file as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Feature: gcePD-external] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: cinder] [Testpattern: Pre-provisioned PV (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Downward API volume should provide container&#39;s cpu limit [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="8.25698071">
          <failure type="Failure">/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc0015ecfb0&gt;: {&#xA;        s: &#34;expected \&#34;2\\n\&#34; in container output: Expected\n    &lt;string&gt;: \nto contain substring\n    &lt;string&gt;: 2\n    &#34;,&#xA;    }&#xA;    expected &#34;2\n&#34; in container output: Expected&#xA;        &lt;string&gt;: &#xA;    to contain substring&#xA;        &lt;string&gt;: 2&#xA;        &#xA;not to have occurred&#xA;/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2395</failure>
          <system-out>[BeforeEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153&#xA;STEP: Creating a kubernetes client&#xA;Jan 22 22:43:44.607: INFO: &gt;&gt;&gt; kubeConfig: /tmp/kubeconfig-259822818&#xA;STEP: Building a namespace api object, basename downward-api&#xA;STEP: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39&#xA;[It] should provide container&#39;s cpu limit [NodeConformance] [Conformance]&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699&#xA;STEP: Creating a pod to test downward API volume plugin&#xA;Jan 22 22:43:44.667: INFO: Waiting up to 5m0s for pod &#34;downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb&#34; in namespace &#34;e2e-tests-downward-api-cdlzq&#34; to be &#34;success or failure&#34;&#xA;Jan 22 22:43:44.670: INFO: Pod &#34;downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Pending&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.261109ms&#xA;Jan 22 22:43:46.673: INFO: Pod &#34;downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb&#34;: Phase=&#34;Succeeded&#34;, Reason=&#34;&#34;, readiness=false. Elapsed: 2.005089966s&#xA;STEP: Saw pod success&#xA;Jan 22 22:43:46.673: INFO: Pod &#34;downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb&#34; satisfied condition &#34;success or failure&#34;&#xA;Jan 22 22:43:46.675: INFO: Trying to get logs from node secconf-node pod downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb container client-container: &lt;nil&gt;&#xA;STEP: delete the pod&#xA;Jan 22 22:43:46.696: INFO: Waiting for pod downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb to disappear&#xA;Jan 22 22:43:46.700: INFO: Pod downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb no longer exists&#xA;Jan 22 22:43:46.700: INFO: Unexpected error occurred: expected &#34;2\n&#34; in container output: Expected&#xA;    &lt;string&gt;: &#xA;to contain substring&#xA;    &lt;string&gt;: 2&#xA;    &#xA;[AfterEach] [sig-storage] Downward API volume&#xA;  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154&#xA;STEP: Collecting events from namespace &#34;e2e-tests-downward-api-cdlzq&#34;.&#xA;STEP: Found 4 events.&#xA;Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:44 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {default-scheduler } Scheduled: Successfully assigned e2e-tests-downward-api-cdlzq/downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb to secconf-node&#xA;Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:45 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Pulled: Container image &#34;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&#34; already present on machine&#xA;Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:45 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Created: Created container&#xA;Jan 22 22:43:46.704: INFO: At 2019-01-22 22:43:45 +0000 UTC - event for downwardapi-volume-2c7a6c8e-1e97-11e9-9284-e2fd7d97dccb: {kubelet secconf-node} Started: Started container&#xA;Jan 22 22:43:46.712: INFO: POD                                                      NODE            PHASE    GRACE  CONDITIONS&#xA;Jan 22 22:43:46.713: INFO: sonobuoy                                                 secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:11 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: sonobuoy-e2e-job-98d427a1d3c0481f                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp  secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn  secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 22:07:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 21:07:14 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: calico-node-6j2nx                                        secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: calico-node-cbdfd                                        secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:36:40 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: coredns-86c58d9df4-9895s                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: coredns-86c58d9df4-kd6j9                                 secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: etcd-secconf-master                                      secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: kube-apiserver-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: kube-controller-manager-secconf-master                   secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: kube-proxy-6m8wc                                         secconf-node    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:35:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:37:56 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: kube-proxy-mc5pl                                         secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 09:32:06 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: kube-scheduler-secconf-master                            secconf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-22 20:29:17 +0000 UTC  }]&#xA;Jan 22 22:43:46.713: INFO: &#xA;Jan 22 22:43:46.716: INFO: &#xA;Logging node info for node secconf-master&#xA;Jan 22 22:43:46.718: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-master,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-master,UID:603a2e50-1d5f-11e9-997a-000c293afc9e,ResourceVersion:47267,Generation:0,CreationTimestamp:2019-01-21 09:31:48 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-master,node-role.kubernetes.io/master: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.100/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.0.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[{node-role.kubernetes.io/master  NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:43:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:43:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:43:45 +0000 UTC 2019-01-21 09:31:40 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:43:45 +0000 UTC 2019-01-22 16:13:28 +0000 UTC KubeletReady kubelet is posting ready status}],Addresses:[{InternalIP 192.168.43.100} {Hostname secconf-master}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7861286c482a4015bfff3886763c7c6f,SystemUUID:C72F4D56-7176-4CF6-7186-C2689E3AFC9E,BootID:834082ce-213e-42ff-ad2a-98f7c960b895,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest] 33410348} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:43:46.718: INFO: &#xA;Logging kubelet events for node secconf-master&#xA;Jan 22 22:43:46.721: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-master&#xA;Jan 22 22:43:46.726: INFO: kube-proxy-mc5pl started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:43:46.726: INFO: coredns-86c58d9df4-kd6j9 started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:43:46.726: INFO: calico-node-cbdfd started at 2019-01-21 09:36:40 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:43:46.726: INFO: coredns-86c58d9df4-9895s started at 2019-01-21 09:32:06 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container coredns ready: true, restart count 2&#xA;Jan 22 22:43:46.726: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-2f9zp started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:43:46.726: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:43:46.726: INFO: etcd-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: kube-apiserver-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: kube-controller-manager-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:43:46.726: INFO: kube-scheduler-secconf-master started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jan 22 22:43:46.745: INFO: &#xA;Latency metrics for node secconf-master&#xA;Jan 22 22:43:46.745: INFO: &#xA;Logging node info for node secconf-node&#xA;Jan 22 22:43:46.748: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:secconf-node,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/secconf-node,UID:3bba26f2-1d60-11e9-997a-000c293afc9e,ResourceVersion:47241,Generation:0,CreationTimestamp:2019-01-21 09:37:56 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/os: linux,kubernetes.io/hostname: secconf-node,node-role.kubernetes.io/node: ,},Annotations:map[string]string{kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock,node.alpha.kubernetes.io/ttl: 0,projectcalico.org/IPv4Address: 192.168.43.101/24,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:100.100.1.0/24,DoNotUse_ExternalID:,ProviderID:,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{18029215744 0} {&lt;nil&gt;} 17194Mi BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1907339264 0} {&lt;nil&gt;} 1862636Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {&lt;nil&gt;} 4 DecimalSI},ephemeral-storage: {{16226294143 0} {&lt;nil&gt;} 16226294143 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{1802481664 0} {&lt;nil&gt;} 1760236Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{MemoryPressure False 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2019-01-22 22:43:43 +0000 UTC 2019-01-22 20:34:22 +0000 UTC KubeletReady kubelet is posting ready status} {OutOfDisk Unknown 2019-01-21 09:37:56 +0000 UTC 2019-01-22 20:34:02 +0000 UTC NodeStatusNeverUpdated Kubelet never posted node status.}],Addresses:[{InternalIP 192.168.43.101} {Hostname secconf-node}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:03005e016aba445bad5f2fbcbd9809a2,SystemUUID:DF764D56-499D-0E95-222B-C38C3CBEDB0A,BootID:b82a6d7a-9f78-4235-913b-517c11a60c18,KernelVersion:3.10.0-957.1.3.el7.x86_64,OSImage:CentOS Linux 7 (Core),ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.13.2,KubeProxyVersion:v1.13.2,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcr.io/heptio-images/kube-conformance@sha256:ad05dd6563350277db4706010fbc237a4f25c558caa9d27a12be266055a48801 gcr.io/heptio-images/kube-conformance:v1.13] 462532101} {[gcr.io/google-samples/gb-frontend@sha256:35cb427341429fac3df10ff74600ea73e8ec0754d78f9ce89e0b4f3d70d53ba6 gcr.io/google-samples/gb-frontend:v6] 373099368} {[gcr.io/heptio-images/sonobuoy-plugin-systemd-logs@sha256:58e077ba773ff5d7b6bba90771fad72217f0d10b913bb66bb0afb387c9552788 gcr.io/heptio-images/sonobuoy-plugin-systemd-logs:latest] 290890605} {[k8s.gcr.io/etcd@sha256:905d7ca17fd02bc24c0eba9a062753aba15db3e31422390bc3238eb762339b20 k8s.gcr.io/etcd:3.2.24] 219655340} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0] 195659796} {[k8s.gcr.io/kube-apiserver@sha256:338e08eae4ffda2b8c11f261631d3c2129ef5c7aacc96bffb784475aa5fb0486 k8s.gcr.io/kube-apiserver:v1.13.2] 180959346} {[k8s.gcr.io/kube-controller-manager@sha256:649c68d0798123187c1b1fbad3f4494372f4a6304e6e19f957a03dd2e59bc858 k8s.gcr.io/kube-controller-manager:v1.13.2] 146227986} {[docker.io/nginx@sha256:b543f6d0983fbc25b9874e22f4fe257a567111da96fd1d8f1b44315f1236398c docker.io/nginx:latest] 109174343} {[gcr.io/google-samples/gb-redisslave@sha256:57730a481f97b3321138161ba2c8c9ca3b32df32ce9180e4029e6940446800ec gcr.io/google-samples/gb-redisslave:v3] 98945667} {[k8s.gcr.io/kube-proxy@sha256:f5ee6ed5c49cbb7717bca7324c548c26948a29d8c1d04a05545c56f375f9825c k8s.gcr.io/kube-proxy:v1.13.2] 80254896} {[k8s.gcr.io/kube-scheduler@sha256:c5dbc28218e2624b8b18b60b17c91977454fb1d95068dc9bcf54261da9fb0f15 k8s.gcr.io/kube-scheduler:v1.13.2] 79615090} {[quay.io/calico/cni@sha256:218209f4948d57ae227fb0fba3b597e99334cb39e2abb11555ec359dae177782 quay.io/calico/cni:v3.3.2] 75355616} {[quay.io/calico/node@sha256:eda836bf7c56e2aacaa3f5802e653a35069678b408c744866c486df8ca3df98e quay.io/calico/node:v3.3.2] 75255380} {[k8s.gcr.io/coredns@sha256:81936728011c0df9404cb70b95c17bbc8af922ec9a70d0561a5d01fefa6ffa51 k8s.gcr.io/coredns:1.2.6] 40017418} {[gcr.io/heptio-images/sonobuoy@sha256:11d129bc56862d008e3351a458faa3ca36a0780cb25efa38a54d163ebc150c77 gcr.io/heptio-images/sonobuoy:latest gcr.io/heptio-images/sonobuoy:v0.13.0] 33410348} {[gcr.io/kubernetes-e2e-test-images/nettest@sha256:6aa91bc71993260a87513e31b672ec14ce84bc253cd5233406c6946d3a8f55a1 gcr.io/kubernetes-e2e-test-images/nettest:1.0] 27413498} {[docker.io/nginx@sha256:385fbcf0f04621981df6c6f1abd896101eb61a439746ee2921b26abc78f45571 docker.io/nginx:1.15-alpine] 17759259} {[docker.io/nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker.io/nginx:1.14-alpine] 17724460} {[docker.io/aquasec/kube-bench@sha256:30eb083990ec1bd8b5b5b36352b4b1eae9c9a0a2eaf27bd0c5ecc638804c7e9f docker.io/aquasec/kube-bench:latest] 16798534} {[gcr.io/kubernetes-e2e-test-images/dnsutils@sha256:2abeee84efb79c14d731966e034af33bf324d3b26ca28497555511ff094b3ddd gcr.io/kubernetes-e2e-test-images/dnsutils:1.1] 9349974} {[gcr.io/kubernetes-e2e-test-images/hostexec@sha256:90dfe59da029f9e536385037bc64e86cd3d6e55bae613ddbe69e554d79b0639d gcr.io/kubernetes-e2e-test-images/hostexec:1.1] 8490662} {[gcr.io/kubernetes-e2e-test-images/netexec@sha256:203f0e11dde4baf4b08e27de094890eb3447d807c8b3e990b764b799d3a9e8b7 gcr.io/kubernetes-e2e-test-images/netexec:1.1] 6705349} {[gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 gcr.io/kubernetes-e2e-test-images/redis:1.0] 5905732} {[gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1] 5851985} {[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0] 4753501} {[gcr.io/kubernetes-e2e-test-images/kitten@sha256:bcbc4875c982ab39aa7c4f6acf4a287f604e996d9f34a3fbda8c3d1a7457d1f6 gcr.io/kubernetes-e2e-test-images/kitten:1.0] 4747037} {[gcr.io/kubernetes-e2e-test-images/test-webserver@sha256:7f93d6e32798ff28bc6289254d0c2867fe2c849c8e46edc50f8624734309812e gcr.io/kubernetes-e2e-test-images/test-webserver:1.0] 4732240} {[gcr.io/kubernetes-e2e-test-images/porter@sha256:d6389405e453950618ae7749d9eee388f0eb32e0328a7e6583c41433aa5f2a77 gcr.io/kubernetes-e2e-test-images/porter:1.0] 4681408} {[gcr.io/kubernetes-e2e-test-images/liveness@sha256:748662321b68a4b73b5a56961b61b980ad3683fc6bcae62c1306018fcdba1809 gcr.io/kubernetes-e2e-test-images/liveness:1.0] 4608721} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester@sha256:ba4681b5299884a3adca70fbde40638373b437a881055ffcd0935b5f43eb15c9 gcr.io/kubernetes-e2e-test-images/entrypoint-tester:1.0] 2729534} {[gcr.io/kubernetes-e2e-test-images/mounttest@sha256:c0bd6f0755f42af09a68c9a47fb993136588a76b3200ec305796b60d629d85d2 gcr.io/kubernetes-e2e-test-images/mounttest:1.0] 1563521} {[gcr.io/kubernetes-e2e-test-images/mounttest-user@sha256:17319ca525ee003681fccf7e8c6b1b910ff4f49b653d939ac7f9b6e7c463933d gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0] 1450451} {[docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/busybox:1.29] 1154361} {[k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jan 22 22:43:46.748: INFO: &#xA;Logging kubelet events for node secconf-node&#xA;Jan 22 22:43:46.750: INFO: &#xA;Logging pods the kubelet thinks is on node secconf-node&#xA;Jan 22 22:43:46.755: INFO: sonobuoy-e2e-job-98d427a1d3c0481f started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container e2e ready: true, restart count 0&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 0&#xA;Jan 22 22:43:46.755: INFO: sonobuoy started at 2019-01-22 21:07:11 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container kube-sonobuoy ready: true, restart count 0&#xA;Jan 22 22:43:46.755: INFO: calico-node-6j2nx started at 2019-01-21 09:37:56 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container calico-node ready: true, restart count 2&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container install-cni ready: true, restart count 2&#xA;Jan 22 22:43:46.755: INFO: kube-proxy-6m8wc started at 2019-01-21 09:37:56 +0000 UTC (0+1 container statuses recorded)&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container kube-proxy ready: true, restart count 2&#xA;Jan 22 22:43:46.755: INFO: sonobuoy-systemd-logs-daemon-set-2e191b3136484f58-49bcn started at 2019-01-22 21:07:14 +0000 UTC (0+2 container statuses recorded)&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container sonobuoy-systemd-logs-config ready: true, restart count 1&#xA;Jan 22 22:43:46.755: INFO: &#x9;Container sonobuoy-worker ready: true, restart count 1&#xA;Jan 22 22:43:46.775: INFO: &#xA;Latency metrics for node secconf-node&#xA;Jan 22 22:43:46.775: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:2m3.027673s}&#xA;Jan 22 22:43:46.775: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:2m3.027673s}&#xA;Jan 22 22:43:46.775: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:2m3.027673s}&#xA;Jan 22 22:43:46.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;STEP: Destroying namespace &#34;e2e-tests-downward-api-cdlzq&#34; for this suite.&#xA;Jan 22 22:43:52.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jan 22 22:43:52.850: INFO: namespace: e2e-tests-downward-api-cdlzq, resource: bindings, ignored listing per whitelist&#xA;Jan 22 22:43:52.864: INFO: namespace e2e-tests-downward-api-cdlzq deletion completed in 6.085444926s&#xA;</system-out>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support restarting containers using directory as subpath [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: vSphere] [Testpattern: Inline-volume (ext4)] volumes should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: aws] [Testpattern: Inline-volume (default fs)] subPath should support creating multiple subpath from same volumes [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] In-tree Volumes [Driver: rbd][Feature:Volumes] [Testpattern: Dynamic PV (default fs)] volumeIO should write files of various sizes, verify size, validate content [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
  </testsuite>